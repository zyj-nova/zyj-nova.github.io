<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>聚类</title>
    <url>/2020/10/28/%E8%81%9A%E7%B1%BB/</url>
    <content><![CDATA[<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><p>参考：《机器学习》，2016，周志华；统计学习方法，李航。</p>
<h3 id="1-聚类指标度量"><a href="#1-聚类指标度量" class="headerlink" title="1. 聚类指标度量"></a>1. 聚类指标度量</h3><p>如何衡量簇（类）与簇之间的距离？</p>
<ul>
<li>平均距离（average linkage）</li>
<li>最大距离（complete linkage）</li>
<li>最小距离（single linkage）</li>
<li>中心距离</li>
</ul>
<p>如何衡量聚类效果的好坏？</p>
<h3 id="2-原型聚类"><a href="#2-原型聚类" class="headerlink" title="2. 原型聚类"></a>2. 原型聚类</h3><h4 id="2-1-K均值聚类"><a href="#2-1-K均值聚类" class="headerlink" title="2.1 K均值聚类"></a>2.1 K均值聚类</h4><p><strong>缺点</strong>：</p>
<ul>
<li>对初始化聚类中心敏感</li>
<li>K值难以指定</li>
<li>只能收敛于局部极小</li>
<li>对 outliers 噪声点敏感</li>
</ul>
<h4 id="2-2-高斯混合聚类"><a href="#2-2-高斯混合聚类" class="headerlink" title="2.2 高斯混合聚类"></a>2.2 高斯混合聚类</h4><h3 id="3-层次聚类"><a href="#3-层次聚类" class="headerlink" title="3. 层次聚类"></a>3. 层次聚类</h3><p><strong>算法描述</strong></p>
<ol>
<li>构造n个类，每个类只包含一个样本</li>
<li>计算n个类之间的距离，得到$n \times n$的距离矩阵$D$</li>
<li><strong>合并</strong>类间距离最小的两个类（注意：类间距离有不同的计算方式，可以根据情形来指定），形成一个新类。</li>
<li>计算新类与剩余类之间的距离，构造距离矩阵，若类别数等于k则算法结束，否则继续执行（3）。</li>
</ol>
<h3 id="4-密度聚类"><a href="#4-密度聚类" class="headerlink" title="4. 密度聚类"></a>4. 密度聚类</h3><p><strong>Density Based Spatial Clustering Applications with Noises</strong>（简称$\tt DBSCAN$）。</p>
<p>模型可视化：<a href="https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/" target="_blank" rel="noopener">https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/</a></p>
<h4 id="4-1-基本概念（白话版）"><a href="#4-1-基本概念（白话版）" class="headerlink" title="4.1 基本概念（白话版）"></a>4.1 基本概念（白话版）</h4><p><strong>参数</strong>：$\epsilon$ （阈值），$min Points$（点数量）</p>
<p><strong>核心对象：</strong>对于任一样本 $\pmb x\in D$，以 $\pmb x$ 为中心，半径为 $\epsilon$ 的圆内（领域）存在的其他样本数大于$min Points$，则 $\pmb x$ 为一个核心对象。（当然这里的“圆’指的是维数为2时，3维是球，更高维….反正就是与$\pmb x$距离小于$\epsilon$ 的）。</p>
<p><strong>直接密度可达（密度直达）：</strong>对于核心对象点 $\pmb x_i$，若 $\pmb x_j$ 在以 $\pmb x_i$ 为中心的圆内，则称 $\pmb x_j$ 由 $\pmb x_i$ 密度直达。</p>
<p><strong>密度可达：</strong> 若有一个样本序列，$\pmb x_0,\pmb x_1,…,\pmb x_k$，对任意$\pmb x_i, \pmb x_{i-1}$是<strong>密度直达</strong>的，则称从$\pmb x_0$ 到 $\pmb x_k$ 密度可达。</p>
<p>可以看出这条序列上的所有样本都是核心对象。这句话说的就是密度可达的传递性。图示：</p>
<p><img src="/images/dbscan.png" alt="image-20201028200926586"></p>
<p>上图中，红色的点都是密度直达的。$B、C$ 是密度相连的。</p>
<p>噪声点：从任何一个核心点都不密度可达。</p>
<h4 id="4-2-算法描述"><a href="#4-2-算法描述" class="headerlink" title="4.2 算法描述"></a>4.2 算法描述</h4><p>具体过程就像病毒的扩散过程一样，尽可能感染邻域内的点，当没有点可以感染，就再选一个没被感染的<strong>核心对象</strong>点继续感染，直到没有核心对象点了或者所有点都被感染了，算法停止。</p>
<p>输入：样本集$D=\{\pmb x_1,\pmb x_2,…,\pmb x_m\}$，领域参数$(\epsilon,min Points)$</p>
<p>输出：簇划分$C$</p>
<ol>
<li>标记所有样本D为unvisited</li>
<li>遍历所有样本点，初始化每个样本的邻域信息若某个对象邻域内样本数量大于minPoints, 加入核心对象集合$\Gamma$。</li>
<li>从核心对象集合随机选取一个核心对象p：<ol>
<li>创建一个核心对象队列$\Omega$，将$p$加入到其中</li>
<li>标记p为visited</li>
<li>创建一个簇C，将p添加到C中</li>
<li>当$\Omega$不为空：<ol>
<li>取出一个核心对象</li>
<li>遍历其邻域集合：<ol>
<li>若邻域内某样本q是核心对象<ol>
<li>将q加入到$\Omega$，从$\Gamma$中删去q</li>
</ol>
</li>
<li>否则如果 q 为unvisited：<ol>
<li>标记q为visited</li>
<li>将q加入到C</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>输出C</li>
</ol>
</li>
<li>D中unvisited的为噪声点</li>
</ol>
<ul>
<li>优势：</li>
</ul>
<ol>
<li>不需要指定簇的个数</li>
<li>擅长找到离群点</li>
<li>可以检测任意形状的簇</li>
</ol>
<ul>
<li>缺点：</li>
</ul>
<ol>
<li>高维数据有些困难</li>
<li>参数难以选择</li>
</ol>
<h3 id="5-均值漂移（Mean-Shift）"><a href="#5-均值漂移（Mean-Shift）" class="headerlink" title="5. 均值漂移（Mean Shift）"></a>5. 均值漂移（Mean Shift）</h3>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>动态规划</title>
    <url>/2020/10/25/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><h3 id="0-导引"><a href="#0-导引" class="headerlink" title="0. 导引"></a>0. 导引</h3><p>一个问题能用动态规划解决，必须具有以下两个基本条件：</p>
<ul>
<li>最优子结构</li>
<li>重叠子问题</li>
</ul>
<p>算法导论中提到，<strong>动态规划有两种等价的实现方法。第一种是自底向上的递推迭代，第二种是带备忘录的自顶向下（带备忘录的递归，如果某个阶段已经计算过，那么直接结束递归）</strong>。</p>
<p>还有一个比较重要的是要求<strong>子问题间无后效性</strong>。有些问题可能不满足这个，或者证明比较麻烦。</p>
<p>下面看个例子。</p>
<script type="math/tex; mode=display">
W(a,b)=\left\{ \begin{array}{rcl}
1     &     & a\leq 0\ or\ b\le0\\
W(a-1,b-1)+W(a,b-1)&     & {a < b}\\
W(a-1,b-1)+W(a-1,b)&     & {a\ge b}\\
\end{array}
\right.</script><p>根据这个递归公式，</p>
<ul>
<li>自底向上：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">W</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= a; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= b; j++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(i == <span class="number">0</span> || j == <span class="number">0</span>)&#123;</span><br><span class="line">                dp[i][j] = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(i &lt; j)&#123;</span><br><span class="line">                dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + dp[i][j-<span class="number">1</span>];</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + dp[i-<span class="number">1</span>][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">	&#125;</span><br><span class="line">    <span class="keyword">return</span> dp[a][b];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>带备忘录的自顶向下：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">W</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(a &lt;= <span class="number">0</span> || b &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(dp[a][b] != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> dp[a][b];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(a &lt; b)&#123;</span><br><span class="line">        dp[a][b] = W(a - <span class="number">1</span>, b - <span class="number">1</span>) + W(a, b - <span class="number">1</span>);</span><br><span class="line">    &#125;<span class="keyword">else</span></span><br><span class="line">        dp[a][b] = W(a - <span class="number">1</span>, b - <span class="number">1</span>) + W(a - <span class="number">1</span>, b);</span><br><span class="line">    <span class="keyword">return</span> dp[a][b];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>更多练习，<a href="http://acm.hdu.edu.cn/showproblem.php?pid=1579" target="_blank" rel="noopener">http://acm.hdu.edu.cn/showproblem.php?pid=1579</a></p>
<h3 id="1-最大子段和"><a href="#1-最大子段和" class="headerlink" title="1. 最大子段和"></a>1. 最大子段和</h3><p>状态转移方程：</p>
<script type="math/tex; mode=display">
\tt dp[i] = \max(\tt num[i],\tt dp[i-1]+num[i])</script><p>$\tt dp[i]$表示到以第 $\tt i$ 个元素结尾的最大子连续段和。</p>
<p>leetcode 53号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">int</span> dp[] = <span class="keyword">new</span> <span class="keyword">int</span>[len];</span><br><span class="line">        dp[<span class="number">0</span>] = nums[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> ans = dp[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; len; i++)&#123;</span><br><span class="line">            <span class="comment">//第i个dp元素代表以i结尾的最大连续子序和</span></span><br><span class="line">            dp[i] = Math.max(dp[i-<span class="number">1</span>] + nums[i],nums[i]);</span><br><span class="line">            ans = Math.max(ans,dp[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-最长上升子序列"><a href="#2-最长上升子序列" class="headerlink" title="2. 最长上升子序列"></a>2. 最长上升子序列</h3><p>状态转移方程：</p>
<script type="math/tex; mode=display">
\tt dp[i] = max(\tt dp[j]+1,dp[i]),\tt  if\ j < i\ and\ num[j] < num[i]</script><p>$\tt dp[i]$表示以第 $\tt i$ 个元素结尾的序列的最长上升子序列。</p>
<p>$\tt Leetcode$ 第300号题</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">lengthOfLIS</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(nums.length == <span class="number">0</span> || nums == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(nums.length == <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">int</span> dp[] = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length];</span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++)</span><br><span class="line">            dp[i] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; nums.length; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[j] &lt; nums[i])&#123;</span><br><span class="line">                    dp[i] = Math.max(dp[i], dp[j] + <span class="number">1</span>);<span class="comment">//理解+1操作</span></span><br><span class="line">                    max = Math.max(dp[i], max);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>进阶：</strong>编写算法，求出一个数字序列中最长递增子序列的<strong>个数</strong>。这是Leetcode 673号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findNumberOfLIS</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = nums.length;</span><br><span class="line">        <span class="keyword">int</span> dp[] = <span class="keyword">new</span> <span class="keyword">int</span>[len + <span class="number">1</span>];</span><br><span class="line">        <span class="comment">//需要利用一个counter数组记录每个数字的最长递增序列的组合数是多少</span></span><br><span class="line">        <span class="keyword">int</span> counter[] = <span class="keyword">new</span> <span class="keyword">int</span>[len + <span class="number">1</span>];</span><br><span class="line">         <span class="keyword">int</span> maxn = -<span class="number">1</span>, res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">            dp[i] = <span class="number">1</span>;</span><br><span class="line">            counter[i] = <span class="number">1</span>;<span class="comment">//自身为一个序列，只有一种组合数</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[j] &lt; nums[i])&#123;</span><br><span class="line">                    <span class="keyword">if</span> (dp[j] + <span class="number">1</span> &gt; dp[i])&#123;</span><br><span class="line">                        dp[i] = dp[j] + <span class="number">1</span>;</span><br><span class="line">                        counter[i] = counter[j];</span><br><span class="line">                    &#125;<span class="keyword">else</span> <span class="keyword">if</span> (dp[j] + <span class="number">1</span> == dp[i])&#123;</span><br><span class="line">                        counter[i] += counter[j];</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            maxn = Math.max(dp[i], maxn);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (dp[i] == maxn)&#123;</span><br><span class="line">                res += counter[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-最长公共子序列-lcs"><a href="#3-最长公共子序列-lcs" class="headerlink" title="3. 最长公共子序列(lcs)"></a>3. 最长公共子序列(lcs)</h3><p>状态转移方程：</p>
<script type="math/tex; mode=display">
dp(i,j)=\left\{ \begin{array}{rcl}
dp[i-1][j-1]+1     &     & s1[i]=s2[j]\\
\max(dp[i-1][j],dp[i][j-1])&     & s1[i]\not=s2[j]\\
\end{array}
\right.\\</script><p><strong>提高：现在不仅要求出最长公共子序列的长度，还要求输出最长公共子序列，该怎么办？</strong></p>
<p>题目链接：<a href="http://www.51nod.com/Challenge/Problem.html#problemId=1006" target="_blank" rel="noopener">http://www.51nod.com/Challenge/Problem.html#problemId=1006</a></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1005</span>][<span class="number">1005</span>], flag[<span class="number">1005</span>][<span class="number">1005</span>];</span><br><span class="line"><span class="keyword">char</span> s1[<span class="number">1005</span>],s2[<span class="number">1005</span>];</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">printLCS</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(i &lt;= <span class="number">0</span> || j &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(flag[i][j] == <span class="number">0</span>)&#123;</span><br><span class="line">        printLCS(i<span class="number">-1</span>,j<span class="number">-1</span>);</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; s1[i<span class="number">-1</span>];</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(flag[i][j] == <span class="number">1</span>)&#123;</span><br><span class="line">        printLCS(i<span class="number">-1</span>,j);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        printLCS(i,j<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; s1 &gt;&gt; s2;</span><br><span class="line">    <span class="keyword">int</span> len1 = <span class="built_in">strlen</span>(s1), len2 = <span class="built_in">strlen</span>(s2);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= len1; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>;j &lt;= len2; j++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(s1[i<span class="number">-1</span>] == s2[j<span class="number">-1</span>])&#123;</span><br><span class="line">                dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span>;</span><br><span class="line">                flag[i][j] = <span class="number">0</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="keyword">if</span>(dp[i<span class="number">-1</span>][j] &gt; dp[i][j<span class="number">-1</span>])&#123;</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">                    flag[i][j] = <span class="number">1</span>;</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    dp[i][j] = dp[i][j<span class="number">-1</span>];</span><br><span class="line">                    flag[i][j] = <span class="number">-1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    printLCS(len1,len2);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们需要一个数组记录每次的最大值是从哪个状态转移过来的，要输出的时候，使用递归函数，自顶向下递归，从里到外打印输出。</p>
<p><strong>进阶：</strong>现在还要求出两个字符串最长公共子序列的个数有多少？<a href="https://www.luogu.com.cn/problem/P2516" target="_blank" rel="noopener">https://www.luogu.com.cn/problem/P2516</a></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> maxn 5010</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MOD 100000000</span></span><br><span class="line"><span class="keyword">char</span> A[maxn],B[maxn],cur;</span><br><span class="line"><span class="keyword">int</span> f[<span class="number">2</span>][maxn], g[<span class="number">2</span>][maxn];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%s%s"</span>, A + <span class="number">1</span>, B + <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">int</span> na = <span class="built_in">strlen</span>(A + <span class="number">1</span>), nb = <span class="built_in">strlen</span>(B + <span class="number">1</span>);</span><br><span class="line">    A[na--] = <span class="string">'\0'</span>; B[nb--] = <span class="string">'\0'</span>;</span><br><span class="line">    cur=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= nb; i++) g[<span class="number">0</span>][i] = <span class="number">1</span>;</span><br><span class="line">    g[<span class="number">0</span>][<span class="number">0</span>] = g[<span class="number">1</span>][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= na; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        cur ^= <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= nb; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(A[i] == B[j]) </span><br><span class="line">                f[cur][j] =f[cur^<span class="number">1</span>][j<span class="number">-1</span>] + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> </span><br><span class="line">                f[cur][j] = <span class="built_in">max</span>(f[cur^<span class="number">1</span>][j], f[cur][j<span class="number">-1</span>]);</span><br><span class="line"></span><br><span class="line">            g[cur][j] = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">if</span>(f[cur][j] == f[cur^<span class="number">1</span>][j]) </span><br><span class="line">                g[cur][j] += g[cur^<span class="number">1</span>][j];</span><br><span class="line">            <span class="keyword">if</span>(f[cur][j] == f[cur][j<span class="number">-1</span>]) </span><br><span class="line">                g[cur][j] += g[cur][j<span class="number">-1</span>];</span><br><span class="line">            <span class="keyword">if</span>(f[cur][j] == f[cur^<span class="number">1</span>][j] &amp;&amp; f[cur][j] == f[cur][j<span class="number">-1</span>] &amp;&amp; f[cur^<span class="number">1</span>][j<span class="number">-1</span>] == f[cur][j])</span><br><span class="line">                g[cur][j] -= g[cur^<span class="number">1</span>][j<span class="number">-1</span>];</span><br><span class="line">            <span class="keyword">if</span>(A[i] == B[j] &amp;&amp; f[cur][j] == f[cur^<span class="number">1</span>][j<span class="number">-1</span>] + <span class="number">1</span>) </span><br><span class="line">                g[cur][j] += g[cur^<span class="number">1</span>][j<span class="number">-1</span>];</span><br><span class="line">            <span class="keyword">if</span>(g[cur][j] &gt; MOD) </span><br><span class="line">                g[cur][j] %= MOD;</span><br><span class="line">            <span class="keyword">if</span>(g[cur][j] &lt; <span class="number">0</span>) </span><br><span class="line">                g[cur][j] = (g[cur][j] % MOD) + MOD;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\n%d\n"</span>, f[cur][nb], g[cur][nb]);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-最长公共子串-lc"><a href="#4-最长公共子串-lc" class="headerlink" title="4. 最长公共子串(lc)"></a>4. 最长公共子串(lc)</h3><p>状态转移方程：</p>
<script type="math/tex; mode=display">
\tt dp[i][j] = dp[i-1][j-1]+ 1,when\ str1[i-1]=str2[j-1]</script><script type="math/tex; mode=display">
\tt i\ from\ 0\ to\ len(str1),j\ from\ 0\ to\ len(str2)</script><p>$\tt dp[i][j]$表示str1前i个字符与str2前j个字符的最长公共子串。</p>
<h3 id="5-背包问题"><a href="#5-背包问题" class="headerlink" title="5. 背包问题"></a>5. 背包问题</h3><p>参考文献：背包问题九讲，崔添翼。</p>
<h4 id="5-1-0-1背包"><a href="#5-1-0-1背包" class="headerlink" title="5.1 0-1背包"></a>5.1 0-1背包</h4><p><strong>问题描述：</strong>给定容量为 $V$ 的背包，有 $n$ 个物品，每个物品有价值 $w$、体积 $c$ 两个属性，求背包能装的物品最大价值。</p>
<p><strong>求解：</strong>二维数组 $dp$，$dp[i][j]$ 表示有 $i$ 个物品，背包容量为 $j$ 的情况下，此时的背包最大价值；$dp[n][V]$即为答案</p>
<p>状态转移公式：</p>
<script type="math/tex; mode=display">
dp[i][j] = \max(dp[i-1][j], dp[i-1][j - c[i]] + w[i])</script><p><strong>理解状态转移公式</strong>：</p>
<p>对于每个背包都有两种选择：装与不装（装的前提是当前容量 $j$ 能装下 $c[i]$），不装的话就是前 $i-1$ 个物品容量为 $j$ 时的价值 $dp[i][j]$；若选择装，则背包需要提前给当前物品留下 $c[i]$ 的空间。</p>
<p>代码实现（杭电2602号问题）：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> t, n, V;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1005</span>][<span class="number">1005</span>], v[<span class="number">1005</span>], w[<span class="number">1005</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;t);</span><br><span class="line">    <span class="keyword">while</span>(t--)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;n, &amp;V);</span><br><span class="line">        <span class="comment">// 输入n件物品的价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;v[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 输入n件物品的体积</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;w[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// dp[i][j]代表了到第i个物品，背包容量为j时的最大价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= V; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j &gt;= w[i])&#123;</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i<span class="number">-1</span>][j - w[i]] + v[i], dp[i<span class="number">-1</span>][j]);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, dp[n][V]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>空间压缩：</strong></p>
<script type="math/tex; mode=display">
dp[j] = \max(dp[j], dp[j - c[i]] + w[i])</script><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> t, n, V;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1005</span>], v[<span class="number">1005</span>], w[<span class="number">1005</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;t);</span><br><span class="line">    <span class="keyword">while</span>(t--)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;n, &amp;V);</span><br><span class="line">        <span class="comment">// 输入n件物品的价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;v[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memset</span>(dp, <span class="number">0</span>, <span class="keyword">sizeof</span>(dp));</span><br><span class="line">        <span class="comment">// 输入n件物品的体积</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;w[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// dp[j]代表背包容量为j时的最大价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= w[i]; j--)&#123;<span class="comment">//注意逆序</span></span><br><span class="line">                dp[j] = <span class="built_in">max</span>(dp[j], dp[j - w[i]] + v[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, dp[V]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>初始化的细节问题</strong></p>
<ul>
<li><p>当有些问题要求“恰好装满背包”时的最优解，这时在初始化的时候，dp数组除了dp[0]为0，其余dp[1…V]均设置为无穷大。可以这样理解：初始化的dp数组事实上就是在没有任何物品可以放入背包时的合法状态。如果要求背包恰好装满，那么此时只有容量为$0$的背包可以在什么也不装且价值为 $0$ 的情况下被“恰好装满”，其它容量的背包均没有合法的解，属于未定义的状态，应该被赋值为无穷大了。如果背包并非必须被装满，那么任何容量的背包都有一个合法解“什么都不装”，这个解的价值为0，所以初始时状态的值也就全部为0了。</p>
</li>
<li><p>还有一种情况就是求背包的最小价值，把转移方程中的$max$改为$min$即可。</p>
</li>
<li>有些题目中，只给了价值或者重量这一个数组，那么此时我们可以认为数值上价值与重量时一致的。比如只给了价值 $v$，此时01背包状态转移公式就成了：$dp[j+1] = \max(dp[j],dp[j - v[i]] + v[i]),j \in [V,v[i]]] $。</li>
</ul>
<p>杭电1114号问题就是以上两种情况的结合。题目大意就是给定存钱罐初始重量和装满时候的重量，然后给定几种不同价值和重量的硬币，问能够恰好装满存钱罐的最小价值是多少？每个种类的硬币可以无限使用，这是一个<strong>完全背包问题</strong>。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> m, n, t, N;</span><br><span class="line"><span class="keyword">int</span> v[<span class="number">502</span>], w[<span class="number">502</span>]; <span class="comment">// 硬币价值 和 硬币重量数组</span></span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">10005</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> inf = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;N);</span><br><span class="line">    <span class="keyword">while</span>(N--)&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//输入初始存钱罐重量和现在存钱罐重量</span></span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;m, &amp;n);</span><br><span class="line">        <span class="keyword">int</span> V = n - m; <span class="comment">// 钱币重量</span></span><br><span class="line">        <span class="comment">// 输入硬币种类</span></span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;t);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>,&amp;v[i], &amp;w[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 注意初始化</span></span><br><span class="line">        <span class="built_in">memset</span>(dp, inf, <span class="keyword">sizeof</span>(dp));</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 找到恰好能装满存钱罐重量的钱币价值中最小的那个</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = w[i]; j &lt;= V; j++)&#123;<span class="comment">// 注意这是顺序</span></span><br><span class="line">                dp[j] = min(dp[j - w[i]] + v[i], dp[j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(dp[V] == inf)&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"This is impossible.\n"</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"The minimum amount of money in the piggy-bank is %d.\n"</span>,dp[V]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Leetcode 416号问题：划分等和子集。</li>
</ul>
<p>给定一个<strong>只包含正整数</strong>的<strong>非空</strong>数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。</p>
<p><strong>注意:</strong></p>
<ol>
<li>每个数组中的元素不会超过 100</li>
<li>数组的大小不会超过 200</li>
</ol>
<p>事实上这是个$NP$难问题，但在给定的数据规模下，可以转化为0-1背包问题求解。具体就是：给定n件物品，背包容量为 sum / 2 能否选择一些物品，恰好装满背包？</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">canPartition</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 转为01背包问题；给定n件物品，背包容量为 sum / 2 能否选择一些物品，恰好装满背包</span></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i : nums)&#123;</span><br><span class="line">            sum += i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>((sum &amp; <span class="number">1</span>) == <span class="number">1</span>)&#123;<span class="comment">//总和为奇数，一定不可以对半分</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> target = sum / <span class="number">2</span>; <span class="comment">// 背包容量</span></span><br><span class="line">        <span class="keyword">int</span> dp[] = <span class="keyword">new</span> <span class="keyword">int</span>[target + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span>  j = target; j &gt;= nums[i]; j--)&#123;</span><br><span class="line">                dp[j] = Math.max(dp[j], dp[j - nums[i]] + nums[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[target] == target;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="5-2-完全背包"><a href="#5-2-完全背包" class="headerlink" title="5.2 完全背包"></a>5.2 完全背包</h4><p><strong>定义</strong></p>
<p>有 $N$ 种物品和一个容量为 $V$ 的背包，每种物品都有<strong>无限件</strong>可用。放入第 $i$ 种物品的费用是 $c_i$，价值是 $w_i$。求解：将哪些物品装入背包，可使这些物品的耗费的费用总和不超过背包容量，且价值总和最大。</p>
<p><strong>求解</strong></p>
<p>虽然物品个数是无限的，但是实际上，由于背包容量有上限，每个物品最多选取的个数也是有限制的，这样可以转换成多重背包问题，进而可以转换成 01 背包问题。</p>
<script type="math/tex; mode=display">
dp[i][j] = \max(dp[i-1][j-kc_i] + kw_i|0 \le kc_i \le j)</script><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= v; j++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k*c[i] &lt;= j; k++)&#123;</span><br><span class="line">            dp[i][j] = max(dp[i<span class="number">-1</span>][j - k * c[i]] + w[i] * k, dp[i][j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>观察状态转移公式，我们可以用$dp[i][j - c_i]$去更新$dp[i][j]$而不用去枚举$k$了，因此状态转移公式就变成：</p>
<script type="math/tex; mode=display">
dp[i][j] = \max(dp[i-1][j],dp[i-1][j-c_i]+w_i)</script><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= v; j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j &gt;= c[i])&#123;</span><br><span class="line">            dp[i][j] = max(dp[i<span class="number">-1</span>][j], dp[i<span class="number">-1</span>][j-c[i]] + w[i]);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>空间压缩</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1010</span>];</span><br><span class="line"><span class="keyword">int</span> w[<span class="number">21</span>],c[<span class="number">21</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N, V;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; N &gt;&gt; V;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; w[i] &gt;&gt; c[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = c[i]; j &lt;= V; j++)&#123; <span class="comment">// 顺序</span></span><br><span class="line">            dp[j] = max(dp[j- c[i]] + w[i],dp[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; dp[V] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>可以看到完全背包就是在第二重循环遍历次序与01背包不同而已，01背包是逆序，完全背包是顺序。</strong></p>
<h4 id="5-3-多重背包"><a href="#5-3-多重背包" class="headerlink" title="5.3 多重背包"></a>5.3 多重背包</h4><p><strong>定义</strong></p>
<p>有 $N$ 种物品和一个容量为 $V$ 的背包。第 $i$ 种物品最多有 $n_i$ 件可用，每件耗费的空间是 $c_i$，价值是 $w_i$。求解将哪些物品装入背包可使这些物品的耗费的空间总和不超过背包容量，且价值总和最大。</p>
<p><strong>求解</strong></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">21</span>][<span class="number">1010</span>];</span><br><span class="line"><span class="keyword">int</span> w[<span class="number">21</span>],c[<span class="number">21</span>],n[<span class="number">21</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N,V;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; N &gt;&gt; V;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; w[i] &gt;&gt; c[i] &gt;&gt; n[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= V; j++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>;  k &lt;= n[i]; k++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j &gt;= c[i]*k)&#123;</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i<span class="number">-1</span>][j - c[i]*k] + w[i]*k, dp[i][j]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; dp[N][V] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>空间优化</strong></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1010</span>];</span><br><span class="line"><span class="keyword">int</span> w[<span class="number">21</span>],c[<span class="number">21</span>],n[<span class="number">21</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N,V;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; N &gt;&gt; V;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; w[i] &gt;&gt; c[i] &gt;&gt; n[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= <span class="number">0</span>; j--)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt;= n[i]; k++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j &gt;= c[i]*k)&#123;</span><br><span class="line">                    dp[j] = <span class="built_in">max</span>(dp[j - c[i]*k] + w[i] * k,dp[j]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; dp[V] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>多重背包的二进制优化</strong></p>
<p>二进制思想即任意数字都可以由$2$的次幂的数字组合而来（用01表示），这是计算机的基础。利用这个思想，我们可以把每个物品数量分成$1,2,4,…,n - 2^K + 1$，每一组的体积和价值分别为$(c_i，w_i),(2c_i,2w_i)，(4c_i，4w_i)$，等等，通过这些组合（选择装与不装）一定可以组成$n$，这样就遍历了$1-n$内所有数字。通过拆分所有物品，就形成了一个新的价值和体积物品组，这些问题组我们可以选择装也可以选择不装，这样利用二进制优化就可以将多重背包转化为01背包问题。</p>
<p>代码实现：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 二进制优化</span></span><br><span class="line"><span class="keyword">int</span> ncnt = <span class="number">1</span>; <span class="comment">// 记录新的拆分组数</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= t; i++)&#123; <span class="comment">// t个物品</span></span><br><span class="line">    <span class="comment">// 将num[i]拆分</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">1</span>; k &lt;= num[i]; k &lt;&lt;=<span class="number">1</span>)&#123;</span><br><span class="line">        nv[ncnt] = k * v[i];<span class="comment">//新拆分的价值数组</span></span><br><span class="line">        nw[ncnt++] = k * w[i];<span class="comment">//新拆分的重量数组</span></span><br><span class="line">        num[i] -= k;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(num[i] &gt; <span class="number">0</span>)&#123;</span><br><span class="line">        nv[ncnt] = v[i] * num[i];</span><br><span class="line">        nw[ncnt++] = w[i] * num[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 01背包</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; ncnt; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= nw[i]; j--)&#123;</span><br><span class="line">        dp[j] = max(dp[j - nw[i]] + nv[i], dp[j]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>杭电1059、2844号题是完全背包的二进制优化。</p>
<h4 id="5-4-有约束的背包问题"><a href="#5-4-有约束的背包问题" class="headerlink" title="5.4 有约束的背包问题"></a>5.4 有约束的背包问题</h4><p>杭电3466号问题。题目大意是说，给定背包容量，每个物品的重量$P$，若要装入该物品要求的最小背包容量$Q$以及该物品的价值$V$，输出不超过背包容量下所能获取的最大物品价值。</p>
<p>相比于裸01背包，这个题目要求装入的时候必须考虑$P,Q$这两个条件，要用到贪心。即$A$物品$p_1=3,q_1=5$，$B$物品$p_2=4,q_2=3$；背包容量为$7$，若先装$A$，所需容量至少为$p_1 + q_2 = 7$，若先装$B$，所需容量至少为$p_2 + q_1 = 9$，因此应该选择容量小的。即：$p_1 + q_2 &lt; p_2 + q_1 \rightarrow q_1-p_1 &gt; q_2 - p_2$，即$q-p$大的先买。但是转移方程中，更新过程是逆向的，因此排序的时候应该按照$p-q$从小到大排序。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> n, V, dp[<span class="number">5005</span>];</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">item</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> p,q,v;</span><br><span class="line">&#125;items[<span class="number">5005</span>];</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(struct item a, struct item b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a.q - a.p &lt; b.q - b.p; <span class="comment">// 按照 q-p 从小到大排序</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(~<span class="built_in">scanf</span>(<span class="string">"%d %d"</span>,&amp;n,&amp;V))&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="comment">// p就是物品的重量 v是物品的价值， 背包容量必须大于p和q才能装入</span></span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;items[i].p, &amp;items[i].q, &amp;items[i].v);</span><br><span class="line">        &#125;</span><br><span class="line">        sort(items + <span class="number">1</span>, items + <span class="number">1</span> + n, cmp);</span><br><span class="line">        <span class="built_in">memset</span>(dp,<span class="number">0</span>,<span class="keyword">sizeof</span>(dp));</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= <span class="built_in">max</span>(items[i].p, items[i].q); j--)&#123;</span><br><span class="line">                dp[j] = <span class="built_in">max</span>(dp[j], dp[j - items[i].p] + items[i].v);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,dp[V]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>背包练习(以下题目皆来自杭电 on-line judge: <a href="http://acm.hdu.edu.cn/" target="_blank" rel="noopener">http://acm.hdu.edu.cn/</a> ，做完还不会背包来找我。</p>
<p><a href="http://acm.hdu.edu.cn/showproblem.php?pid=2602" target="_blank">2602</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1114" target="_blank">1114</a>  <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1171" target="_blank">1171</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=2844" target="_blank">2844</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1059" target="_blank">1059</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=2955" target="_blank">2955</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1203" target="_blank">1203</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=3466" target="_blank">3466</a></p>
<h3 id="6-编辑距离"><a href="#6-编辑距离" class="headerlink" title="6. 编辑距离"></a>6. 编辑距离</h3><p>问题描述：</p>
<p>给你两个单词word1和word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。</p>
<p>你可以对一个单词进行如下三种操作：</p>
<ul>
<li>插入一个字符</li>
<li>删除一个字符</li>
<li>替换一个字符</li>
</ul>
<p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode-cn.com/problems/edit-distance" target="_blank" rel="noopener">https://leetcode-cn.com/problems/edit-distance</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>视频讲解：<a href="https://www.youtube.com/watch?v=MiqoA-yF-0M" target="_blank" rel="noopener">https://www.youtube.com/watch?v=MiqoA-yF-0M</a></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minDistance</span><span class="params">(String word1, String word2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = word1.length();</span><br><span class="line">        <span class="keyword">int</span> m = word2.length();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[n + <span class="number">1</span>][m + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= m; j++) &#123;</span><br><span class="line">            dp[<span class="number">0</span>][j] = j;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            dp[i][<span class="number">0</span>] = i;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= m; j++) &#123;</span><br><span class="line">                <span class="comment">// 前三种情况</span></span><br><span class="line">                dp[i][j] = Math.min(dp[i][j - <span class="number">1</span>] + <span class="number">1</span>, Math.min(dp[i - <span class="number">1</span>][j] + <span class="number">1</span>, dp[i - <span class="number">1</span>][j - <span class="number">1</span>] + <span class="number">1</span>));</span><br><span class="line">                <span class="comment">// 第四种情况</span></span><br><span class="line">                <span class="keyword">if</span> (word1.charAt(i - <span class="number">1</span>) == word2.charAt(j - <span class="number">1</span>)) &#123;</span><br><span class="line">                    dp[i][j] = Math.min(dp[i][j], dp[i - <span class="number">1</span>][j - <span class="number">1</span>]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[n][m];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="7-区间DP"><a href="#7-区间DP" class="headerlink" title="7. 区间DP"></a>7. 区间DP</h3><p><strong>概述：</strong>区间类动态规划是线性动态规划的扩展，它在分阶段地划分问题时，与阶段中元素出现的顺序和由前一阶段的哪些元素合并而来由很大的关系。令状态 $f(i,j)$ 表示将下标位置 $i$ 到 $j$ 的所有元素合并能获得的价值的最大（最小）值，那么 $f(i,j) = \max\{f(i,k) + f(k+1,j) + cost\}$，$cost$ 为将这两组元素合并起来的代价。</p>
<p><strong>求解：</strong>对整个问题设最优值，枚举合并点，将问题分解为左右两个部分，最后合并两个部分的最优值得到原问题的最优值。</p>
<h4 id="7-1-矩阵相乘的最少次数"><a href="#7-1-矩阵相乘的最少次数" class="headerlink" title="7.1 矩阵相乘的最少次数"></a>7.1 矩阵相乘的最少次数</h4><p><a href="https://onlinejudge.u-aizu.ac.jp/problems/ALDS1_10_B" target="_blank" rel="noopener">https://onlinejudge.u-aizu.ac.jp/problems/ALDS1_10_B</a></p>
<p>本质上是在填一张表。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">matrix</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> row, col;</span><br><span class="line">&#125;matrices[<span class="number">105</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; n)&#123;</span><br><span class="line">        <span class="keyword">int</span> dp[<span class="number">105</span>][<span class="number">105</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; matrices[i].row &gt;&gt; matrices[i].col;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//填表：自左向右，自下而上 只要填写上半部分</span></span><br><span class="line">        <span class="comment">// dp[i,j] = min&#123;dp[i:k] + dp[k+1:j] + row_i * p * col_j&#125; i &lt;= k &lt; j</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;<span class="comment">//从左到右</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = i; j &gt; <span class="number">0</span>; j--)&#123;<span class="comment">//自下而上</span></span><br><span class="line">                <span class="keyword">if</span>(i == j)&#123;<span class="comment">// 对角线元素 dp[i][i] = 0</span></span><br><span class="line">                    dp[i][j] = <span class="number">0</span>;</span><br><span class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span>(i - j == <span class="number">1</span>)&#123;</span><br><span class="line">                    <span class="comment">// i矩阵与j矩阵相乘：</span></span><br><span class="line">                    dp[j][i] = matrices[j].row * matrices[j].col * matrices[i].col;</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    dp[j][i] = <span class="number">2</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">                    <span class="keyword">for</span>(<span class="keyword">int</span> k = j; k &lt; i; k++)&#123;</span><br><span class="line">                        <span class="keyword">int</span> t = matrices[j].row * matrices[k].col * matrices[i].col;</span><br><span class="line">                        dp[j][i] = <span class="built_in">min</span>(dp[j][k] + dp[k+<span class="number">1</span>][i] + t,dp[j][i]);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; dp[<span class="number">1</span>][n] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="7-2-石子合并"><a href="#7-2-石子合并" class="headerlink" title="7.2 石子合并"></a>7.2 石子合并</h4><p><a href="https://vjudge.net/problem/51Nod-1021" target="_blank" rel="noopener">https://vjudge.net/problem/51Nod-1021</a></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = <span class="number">205</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> INF = <span class="number">2</span> &lt;&lt; <span class="number">21</span>;</span><br><span class="line"><span class="comment">// dp[i][j]表示区间[i,j]所能形成的最优答案</span></span><br><span class="line"><span class="keyword">int</span> n, nums[maxn], dp[maxn][maxn], sum[maxn];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; n;</span><br><span class="line">    <span class="keyword">int</span> tmp = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; nums[i];</span><br><span class="line">        tmp += nums[i];</span><br><span class="line">        sum[i] = tmp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt;= n; j++)&#123;</span><br><span class="line">            dp[i][j] = INF;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> len = <span class="number">1</span>; len &lt;= n; len++)&#123;<span class="comment">//枚举区间长度</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i + len - <span class="number">1</span> &lt;= n; i++)&#123;<span class="comment">//枚举区间起点</span></span><br><span class="line">            <span class="keyword">int</span> j = i + len - <span class="number">1</span>;<span class="comment">//区间终点</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k = i; k &lt;= j; k++)&#123;<span class="comment">//枚举分割点</span></span><br><span class="line">                dp[i][j] = <span class="built_in">min</span>(dp[i][j], dp[i][k] + dp[k+<span class="number">1</span>][j] + sum[j] - sum[i<span class="number">-1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; dp[<span class="number">1</span>][n] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="7-3-Dire-Wolf"><a href="#7-3-Dire-Wolf" class="headerlink" title="7.3 Dire Wolf"></a>7.3 Dire Wolf</h4><p>杭电：<a href="http://acm.hdu.edu.cn/showproblem.php?pid=5115" target="_blank" rel="noopener">http://acm.hdu.edu.cn/showproblem.php?pid=5115</a></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> n, t, a[<span class="number">205</span>], b[<span class="number">205</span>], dp[<span class="number">205</span>][<span class="number">205</span>];</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> INF = <span class="number">2</span> &lt;&lt; <span class="number">25</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    dp[i][j] 代表 i 到 j的狼被杀死 所受的最小伤害，结果就是dp[0][n-1]</span></span><br><span class="line"><span class="comment">    当 1 &lt; i &lt; t ,每条狼的攻击力 a[i] + b[i-1] + b[i+1]</span></span><br><span class="line"><span class="comment">    当这只狼被打败，那么他给邻接的狼提供的攻击力会失效</span></span><br><span class="line"><span class="comment">    考虑没有附加攻击的情况，无论怎么打都是所有a求和，</span></span><br><span class="line"><span class="comment">    现在添加了b，本质上是要我们选择使附加攻击最小的一种方案。</span></span><br><span class="line"><span class="comment">    通过样例我们可以总结出来如果有附带伤害高的狼先杀他，</span></span><br><span class="line"><span class="comment">    然后尽量杀两端的狼，这样只有一边有附带伤害</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; n)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> cnt = <span class="number">0</span>; cnt &lt; n; cnt++)&#123;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; t;</span><br><span class="line">            <span class="built_in">memset</span>(a, <span class="number">0</span>, <span class="keyword">sizeof</span>(a));</span><br><span class="line">            <span class="built_in">memset</span>(b, <span class="number">0</span>, <span class="keyword">sizeof</span>(b));</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= t; i++)&#123;</span><br><span class="line">                <span class="built_in">cin</span> &gt;&gt; a[i];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= t; i++)&#123;</span><br><span class="line">                <span class="built_in">cin</span> &gt;&gt; b[i];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">memset</span>(dp,<span class="number">0</span>,<span class="keyword">sizeof</span>(dp));</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>;i &lt;= t; i++)&#123;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> j = i; j &lt;= t; j++)&#123;</span><br><span class="line">                    dp[i][j] = INF;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> len = <span class="number">1</span>; len &lt;= t; len++)&#123;<span class="comment">// 枚举区间长度</span></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i + len - <span class="number">1</span> &lt;= t; i++)&#123;<span class="comment">//枚举区间起点</span></span><br><span class="line">                    <span class="keyword">int</span> j = i + len - <span class="number">1</span>;<span class="comment">//终点</span></span><br><span class="line">                    <span class="keyword">for</span>(<span class="keyword">int</span> k = i; k &lt;= j; k++)&#123;<span class="comment">//枚举分割点</span></span><br><span class="line">                        dp[i][j] = <span class="built_in">min</span>(dp[i][j],dp[i][k<span class="number">-1</span>] + dp[k+<span class="number">1</span>][j] + a[k] + b[i<span class="number">-1</span>] + b[j+<span class="number">1</span>]);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"Case #%d: %d\n"</span>,cnt+<span class="number">1</span>,dp[<span class="number">1</span>][t]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="7-4-Brackets"><a href="#7-4-Brackets" class="headerlink" title="7.4 Brackets"></a>7.4 Brackets</h4><p>题目链接：<a href="https://vjudge.net/problem/POJ-2955" target="_blank" rel="noopener">https://vjudge.net/problem/POJ-2955</a></p>
<p>状态转移方程：当<code>s[i]=&#39;(&#39; \ and\ s[j]==&#39;)&#39;\ or\ s[i]=&#39;[&#39; \ and\ s[j]==&#39;]&#39;</code> 时，<code>dp[i][j] = dp[i+1][j-1] + 2;</code>还有就是<code>dp[i][j] = max(dp[i][j], dp[i][k] + dp[k+1][j]);</code>。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">char</span> ch[<span class="number">105</span>];</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">105</span>][<span class="number">105</span>];<span class="comment">//表示区间dp[i][j]表示[i,j]内的最大匹配数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; ch &amp;&amp; <span class="built_in">strcmp</span>(ch,<span class="string">"end"</span>)!= <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">int</span> len = <span class="built_in">strlen</span>(ch);</span><br><span class="line">        <span class="built_in">memset</span>(dp, <span class="number">0</span>, <span class="keyword">sizeof</span>(dp));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> t = <span class="number">2</span>; t &lt;= len; t++)&#123;<span class="comment">//枚举区间长度</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i + t - <span class="number">1</span> &lt; len; i++)&#123;<span class="comment">//枚举区间起点</span></span><br><span class="line">                <span class="keyword">int</span> j = i + t - <span class="number">1</span>;<span class="comment">//区间终点</span></span><br><span class="line">                <span class="keyword">if</span>((ch[i] == <span class="string">'('</span> &amp;&amp; ch[j] == <span class="string">')'</span>) || (ch[i] == <span class="string">'['</span> &amp;&amp; ch[j] == <span class="string">']'</span>))&#123;</span><br><span class="line">                    dp[i][j] = dp[i+<span class="number">1</span>][j<span class="number">-1</span>] + <span class="number">2</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> k = i; k &lt;= j; k++)&#123;</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i][j], dp[i][k] + dp[k+<span class="number">1</span>][j]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; dp[<span class="number">0</span>][len<span class="number">-1</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
  </entry>
  <entry>
    <title>机器视觉基础Lecture1</title>
    <url>/2020/10/21/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80Lecture1/</url>
    <content><![CDATA[<h2 id="机器视觉基础-Lecture-1"><a href="#机器视觉基础-Lecture-1" class="headerlink" title="机器视觉基础 Lecture 1"></a>机器视觉基础 Lecture 1</h2><h3 id="1-卷积"><a href="#1-卷积" class="headerlink" title="1. 卷积"></a>1. 卷积</h3><p>卷积操作即用一个窗口在图像上滑动，图像上的像素与窗口内的数值相乘再相加，最后得到新的图像的过程；这个窗口也被称为滤波器/核。</p>
<p><strong>卷积运算的性质</strong>：</p>
<ul>
<li>交换律：$a <em> b = b </em> a$</li>
<li>结合律：$a <em> (b </em> c) = (a <em> b) </em> c$</li>
<li>分配律：$a<em>(b + c) = a</em>b + a*c$</li>
<li>数乘：$ka<em>b = a </em> kb = k(a*b)$</li>
</ul>
<p>可以看出，卷积运算是一种<strong>线性</strong>运算。</p>
<p><strong>Question</strong>：</p>
<ul>
<li><p>设计一个窗口，卷积过后图像保持不变</p>
<p><img src="/images/k1.jpg" alt="image-20201019182152123"></p>
</li>
</ul>
<ul>
<li><p>设计一个窗口，卷积过后图像向左平移1像素</p>
<p><img src="/images/k2.png" alt="image-20201019182236198"></p>
</li>
</ul>
<p>  思考：为什么？对于每一个中心点像素，卷积过后更新为右边像素点像素值，整幅图卷积完就会向左平移。</p>
<ul>
<li><p>设计一个窗口，卷积过后达到锐化（sharpen）的效果</p>
<p><img src="/images/k3.png" alt="image-20201019182428504"></p>
</li>
</ul>
<p>  思考：为什么要这么设计？</p>
<p>  步骤1：</p>
<p>  <img src="/images/f1.png" alt="image-20201019182727419"></p>
<p>  步骤2：</p>
<p>  <img src="/images/f2.png" alt="image-20201019182746490"></p>
<p>  综合步骤1、2可以看出，相当于将原来的像素值乘2再减去均值滤波平滑后的图像，就可以得到锐化后的图像。写成卷积核形式就是上述形式。</p>
<h3 id="2-各种滤波器"><a href="#2-各种滤波器" class="headerlink" title="2. 各种滤波器"></a>2. 各种滤波器</h3><h4 id="2-1-均值滤波"><a href="#2-1-均值滤波" class="headerlink" title="2.1 均值滤波"></a>2.1 均值滤波</h4><p>会出现“振零”现象。</p>
<h4 id="2-2-高斯滤波"><a href="#2-2-高斯滤波" class="headerlink" title="2.2 高斯滤波"></a>2.2 高斯滤波</h4><p>如何生成一个高斯滤波器？高斯函数（连续）：$G(x,y)=e^{-\frac{x^2+y^2}{\sigma^2}}$到卷积核（离散）。</p>
<p>matlab生成$\sigma=1$和$\sigma=3$的高斯核（归一化后的）：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="number">0.0030</span>    <span class="number">0.0133</span>    <span class="number">0.0219</span>    <span class="number">0.0133</span>    <span class="number">0.0030</span></span><br><span class="line"><span class="number">0.0133</span>    <span class="number">0.0596</span>    <span class="number">0.0983</span>    <span class="number">0.0596</span>    <span class="number">0.0133</span></span><br><span class="line"><span class="number">0.0219</span>    <span class="number">0.0983</span>    <span class="number">0.1621</span>    <span class="number">0.0983</span>    <span class="number">0.0219</span></span><br><span class="line"><span class="number">0.0133</span>    <span class="number">0.0596</span>    <span class="number">0.0983</span>    <span class="number">0.0596</span>    <span class="number">0.0133</span></span><br><span class="line"><span class="number">0.0030</span>    <span class="number">0.0133</span>    <span class="number">0.0219</span>    <span class="number">0.0133</span>    <span class="number">0.0030</span></span><br></pre></td></tr></table></figure>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="number">0.0318</span>    <span class="number">0.0375</span>    <span class="number">0.0397</span>    <span class="number">0.0375</span>    <span class="number">0.0318</span></span><br><span class="line"><span class="number">0.0375</span>    <span class="number">0.0443</span>    <span class="number">0.0469</span>    <span class="number">0.0443</span>    <span class="number">0.0375</span></span><br><span class="line"><span class="number">0.0397</span>    <span class="number">0.0469</span>    <span class="number">0.0495</span>    <span class="number">0.0469</span>    <span class="number">0.0397</span></span><br><span class="line"><span class="number">0.0375</span>    <span class="number">0.0443</span>    <span class="number">0.0469</span>    <span class="number">0.0443</span>    <span class="number">0.0375</span></span><br><span class="line"><span class="number">0.0318</span>    <span class="number">0.0375</span>    <span class="number">0.0397</span>    <span class="number">0.0375</span>    <span class="number">0.0318</span></span><br></pre></td></tr></table></figure>
<p>一般设计的高斯滤波默认均值为0，并且根据“3$\sigma$原则”，窗口大小可以根据方差$\sigma$来设定，因此高斯滤波通常只指定一个参数：方差$\sigma$。</p>
<p><strong>3$\sigma$原则</strong>：一维高斯函数我们可以发现，在以原点为中心，向左向右$3|\sigma|$距离内，函数积分可以达到$99.73\%$，也就是说$|x|&gt;3\sigma$的点几乎约等于0，对卷积没有什么贡献。因此设计卷积核的时候。</p>
<p><strong>窗口大小一定，方差$\sigma$对平滑效果的影响：$\sigma$越大，平滑效果越明显。</strong></p>
<p>高斯函数的特点是：$\sigma$越小越集中（函数图像越尖，中心点的权值越大），这样加权平均自身权重更大，当$\sigma$逐渐增大，自身权重变小，加权过后自身信息丧失更加严重。</p>
<p><strong>方差一定且在3$\sigma$原则下，窗口大小对平滑效果的影响：窗口越大，平滑效果越明显。</strong></p>
<p>下面两张图展示了$\sigma=3$的情况下，窗口值大小为$3、9$的滤波结果。</p>
<p><img src="/images/var3_kernelsize_3.jpg" style="zoom:80%;" /></p>
<p><img src="/images/var3_kernelsize_9.jpg" style="zoom:80%;" /></p>
<p>窗口加大需要考虑的像素值越多自身信息丧失的也就越多。要求限定$3\sigma$原则原因是，超过这个范围，无论窗口多大平滑效果几乎一致（最外围差不多都是0了，对加权求和没有贡献）。</p>
<p>高斯噪声：采样于均值为0，方差为$\sigma$的高斯函数的值加到源图像上。</p>
<p>二维高斯函数（均值都是0）：</p>
<script type="math/tex; mode=display">
G(x,y) = \frac{1}{ {2\pi\sigma^2}}e^{-\frac{x^2+y^2}{2\sigma^2}} = G(x)G(y)</script><script type="math/tex; mode=display">
G_x(x,y) = \frac{\partial G}{\partial x} =-\frac{1}{2\pi\sigma^2}\frac{x}{\sigma^2}e^{-\frac{x^2+y^2}{2\sigma^2}}</script><script type="math/tex; mode=display">
G_y(x,y)=\frac{\partial G}{\partial y} =-\frac{1}{2\pi\sigma^2}\frac{y}{\sigma^2}e^{-\frac{x^2+y^2}{2\sigma^2}}</script><script type="math/tex; mode=display">
G_{xx}(x,y) =-\frac{1}{2\pi\sigma^2}(\frac{1}{\sigma^2}-\frac{x^2}{\sigma^4})e^{-\frac{x^2+y^2}{2\sigma^2}}</script><script type="math/tex; mode=display">
G_{yy}(x,y) =-\frac{1}{2\pi\sigma^2}(\frac{1}{\sigma^2}-\frac{y^2}{\sigma^4})e^{-\frac{x^2+y^2}{2\sigma^2}}</script><p>Laplacian of Gaussian (LoG)定义如下：</p>
<script type="math/tex; mode=display">
LoG = \frac{\partial^2 G}{\partial x^2} + \frac{\partial^2 G}{\partial y^2} = G_{xx}(x,y) + G_{yy}(x,y) = \frac{1}{2\pi\sigma^2}\frac{x^2 + y^2-2\sigma^2}{\sigma^4}e^{-\frac{x^2+y^2}{2\sigma^2}}</script><p>当用LoG核与图像做卷积会发现信号衰减，因此通常乘以$\sigma^2$ 做一下scale normaliz ation。</p>
<p>一个近似于该函数的离散卷积核($\sigma=1.4$)：</p>
<p><img src="https://homepages.inf.ed.ac.uk/rbf/HIPR2/figs/logdisc.gif" alt=""></p>
<p><strong>一维一阶</strong>高斯导数如下图所示（省略了系数$1/\sqrt{2\pi\sigma^2}$）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.linspace(<span class="number">-6</span>,<span class="number">6</span>,<span class="number">1000</span>)</span><br><span class="line">sigma_1 = <span class="number">1.0</span></span><br><span class="line">g_prime = -x/sigma_1**<span class="number">2</span> * np.exp(-x**<span class="number">2</span> / <span class="number">2</span> / sigma_1 ** <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/dergaussian.png" alt=""></p>
<p>一维二阶高斯导数如下图所示：</p>
<p><img src="C:\Users\zhang\Desktop\下载 (2" alt="">.png)</p>
<p><strong>将高斯函数离散化就可以得到高斯卷积核、一阶高斯卷积核</strong>。</p>
<p>在图像上，用高斯滤波就是取一个窗口，窗口内以中心点为（0,0）点然后给窗口内的不同坐标的窗口带入高斯函数中取值归一化后，在图像上滑动做卷积。</p>
<p>用高斯一阶导去和信号做卷积，干了两件事：一是平滑信号，二是求导。相当于先用高斯滤波平滑再求导。 同理高斯二阶导 Laplacian of Gaussian，用高斯二阶导去和信号卷积。</p>
<p>高斯差分 DoG （Difference of Gaussian）相当于一个核（滤波器）。下图显示两个正态分布（均值都为1）的函数图像以及二者的差值图像，即DoG。参考：<a href="http://fourier.eng.hmc.edu/e161/lectures/gradient/node9.html" target="_blank" rel="noopener">http://fourier.eng.hmc.edu/e161/lectures/gradient/node9.html</a></p>
<p><img src="http://fourier.eng.hmc.edu/e161/lectures/gradient/img111.png" alt=""></p>
<p><img src="http://fourier.eng.hmc.edu/e161/lectures/gradient/img112.png" alt=""></p>
<p><img src="/images/dog.png" alt=""></p>
<ul>
<li>高斯核求和为1，用途：平滑</li>
<li>高斯一阶导核求和为0，用途：边缘提取</li>
</ul>
<p>可以发现LoG与DoG的函数图像近似，因此为了提高计算效率（当 $\sigma$ 变大，模板尺寸随之也变大，计算开销增大）常用DoG核来替代LoG核（SIFT就是这么干的）。</p>
<h4 id="2-3-中值滤波"><a href="#2-3-中值滤波" class="headerlink" title="2.3 中值滤波"></a>2.3 中值滤波</h4><p>中值滤波的窗口内没有权值，它的作用是把窗口内图像上所有像素值排序后取中值作为窗口中心点的像素值。</p>
<p><strong>对椒盐噪声有效，是非线性滤波</strong>。</p>
<h4 id="2-4-总结"><a href="#2-4-总结" class="headerlink" title="2.4 总结"></a>2.4 总结</h4><p>高斯滤波以及均值滤波都可以起到平滑图像的作用，高斯滤波可以视为有权的均值滤波，图像在平滑的时候不仅会把噪声，也会把图像边缘信息平滑掉（噪声、边缘都是高频成分，因为像素值通常会发生剧烈变化），两者都是低通滤波（low-pass filter）。</p>
<p>而中值滤波能够保留边缘信息，是高通滤波（high-pass filter）。</p>
<h3 id="3-其他滤波"><a href="#3-其他滤波" class="headerlink" title="3. 其他滤波"></a>3. 其他滤波</h3><p>双边滤波（bilateral filter）、导向滤波（guided filter）、拉普拉斯滤波等。</p>
<p>拉普拉斯滤波（和LoG区分一下）：</p>
<script type="math/tex; mode=display">
L(x,y) = \frac{\partial^2I}{\partial x^2} + \frac{\partial^2I}{\partial y^2}</script><p>近似图像上二阶梯度的离散卷积核（拉普拉斯算子）：</p>
<p><img src="https://homepages.inf.ed.ac.uk/rbf/HIPR2/figs/lapmask2.gif" alt=""></p>
<p>拉普拉斯滤波用途：边缘检测等。</p>
<p><img src="/images/Zly.jpg" alt=""></p>
<p>经过拉普拉斯卷积后：</p>
<p><img src="/images/edge.jpg" alt=""></p>
<p>手动实现效果与<code>cv2.Laplacian</code>效果相同</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">kernel = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">-4</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># no padding 卷积</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, img.shape[<span class="number">0</span>] - <span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, img.shape[<span class="number">1</span>] - <span class="number">2</span>):</span><br><span class="line">        img[i,j] = np.sum(kernel * img[i:i+<span class="number">3</span>, j:j+<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器视觉基础</category>
      </categories>
  </entry>
  <entry>
    <title>机器视觉基础Lecture 3局部特征</title>
    <url>/2020/10/15/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80Lecture%203%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81/</url>
    <content><![CDATA[<h3 id="Lecture-3-局部特征"><a href="#Lecture-3-局部特征" class="headerlink" title="Lecture 3 局部特征"></a>Lecture 3 局部特征</h3><h3 id="1-Harris角点检测"><a href="#1-Harris角点检测" class="headerlink" title="1. Harris角点检测"></a>1. Harris角点检测</h3><h4 id="1-1-特征点需要满足的性质"><a href="#1-1-特征点需要满足的性质" class="headerlink" title="1.1 特征点需要满足的性质"></a>1.1 特征点需要满足的性质</h4><p>可重复性/区分性/容易计算</p>
<p>为什么选角点？</p>
<p>一个窗口在图像上滑动，处于平坦（flat）区域时，窗口内的内容不会发生明显改变；处于边缘（edge）区域时，只有沿一个方向（竖直或水平）滑动会发生变化；而处于角点（corner）区域时，沿两个方向滑动窗口内容都会发生改变。</p>
<h4 id="1-2-角点"><a href="#1-2-角点" class="headerlink" title="1.2 角点"></a>1.2 角点</h4><script type="math/tex; mode=display">
E(u,v) = \sum_{x,y} w(x,y)[I(x + u, y + v) - I(x,y)]^2</script><p>说明：$x,y$的取值范围是窗口的范围；$w(x,y)$指权值函数，比如$Gaussian$函数，本质上取到一个加权的作用。$u,v$即平移的距离。$E(u,v)$表示窗口内容变化程度。</p>
<p>我们希望直接观察到$u,v$和$E(u,v)$之间的关系，而上式定义较麻烦不直观（还得通过$I(x,y)$），怎么直接建立关系呢？答案：二维泰勒展开！</p>
<script type="math/tex; mode=display">
E(u,v) \approx E(0,0) + [u,v]\begin{pmatrix}E_u(0,0)\\E_v(0,0)\end{pmatrix} +\frac{1}{2}[u,v]\begin{pmatrix}E_{uu}&E_{uv}\\E_{vu}&E_{vv}\end{pmatrix}\left [\begin{matrix}u\\v\end{matrix} \right]</script><script type="math/tex; mode=display">
E(0,0) = 0</script><script type="math/tex; mode=display">
E_{u}(u,v) = \sum_{x,y}2w(x,y)[I(x+u,y+v)-I(x,y)]I_x(x+u,y+v)</script><p>将(0,0)带入：得：$E_u(0,0) = 0$；同理可得$E_v(0,0)=0$；</p>
<p>为了表示方便，令：$I(x+u,y+v)$简写为$I$，$I_x(x+u,y+v)$简写为$I_x$等</p>
<script type="math/tex; mode=display">
E_{uu}(u,v) = \sum_{x,y}2w(x,y)I_xI_x+\sum_{x,y}2w(x,y)[I-I(x,y)]I_{xx}</script><script type="math/tex; mode=display">
E_{uv}(u,v) =\sum_{x,y}2w(x,y)I_yI_x+\sum_{x,y}2w(x,y)[I-I(x,y)]I_{xy}</script><p>带(0,0)有：</p>
<script type="math/tex; mode=display">
E_{uu}(0,0) = \sum_{x,y}2w(x,y)I_x(x,y)I_x(x,y)\\
E_{vv}(0,0) = \sum_{x,y}2w(x,y)I_y(x,y)I_y(x,y)\\
E_{uv}(0,0) =E_{vu}(0,0)= \sum_{x,y}2w(x,y)I_x(x,y)I_y(x,y)</script><p>即：</p>
<script type="math/tex; mode=display">
E(u,v) \approx [u,v]M\left[ \begin{matrix}u\\v\end{matrix} \right]</script><script type="math/tex; mode=display">
M_{2\times2} = \sum_{x,y}w(x,y)\left[\begin{matrix}I_x^2&I_xI_y\\I_xI_y&I_y^2\end{matrix}\right] = \left[\begin{matrix}\lambda_1&0\\0&\lambda_2\end{matrix}\right]</script><p>$I_x,I_y$即上一个Lecture讲过的图像上定义的梯度（同方向相邻两个像素的差值）。$M$被称为second moment matrix。它是一个对称正定矩阵，因此可以<strong>对角化</strong>为特征值组成的对角矩阵。</p>
<p>考虑：</p>
<script type="math/tex; mode=display">
E(u,v) = \lambda_1u^2 + \lambda_2v^2=C,C\in R</script><p>可知$E(u,v)$在几何上是一个椭圆。其轴分别为$1/\sqrt\lambda_1,1/\sqrt\lambda_2$。</p>
<p>$\lambda_1,\lambda_2$表示窗口内的$x,y$方向的图像梯度。当两个方向变化都很大时可能为角点。</p>
<script type="math/tex; mode=display">
R = \det(M) - \alpha *trace(M)^2 = \lambda_1\lambda_2-\alpha(\lambda_1+\lambda_2)^2</script><p><img src="/images/image-20201015212534161.png" alt="image-20201015212534161"></p>
<p>优点：对光照、亮度不敏感</p>
<p>缺点：对尺度（scale）变换敏感！</p>
<h3 id="2-Blob-Detection"><a href="#2-Blob-Detection" class="headerlink" title="2. Blob Detection"></a>2. Blob Detection</h3><h3 id="3-SIFT"><a href="#3-SIFT" class="headerlink" title="3. SIFT"></a>3. SIFT</h3><p>SIFT即尺度不变特征变换。光照、尺度、旋转、平移。</p>
<p>添加仿射自适应：更好的处理视角</p>
<p>观察dog图像和log图像发现他俩走势几乎一致，可以用dog来代替log提高计算效率</p>
<p>sift $4 \times 4 \times 8=128$ 维描述子，sift算法将邻域内像素划分为 $16×16$ 个子域，进一步将其划分为 $4×4$ 块（每个块又是由 $4×4$ 小域组成），具体见下，分别计算每个块内8个方向的梯度方向直方图。所以最终得到 $4×4×8=128$ 4×4×8=128 的描述子向量。</p>
<p><strong>梯度强度直方图</strong>：将$[0,2\pi]$划分为8份，也就是说$[0,\frac{\pi}{4}],[\frac{\pi}{4},\frac{\pi}{2}]，…,[\frac{7\pi}{4},2\pi]$。然后在这些小区域（16个像素）的范围内，根据每个像素点的梯度方向所在范围，将该像素点的梯度幅值加到对应的bin上，最后就形成了小区域一个梯度强度直方图（8个数字描述）。16个小区域的整体特征就可以用这个$4 \times 4 \times 8$的特征向量来描述。</p>
<p><img src="https://img-blog.csdn.net/20171025223351016?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2hpeW9uZ3Jhb3c=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p>
<p>在小区域上（16个像素）统计梯度幅值直方图能更好的捕捉局部特征。</p>
]]></content>
      <categories>
        <category>机器视觉基础</category>
      </categories>
  </entry>
  <entry>
    <title>降维方法</title>
    <url>/2020/10/13/%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="降维方法"><a href="#降维方法" class="headerlink" title="降维方法"></a>降维方法</h2><h3 id="1-主成分分析"><a href="#1-主成分分析" class="headerlink" title="1. 主成分分析"></a>1. 主成分分析</h3><h4 id="1-1-算法分析"><a href="#1-1-算法分析" class="headerlink" title="1.1 算法分析"></a>1.1 算法分析</h4><p>已知样本$\pmb a_1,\pmb a_2,…,\pmb a_N,\pmb a_i \in \mathbb{R}^n$，则按列组成的矩阵:</p>
<script type="math/tex; mode=display">
A_{n \times m} = \begin{pmatrix}\pmb a_1,&\pmb a_2,...&,\pmb a_N\end{pmatrix}</script><p><strong>矩阵的弗罗贝尼乌斯范数</strong>：</p>
<script type="math/tex; mode=display">
||A||_2 = \sqrt {tr (A^TA)} = \sqrt{\sum_{i=1}^m\sum_{j=1}^na_{ij}^2}</script><p>$tr$代表矩阵的迹函数$trace$（主对角线元素求和，$A^TA$一定是个方阵）。</p>
<p>给定一组标准正交基$\pmb w_1,\pmb w_2,…,\pmb w_N,\pmb w_i \in \mathbb{R}^n$，$\mathbb{R}^n$中的某个向量$\pmb a$可以表示为：</p>
<script type="math/tex; mode=display">
\pmb a = \sum_{j = 1}^n <\pmb a,\pmb w_j>\pmb w_j</script><p>$&lt;,&gt;$表示向量的内积操作，可以看出$&lt;\pmb a,\pmb w_j&gt;$即$\pmb a$在空间中的坐标。现在我们考虑，是否可以将$\pmb a$表示在由$\pmb w_1,\pmb w_2,…,\pmb w_m,m \mathbb{&lt;&lt;} n$，生成的子空间$\mathbb{R}^m$中，使得$&lt;\pmb a,\pmb w_{m+1}&gt;=0,…,&lt;\pmb a, \pmb w_n&gt; = 0$ ?也就是说只需要前$m$个基就可以表示$\pmb a$。</p>
<p><strong>降维准则：</strong> 找到一组基，使得数据在这组基表示下，$m+1,…n$维的坐标都是0。</p>
<p><strong>注意：</strong>$\pmb w_i$仍然是包含$n$个分量的向量，$m$个包含$n$个元素基向量构成了$n$维空间的一个$m$维子空间（空间的维数等于基向量个数）。</p>
<p>这就可以将问题转化为一个优化问题：</p>
<script type="math/tex; mode=display">
\min_{\pmb w_1,\pmb w_2,...,\pmb w_m} = \sum_{i=1}^N ||\pmb a_i - \sum_{j=1}^m<\pmb a_i,\pmb w_j>\pmb w_j||_2^2\\
s.t.<\pmb w_i,\pmb w_j> = \delta_{ij},i=j,\delta_{ij}=1;i\not=j,\delta_{ij}=0</script><p>写为矩阵形式即：</p>
<script type="math/tex; mode=display">
\arg \min_{\pmb w_1,\pmb w_2,...,\pmb w_m} = ||A-WW^TA||_2^2\\
s.t. W^TW = I_{m \times m}</script><p>其中，$A$为数据的特征矩阵，$W_{n \times m}$为前$m$个正交基按列组成的矩阵，它是半正交的。</p>
<p><strong>tips：</strong>因为有$m$个基向量，两两相乘，最后矩阵是一个$m \times m$的单位阵。</p>
<script type="math/tex; mode=display">
||A-WW^TA||_2^2 = tr\{(A-WW^TA)^T(A-WW^TA)\} = tr(A^TA) - tr(A^TWW^TA)</script><p>由于$tr(A^TA)$这一项与优化目标无关，因此目标函数变为：</p>
<script type="math/tex; mode=display">
\arg\min_{W}-tr(A^TWW^TA) = -tr\{(W^TA)^T(W^TA)\} = -||W^TA||^2_2\\s.t: W^TW = I</script><p>求出最忧$W$后，降维数据：</p>
<script type="math/tex; mode=display">
Y_{m \times N} = W_{m \times n}^TX_{n\times N}</script><p>以上推导过程从<strong>基向量</strong>、最优化角度解释了主成分分析算法。</p>
<p><strong>注意：</strong>主成分分析是无监督、线性的降维方法。线性判别分析是有监督的线性降维方法。</p>
<p><strong>说明：</strong>降维后的空间与原空间没有对应关系，主成分是原始维度的线性组合得到。</p>
<h4 id="1-2-算法描述"><a href="#1-2-算法描述" class="headerlink" title="1.2 算法描述"></a>1.2 算法描述</h4><p><img src="https://img-blog.csdnimg.cn/20191109182949751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05FRlVaWUo=,size_16,color_FFFFFF,t_70" alt="img"></p>
<p><img src="https://img-blog.csdnimg.cn/20191109191048929.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05FRlVaWUo=,size_16,color_FFFFFF,t_70" alt="img"></p>
<h4 id="1-3-sklearn中的PCA"><a href="#1-3-sklearn中的PCA" class="headerlink" title="1.3 sklearn中的PCA"></a>1.3 sklearn中的PCA</h4><p>PCA位于$sklearn.decomposition$包下。</p>
<p>注意“explained_variance_ratio_”这个指标，表示了原始数据在不同主成分上的方差占比（保留了多少原始数据的方差信息）。</p>
<p>扩展的PCA：Incremental PCA，当数据量特别大无法一次性加载至内存进行奇异值分解时，可以将数据划分为多个mini-batch，然后进行降维。</p>
<p>Kernel PCA：非线性降维。</p>
<p><strong>Question: Can PCA  be used to reduce the dimensionality of a highly nonlinear dataset？</strong></p>
<p><strong>Answer</strong>: PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear, because it can at least get rid of useless dimensions. However, if there are no useless dimensions — for example, the Swiss roll — then reducing dimensionality with PCA will lose too much information. You want to unroll the Swiss roll, not squash it</p>
<h3 id="2-非负矩阵分解"><a href="#2-非负矩阵分解" class="headerlink" title="2. 非负矩阵分解"></a>2. 非负矩阵分解</h3><p>非负矩阵分解：Nonnegative Matrix Factorization</p>
<script type="math/tex; mode=display">
\min ||X - WH||^2\\
s.t.W>=0,H>=0</script><h3 id="3-局部线性嵌入"><a href="#3-局部线性嵌入" class="headerlink" title="3. 局部线性嵌入"></a>3. 局部线性嵌入</h3><p>局部线性嵌入：Locally Linear Embedding（LLE），<strong>属于非线性降维</strong></p>
<p>参考： <a href="https://www.cnblogs.com/pinard/p/6266408.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6266408.html</a></p>
<p>流形学习（manifold learning）第一次听说”manifold”是在学习生成对抗网络的时候看到的。</p>
<p>推导过程中需要注意的地方：</p>
<p>下图中由$(1)\rightarrow(2)$过程中，因为$\sum_{j\in Q(i)}w_{ij} = 1$，所以$x_i = x_i \times 1 = \sum_{j\in Q(i)}w_{ij}x_i$。</p>
<p><img src="E:\hexo\themes\ayer\source\images\image-20201013195547190.png" alt="image-20201013195547190"></p>
<p><strong>算法描述</strong>：</p>
<h3 id="4-线性判别分析"><a href="#4-线性判别分析" class="headerlink" title="4. 线性判别分析"></a>4. 线性判别分析</h3><p>线性判别分析：Linear Discriminate Analysis（LDA）</p>
<p>降维原则：<strong>投影后</strong>最小化类内方差，最大化类间方差。属于线性、有监督的降维方法。</p>
<p>参考：<a href="https://www.jianshu.com/p/13ec606fdd5f" target="_blank" rel="noopener">https://www.jianshu.com/p/13ec606fdd5f</a> （二分类为例）</p>
<p>样本集：$\{(\pmb x_1,y_1),(\pmb x_2,y_2),…,(\pmb x_N,y_N)\},\pmb x_i \in \mathbb{R}^n$形成的样本矩阵$X_{n \times N}$，降维后数据$\pmb y$是一个$N \times 1$的向量</p>
<p>各个类的均值向量：</p>
<script type="math/tex; mode=display">
\pmb \mu_0 = \frac{1}{N_0}\sum_{\pmb x\in {0}}\pmb x\\
\pmb \mu_1 = \frac{1}{N_1}\sum_{\pmb x\in {1}}\pmb x</script><p>降维后的均值向量：</p>
<script type="math/tex; mode=display">
\tilde {\pmb \mu_0} = \frac{1}{N_0}\sum_{\pmb y\in 0}\pmb y =\frac{1}{N_0}\sum_{\pmb y\in 0}\pmb w^T\pmb x = \pmb w^T\pmb \mu_0</script><p>$\pmb w_{n \times 1}$就是我们要求的降维变换，同理：</p>
<script type="math/tex; mode=display">
\tilde {\pmb \mu_1} = \pmb w^T\pmb \mu_1</script><p>类内方差：</p>
<script type="math/tex; mode=display">
\tilde s_i^2 = \sum (\pmb y - \tilde{\pmb {\mu_i})^2}) = \sum (\pmb w^T\pmb x - \pmb w^T\pmb \mu_i)^2</script><p>即：</p>
<script type="math/tex; mode=display">
\tilde s_i^2 = \sum_{\pmb x\in i}[\pmb w^T(\pmb x - \pmb \mu_i)]^2 = \sum_{\pmb x\in i} [\pmb w^T(\pmb x-\pmb \mu_i)]^T[\pmb w^T(\pmb x-\pmb \mu_i)] = \sum_{\pmb x\in i} (\pmb x -\pmb \mu_i)^T\pmb w\pmb w^T(\pmb x-\pmb \mu_i)\\=\sum_{\pmb x\in i}\pmb w^T(\pmb x - \pmb \mu_i)(\pmb x - \pmb \mu_i)^T\pmb w</script><p><strong>说明：</strong>因为$(\pmb x -\pmb \mu_i)^T\pmb w$与$\pmb w^T(\pmb x-\pmb \mu_i)$ 结果分别是两个实数，可以交换。</p>
<p>令</p>
<script type="math/tex; mode=display">
\sum_{\pmb x\in i}(\pmb x - \pmb \mu_i)(\pmb x - \pmb \mu_i)^T = S_i</script><p>根据投影后类内方差小（$\tilde s_1^2 + \tilde s_0^2$ 两个都要小总体才能小），类间方差大（$||\pmb \mu_1 - \pmb \mu_2||^2$）。可以写出目标函数：</p>
<script type="math/tex; mode=display">
\max J(\pmb w) = \max \frac{||\pmb \mu_1 - \pmb \mu_2||^2}{\tilde s_1^2 + \tilde s_0^2}</script><p>并且根据上面的式子有：</p>
<script type="math/tex; mode=display">
\tilde s_1^2 + \tilde s_0^2 = \pmb w^T(S_0 + S_1)\pmb w =\pmb w^TS_W\pmb w = \tilde S_w</script><p>$S_W$：类内方差（with class）维度：$n \times n$</p>
<p>对于分子，我们有：</p>
<script type="math/tex; mode=display">
||\pmb \mu_1 - \pmb \mu_2||^2 = (\pmb w^T\pmb \mu_1 - \pmb w^T\pmb \mu_2)^2=\pmb w^T(\pmb \mu_1-\pmb \mu_2)(\pmb \mu_1-\pmb \mu_2)^T\pmb w</script><p>令：</p>
<script type="math/tex; mode=display">
(\pmb \mu_1-\pmb \mu_2)(\pmb \mu_1-\pmb \mu_2)^T = S_B</script><p>$S_B$：类间方差（between class）维度：$n \times n$</p>
<p>所以：</p>
<script type="math/tex; mode=display">
||\pmb \mu_1 - \pmb \mu_2||^2  = \pmb w^TS_B\pmb w = \tilde S_B</script><p>目标函数变为：</p>
<script type="math/tex; mode=display">
\max_{\pmb w} \frac{\pmb w^TS_B\pmb w}{\pmb w^TS_W\pmb w }</script><p>对$J(\pmb w)$对$\pmb w$求导并令其等于0即：</p>
<script type="math/tex; mode=display">
2S_B\pmb w(\pmb w^TS_W\pmb w)-2S_W\pmb w(\pmb w^TS_B\pmb w) = 0</script><p>注意：$\pmb w^TS_W\pmb w,\pmb w^TS_B\pmb w$结果都是实数，因此等式两边同除$\pmb w^TS_B\pmb w$可得：</p>
<script type="math/tex; mode=display">
S_W\pmb w =\frac{\pmb w^TS_W\pmb w}{\pmb w^TS_B\pmb w} S_B\pmb w</script><script type="math/tex; mode=display">
\pmb w = CS_W^{-1}S_B\pmb w</script><p>注意到：</p>
<script type="math/tex; mode=display">
S_B\pmb w =(\pmb \mu_1-\pmb \mu_2)(\pmb \mu_1-\pmb \mu_2)^T\pmb w</script><p>且$(\pmb \mu_1-\pmb \mu_2)^T\pmb w$结果是一个实数，C也是实数，我们要求的最关键的是$\pmb w$的方向，这些实数并不会改变方向，因此</p>
<script type="math/tex; mode=display">
\pmb w^* = S_W^{-1}(\pmb \mu_1 - \pmb \mu_2)</script><h3 id="5-多维尺度分析"><a href="#5-多维尺度分析" class="headerlink" title="5. 多维尺度分析"></a>5. 多维尺度分析</h3><p>Multidimensional Scaling</p>
<p>基本思想：尽量满足原始高维度空间中样本之间的距离在低维空间中得以保持。</p>
<p>“metric” or “non-metric”</p>
<p>给定样本$(\pmb x_1,\pmb x_2,…,\pmb x_N),\pmb x_i \in \mathbb{R}^n$构成的样本矩阵$X_{n \times N}$，降维后的样本矩阵$Y_{m \times N}$。根据多维尺度分析的基本思想：保持样本间距离不变，原始样本间距离$d_{ij}=||\pmb x_i - \pmb x_j||$构成的矩阵$D_{N \times N}$。</p>
<p>可知$X^TX = -\frac{1}{2}HD_xH, Y^TY = \frac{1}{2}HD_yH$，我们想让$D_x=D_y$，等价的可以通过以下目标函数来求得：</p>
<p>目标函数：</p>
<script type="math/tex; mode=display">
\min ||X^TX - Y^TY||^2</script><p>特征值分解：</p>
<script type="math/tex; mode=display">
X^TX = U^T\Lambda U</script><p>那么降维后的数据$Y$：</p>
<script type="math/tex; mode=display">
Y = \Lambda^{1/2}U</script><h3 id="6-典型关联分析"><a href="#6-典型关联分析" class="headerlink" title="6. 典型关联分析"></a>6. 典型关联分析</h3><p>Canonical correlation analysis</p>
<p>参考：<a href="https://www.cnblogs.com/pinard/articles/6288716.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/articles/6288716.html</a></p>
<p>具体思想：相关系数 $\rho$ 可以分析两组一维数据$X,Y$的线性相关性，$\rho$取值越接近1则$X,Y$的线性相关性越高。虽然相关系数可以很好的帮我们分析一维数据的相关性，但是对于高维数据就不能直接使用了。比如$X$是2维数据，$Y$是三维数据，就不能用相关系数进行分析。那么有没有变通的方法呢？典型关联分析给出了思路，具体就是将多维的$X,Y$分别用线性变换为1维的$X^\prime,Y^\prime$，然后使用相关系数分析1维$X^\prime,Y^\prime$的相关性。问题又来了，如何把$X,Y$变换为$X^\prime，Y^\prime$呢？也就是说降维的准则是什么？<strong>典型关联分析使用的准则是变换到1维后，$X^\prime,Y^\prime$的相关系数最大。</strong></p>
<p>亦即：</p>
<script type="math/tex; mode=display">
arg \max_{\pmb a,\pmb b} ||X^T\pmb a - Y^T\pmb b||^2</script><p><strong>算法描述</strong>：</p>
<h3 id="7-字典学习"><a href="#7-字典学习" class="headerlink" title="7. 字典学习"></a>7. 字典学习</h3><p>参考：<a href="https://zhuanlan.zhihu.com/p/46085035" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46085035</a></p>
<p><strong>基本思想：</strong></p>
<p>在人类发展的近几千年历史中，文字对人类文明的推动起着举足轻重的作用。人类用文字记述了千年的历史，用文字留下了各种思想火花，用文字抒发了各种各样的情感等等。但是这一切的内容，只需要一本字典就能表述完。因为人在这环节中的功能，无非就是使用字典当中的字词进行了适当的排列了而已。</p>
<p>基于这种思想，先前的大佬提出了字典学习——Dictionary Learning。</p>
<p>字典学习的目标，就是提取事物<strong>最本质的特征（类似于字典当中的字或词语）。</strong>如果我们能都获取这本包括<strong>最本质的特征</strong>的字典，那我们就掌握了这个事物的最本质的内涵。换言之，字典学习将我们的到的对于物体的信息降维，减少了该物体一些无关紧要信息对我们定义这个物体的干扰。</p>
<p><strong>模型建立：</strong></p>
<script type="math/tex; mode=display">
\pmb x = \sum_{i=1}^m \lambda_i\pmb w_i</script><p>$\lambda_i$称为表示系数，要求尽可能稀疏；$\pmb w_i$被称为”字典“。</p>
<p>问题是如何找到表示系数与字典？可转化为如下优化问题：</p>
<script type="math/tex; mode=display">
\min \sum_{i=1}^N||\pmb x_i - \sum_{j=1}^m y_{ij}\pmb w_j||^2 \\
s.t. ||\pmb y_i|| <= C</script><p>其中：$y$即为表示系数，约束条件对应要求表示系数尽可能稀疏。$m &gt;&gt;N$。</p>
<p>写成矩阵形式（有点类似非负矩阵分解形式，W是基，Y是系数）：</p>
<script type="math/tex; mode=display">
\min ||X - WY||^2\\
s.t.||Y|| <= C</script><p>由于上述优化问题含有两个优化变量，可采用坐标优化方法，即先固定系数$y$，求出最优的$\pmb w$；然后在求出系数。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>机器视觉基础Lecture 2</title>
    <url>/2020/10/08/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80Lecture%202/</url>
    <content><![CDATA[<h2 id="机器视觉基础-Lecture-2"><a href="#机器视觉基础-Lecture-2" class="headerlink" title="机器视觉基础 Lecture 2"></a>机器视觉基础 Lecture 2</h2><h3 id="0-数字图像基础"><a href="#0-数字图像基础" class="headerlink" title="0. 数字图像基础"></a>0. 数字图像基础</h3><h4 id="0-1-相机的成像原理"><a href="#0-1-相机的成像原理" class="headerlink" title="0.1 相机的成像原理"></a>0.1 相机的成像原理</h4><p>数码相机使用了感光器件（CMOS/CCD），将光信号转变为电信号（模拟信号），再经模/数转换后形成数字信号，在经过一系列处理形成特定的图像文件格式存储于存储卡上。</p>
<h4 id="0-2-图像的采样和量化"><a href="#0-2-图像的采样和量化" class="headerlink" title="0.2 图像的采样和量化"></a>0.2 图像的采样和量化</h4><p>采样，就是把一幅连续图像在空间上分割成M×N个网格，每个网格用一亮度值来表示。一般来说，采样间隔越大，所得图像像素数越少，空间分辨率低，质量差，严重时出现马赛克效应；采样间隔越小，所得图像像素数越多，空间分辨率高，图像质量好，但数据量大。</p>
<p>量化，就是把采样点上对应的亮度连续变化区间转换为单个特定数码的过程。量化后，图像就被表示成一个整数矩阵。每个像素具有两个属性：位置和灰度。位置由行、列表示。灰度表示该像素位置上亮暗程度的整数。此数字矩阵M×N就作为计算机处理的对象了。灰度级一般为0－255（8bit量化）。</p>
<h4 id="0-3-数字图像和信号处理"><a href="#0-3-数字图像和信号处理" class="headerlink" title="0.3 数字图像和信号处理"></a>0.3 数字图像和信号处理</h4><p>图片也是一种特殊的信号。</p>
<h4 id="0-4-颜色空间"><a href="#0-4-颜色空间" class="headerlink" title="0.4 颜色空间"></a>0.4 颜色空间</h4><p>RGB、HSV、Lab、CIE等等。</p>
<h4 id="0-5-图像的直方图"><a href="#0-5-图像的直方图" class="headerlink" title="0.5 图像的直方图"></a>0.5 图像的直方图</h4><h3 id="1-图像滤波"><a href="#1-图像滤波" class="headerlink" title="1. 图像滤波"></a>1. 图像滤波</h3><h4 id="1-1-滤波器"><a href="#1-1-滤波器" class="headerlink" title="1.1 滤波器"></a>1.1 滤波器</h4><p>滤波器英文filter/mask/kernel。可分为线性滤波器和非线性滤波器，高通滤波器（只允许图像中高频成分通过，换句话说滤掉低频成分）和低通滤波器。</p>
<p>均值滤波（低通滤波）、高斯滤波（低通滤波，重要的两个参数：kernel size和方差$\sigma^2$）、中值滤波（高通滤波，非线性，会锐化sharpen图片，比如增强边缘）、拉普拉斯滤波。</p>
<p>窗口滑动时如何处理边界问题：full、same、valid。</p>
<p>低通滤波会不同程度的平滑图片。</p>
<p>高频成分：图像中的边缘、噪声。</p>
<p>本质上：采用一个kernel然后在图像上滑动加权求和，因此<strong>kernel的大小和权重</strong>很重要。</p>
<h4 id="1-2-梯度与边缘检测"><a href="#1-2-梯度与边缘检测" class="headerlink" title="1.2 梯度与边缘检测"></a>1.2 梯度与边缘检测</h4><p>由于图像边缘处像素值通常变化剧烈，由此形成像素值上的不连续，因此可以通过求梯度来检测图像中的边缘。将图像看作一个二维离散函数$f(x,y)$，图像梯度其实就是这个二维离散函数的求导。</p>
<p>正常微分定义：</p>
<script type="math/tex; mode=display">
\frac{\partial f(x,y)}{\partial x} = \lim_{\epsilon \rightarrow 0} \frac{f(x + \epsilon,y) - f(x,y)}{\epsilon},\frac{\partial f(x,y)}{\partial y} = \lim_{\epsilon \rightarrow 0} \frac{f(x,y  + \epsilon) - f(x,y)}{\epsilon}</script><p>但是图像是按照像素来离散的，最小也就1像素，因此普通的微分形式变成了：</p>
<script type="math/tex; mode=display">
\frac{\partial f(x,y)}{\partial x} = f(x + 1,y) - f(x,y),\frac{\partial f(x,y)}{\partial y} =f(x,y+1) - f(x,y)</script><p><strong>可以看出，图像的梯度本质上相当于2个相邻像素之间的差值。</strong></p>
<p>因此定义梯度算子（kernel）：水平$[-1,1]$ 右边像素 - 左边像素；垂直：$[-1,1]^T$下边像素-上边像素</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">"126007.jpg"</span>,<span class="number">0</span>)</span><br><span class="line">img = img.astype(<span class="string">'float'</span>)</span><br><span class="line"></span><br><span class="line">row = img.shape[<span class="number">0</span>]</span><br><span class="line">col = img.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">gradient_x = np.zeros((row, col))</span><br><span class="line">gradient_y = np.zeros((row, col))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(row - <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(col - <span class="number">1</span>):</span><br><span class="line">        gy = abs(img[i + <span class="number">1</span>, j] - img[i, j]) <span class="comment">#下边像素-上边像素</span></span><br><span class="line">        gx = abs(img[i, j + <span class="number">1</span>] - img[i, j]) <span class="comment">#右边像素-左边像素</span></span><br><span class="line">        gradient_x[i, j] = gx</span><br><span class="line">        gradient_y[i, j] = gy</span><br><span class="line">gradient_x = gradient_x.astype(<span class="string">'uint8'</span>)</span><br><span class="line">gradient_y = gradient_y.astype(<span class="string">'uint8'</span>)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">"gradient_x"</span>, gradient_x)</span><br><span class="line">cv2.imshow(<span class="string">"gradient_y"</span>, gradient_y)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure>
<p><img src="/images/grad_x.jpg" alt=""></p>
<p>水平方向梯度</p>
<p><img src="/images/grad_y.jpg" alt=""></p>
<p>垂直方向梯度</p>
<p>水平方向求偏导：$\frac{\partial f}{\partial x}$，竖直方向求偏导：$\frac{\partial f}{\partial y}$，图像梯度：$\nabla f = [\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}]$。</p>
<p>水平：$\nabla f = [\frac{\partial f}{\partial x},0]$，垂直：$\nabla f = [0,\frac{\partial f}{\partial y}]$</p>
<p>梯度方向：$\theta = \tan^{-1}(\frac{\partial f}{\partial y}/\frac{\partial f}{\partial x})$</p>
<p><strong>梯度幅值（gradient magnitude）</strong>：$||\nabla f||=\sqrt {(\frac{\partial f}{\partial x})^2+(\frac{\partial f}{\partial y})^2}$</p>
<p>注意：求梯度之前先做平滑，否则噪声也是高频，求梯度后无法分辨哪里是噪声哪里是边缘。如下图。</p>
<p><img src="/images/image-20201008191252006.png" alt="image-20201008191252006" style="zoom:67%;" /></p>
<p><img src="/images/image-20201008191306409.png" alt="image-20201008191306409" style="zoom:67%;" /></p>
<p>首先平滑图像后：</p>
<p><img src="/images/image-20201008191404392.png" alt="image-20201008191404392" style="zoom:67%;" /></p>
<p>$f:signal;g:kernel$</p>
<script type="math/tex; mode=display">
\frac{d}{dx}(f*g) = f*\frac{d}{dx}g</script><p>用高斯一阶导去和信号做卷积，干了两件事：一是平滑信号，二是求导。相当于先用高斯滤波平滑再求导。 同理高斯二阶导，用高斯二阶导去和信号卷积。</p>
<ul>
<li>高斯核求和为1，用途：平滑</li>
<li>高斯一阶导核求和为0，用途：边缘提取</li>
</ul>
<p><strong>Canny边缘检测</strong></p>
<p>代码参见同专题下另一篇博客。</p>
<h4 id="1-3-模板匹配"><a href="#1-3-模板匹配" class="headerlink" title="1.3 模板匹配"></a>1.3 模板匹配</h4><p>滤波器的一个应用。</p>
<h3 id="2-图像的特征"><a href="#2-图像的特征" class="headerlink" title="2. 图像的特征"></a>2. 图像的特征</h3><h4 id="图像纹理texture"><a href="#图像纹理texture" class="headerlink" title="图像纹理texture"></a>图像纹理texture</h4><p>纹理分类</p>
<p>纹理是由重复的局部<strong>模式</strong>（local pattern）构成的。</p>
<p>如何描述纹理？最简单的就是用图像上x方向、y方向的梯度这两个维度来描述某一window的纹理特征；但这显然是不够的，因此可以使用多个不同的卷积核去与图像卷积，若采用 $d$ 个不同的卷积核，则特征向量的维度就是 $d$ 维。（可以看到卷积神经网络的前身，这里的卷积核是我们设计好的，神经网络中的核是学习到的）。下图中，使用48个卷积核，卷积过后可以得到48维的特征向量。（其实就是48张feature map）</p>
<p><img src="/images/kernels.png" alt="image-20201029180022688" style="zoom:80%;" /></p>
<p>不同的核卷积过后会得到不同的response。</p>
<p><img src="/images/texture-response.png" alt="image-20201029180511558"></p>
<p>注意：方块对应的颜色代表数值强弱，白色说明数值最大，黑色说明数值最小，这个数值是怎么来的呢？其实就是用一个模板卷积过后得到一幅图像，将卷积后图像中的数值全部相加再取<strong>平均</strong>得到一个数值（mean response）。上图中一幅图像就可以表示成7维的向量。</p>
]]></content>
      <categories>
        <category>机器视觉基础</category>
      </categories>
  </entry>
  <entry>
    <title>多元高斯及其极大似然估计</title>
    <url>/2020/10/06/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%8F%8A%E5%85%B6%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</url>
    <content><![CDATA[<h2 id="多元高斯及其极大似然估计"><a href="#多元高斯及其极大似然估计" class="headerlink" title="多元高斯及其极大似然估计"></a>多元高斯及其极大似然估计</h2><p>参考：<a href="https://blog.csdn.net/Joyliness/article/details/80097491" target="_blank" rel="noopener">https://blog.csdn.net/Joyliness/article/details/80097491</a></p>
<h3 id="1-独立多元高斯的概率密度函数"><a href="#1-独立多元高斯的概率密度函数" class="headerlink" title="1. 独立多元高斯的概率密度函数"></a>1. 独立多元高斯的概率密度函数</h3><p>$n$个独立的随机变量$(x_1,x_2,…x_n)$的联合密度函数为</p>
<script type="math/tex; mode=display">
f_{\mu,\Sigma}(\pmb x)=\frac{1}{\sqrt{(2\pi)^n\det\Sigma}}e^{-\frac{1}{2}(\pmb x - \pmb \mu)^T\Sigma^{-1}(\pmb x - \pmb \mu)}</script><p>就是多元高斯分布（Multivariate Normal Distribution，MVN）。其中，$\pmb \mu$是各个随机变量的期望值所组成的向量，$\Sigma$是随机变量间的协方差矩阵，是$n \times n$的正定矩阵。</p>
<h3 id="2-多元高斯的极大似然估计"><a href="#2-多元高斯的极大似然估计" class="headerlink" title="2. 多元高斯的极大似然估计"></a>2. 多元高斯的极大似然估计</h3><p>对于$N$个满足多元高斯分布的独立样本集：$\{\pmb x_1,\pmb x_2,..,\pmb x_N\},\pmb x \in  \mathbb{R}^n$，似然函数为：</p>
<script type="math/tex; mode=display">
\prod_{i=1}^Nf_{\mu,\Sigma}(\pmb x_i) = (2\pi)^{-\frac{Nn}{2}}|\Sigma|^{-\frac{N}{2}}e^{-\frac{1}{2}\sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)}</script><p>取对数：</p>
<script type="math/tex; mode=display">
\ln L(\pmb \mu,\Sigma) =\ln\prod_{i=1}^Nf_{\mu,\Sigma}(\pmb x_i) =-\frac{Nn}{2}\ln2\pi -\frac{N}{2}\ln |\Sigma| - \sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)\\
=C - \frac{N}{2}\ln |\Sigma| - \frac{1}{2}\sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)</script><p>其中$C = -\frac{Nn}{2}\ln2\pi$是一个与参数$\pmb \mu,\Sigma$ 无关的常数，因此：</p>
<script type="math/tex; mode=display">
arg\max_{\pmb \mu,\Sigma}\ln\prod_{i=1}^Nf_{\mu,\Sigma}(\pmb x_i)</script><ul>
<li>$\ln L(\pmb \mu,\Sigma)$对$\pmb \mu$求偏导（标量对向量求偏导）</li>
</ul>
<p>先将$\sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)$展开，得到：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N\pmb x_i^T\Sigma^{-1}\pmb x_i - 2\sum_{i=1}^N\pmb x^T\Sigma^{-1}\pmb \mu + N\pmb \mu^T\Sigma^{-1}\pmb \mu</script><p>后两项对$\pmb \mu$求偏导得到：</p>
<script type="math/tex; mode=display">
-2\sum_{i=1}^N\Sigma^{-1}\pmb x_i + 2N\Sigma^{-1}\pmb \mu</script><p>因此：</p>
<script type="math/tex; mode=display">
\frac{\partial \ln L(\pmb \mu,\Sigma)}{\partial \pmb \mu} = 2\sum_{i=1}^N\Sigma^{-1}\pmb x_i - 2N\Sigma^{-1}\pmb \mu = 0</script><p>求得：</p>
<script type="math/tex; mode=display">
\bar{\pmb \mu} = \frac{1}{N}\sum_{i=1}^N \pmb x_i</script><ul>
<li>$\ln L(\pmb \mu, \Sigma)$对$\Sigma$求偏导（标量对矩阵求偏导）</li>
</ul>
<p>tips：$\frac{\partial \det(X)}{\partial X } = \det (X)tr(X^{-1})$</p>
<script type="math/tex; mode=display">
\frac{\partial -\frac{N}{2}\ln \det(\Sigma)}{\partial \Sigma} = -\frac{N}{2}tr(X^{-1})</script><script type="math/tex; mode=display">
\frac{\partial \ln L(\pmb \mu, \Sigma)}{\partial \Sigma} = 0</script><p>解得：</p>
<script type="math/tex; mode=display">
\bar \Sigma = \frac{1}{N} \sum_{i=1}^N(\pmb x_i - \bar{\pmb \mu})(\pmb x_i - \bar{\pmb \mu})^T</script>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>Transformer模型介绍</title>
    <url>/2020/10/03/Transformer%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h2 id="Transformer模型介绍"><a href="#Transformer模型介绍" class="headerlink" title="Transformer模型介绍"></a>Transformer模型介绍</h2><p>文献：Vaswani A,  Shazeer N,  Parmar N,  et al.  Attention is all you need[C]. Advances in neural information processing systems. 2017: 5998-6008.</p>
<p>详解：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a>  The Annotated Transformer.</p>
<p>视频：<a href="https://www.youtube.com/watch?v=ugWDIIOHtPA" target="_blank" rel="noopener">https://www.youtube.com/watch?v=ugWDIIOHtPA</a>  李宏毅：Transformer</p>
<h3 id="1-encoder-decoder"><a href="#1-encoder-decoder" class="headerlink" title="1. encoder-decoder"></a>1. encoder-decoder</h3><h3 id="2-self-attention"><a href="#2-self-attention" class="headerlink" title="2. self attention"></a>2. self attention</h3><h3 id="3-multi-head-attention"><a href="#3-multi-head-attention" class="headerlink" title="3. multi head attention"></a>3. multi head attention</h3><h3 id="4-attention和self-attention区别"><a href="#4-attention和self-attention区别" class="headerlink" title="4. attention和self-attention区别"></a>4. attention和self-attention区别</h3>]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>贝叶斯垃圾邮件分类器</title>
    <url>/2020/09/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    <content><![CDATA[<h2 id="贝叶斯垃圾邮件分类器"><a href="#贝叶斯垃圾邮件分类器" class="headerlink" title="贝叶斯垃圾邮件分类器"></a>贝叶斯垃圾邮件分类器</h2><p>利用朴素贝叶斯算法，在给定训练数据上，训练一个垃圾分类器。有关朴素贝叶斯的基本思想，可以参见另一篇博客：<a href="https://zyj-nova.github.io/2020/02/09/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88Naive%20Bayes%EF%BC%89/" target="_blank" rel="noopener">https://zyj-nova.github.io/2020/02/09/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88Naive%20Bayes%EF%BC%89/</a></p>
<h3 id="0-数据集获取"><a href="#0-数据集获取" class="headerlink" title="0 数据集获取"></a>0 数据集获取</h3><p>链接：<a href="https://pan.baidu.com/s/1P3Jg67nerg63GVbCkc1YJg" target="_blank" rel="noopener">https://pan.baidu.com/s/1P3Jg67nerg63GVbCkc1YJg</a> ，提取码：576u 。</p>
<p>数据集为英文语料，分为训练集和测试集。训练集包含351封垃圾邮件（文件名标有“spmsg”）、351封正常邮件；测试集包含130封正常邮件和130封垃圾邮件。</p>
<h3 id="1-特征提取"><a href="#1-特征提取" class="headerlink" title="1 特征提取"></a>1 特征提取</h3><p>由于每一封邮件都不是数值型的特征，因此我们需要把他们编码。首先需要构建一个字典，统计所有邮件中单词出现的频率，同时删除字典中的非字母字符、长度为1的字符，之后取前3000个出现频率最高的作为最终的字典。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_dim = <span class="number">3000</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_Dictionary</span><span class="params">(train_dir)</span>:</span></span><br><span class="line">    emails = [os.path.join(train_dir, f) <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(train_dir)]</span><br><span class="line">    all_words = []</span><br><span class="line">    <span class="keyword">for</span> mail <span class="keyword">in</span> emails:</span><br><span class="line">        <span class="keyword">with</span> open(mail) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment"># 遍历每一行</span></span><br><span class="line">            <span class="keyword">for</span> i,line <span class="keyword">in</span> enumerate(f):</span><br><span class="line">                <span class="keyword">if</span>(i == <span class="number">2</span>):</span><br><span class="line">                    <span class="comment">#正文开始</span></span><br><span class="line">                    words = line.split()<span class="comment">#默认按照空格分词</span></span><br><span class="line">                    all_words += words</span><br><span class="line">    dictionary = Counter(all_words)</span><br><span class="line">    <span class="comment"># 词典构建好，删除多余的词汇</span></span><br><span class="line">    list_to_remove = list(dictionary.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> list_to_remove:</span><br><span class="line">        <span class="keyword">if</span> item.isalpha() == <span class="literal">False</span>:</span><br><span class="line">            <span class="keyword">del</span> dictionary[item]</span><br><span class="line">        <span class="keyword">elif</span> len(item) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">del</span> dictionary[item]</span><br><span class="line">    dictionary = dictionary.most_common(n_dim)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dictionary</span><br></pre></td></tr></table></figure>
<p>构建完字典以后，就可以对每一封邮件进行编码了。特征向量的每一个分量即邮件中出现的对应词频。例如邮件“do over，do again”可以被编码为$[0,0,…,2,…,1,…,1]$。每一个分量都代表对应的词频。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将每个邮件转化为词频向量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_features</span><span class="params">(mail_dir)</span>:</span></span><br><span class="line">    </span><br><span class="line">    files = [os.path.join(mail_dir, f) <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(mail_dir)]</span><br><span class="line">    labels = np.ones(len(files))</span><br><span class="line">    feature_matrix = np.zeros((len(files), n_dim))</span><br><span class="line">    docID = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        <span class="keyword">with</span> open(file) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">if</span> f.name[<span class="number">30</span>:].find(<span class="string">"sp"</span>) != <span class="number">-1</span>:</span><br><span class="line">                <span class="comment">#垃圾邮件</span></span><br><span class="line">                labels[docID] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(f):</span><br><span class="line">                <span class="keyword">if</span> i == <span class="number">2</span>:</span><br><span class="line">                    words = line.split()</span><br><span class="line">                    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">                        wordID = <span class="number">0</span></span><br><span class="line">                        <span class="keyword">for</span> i,d <span class="keyword">in</span> enumerate(dictionary):</span><br><span class="line">                            <span class="keyword">if</span>(d[<span class="number">0</span>] == w):</span><br><span class="line">                                wordID = i</span><br><span class="line">                                feature_matrix[docID, wordID] += <span class="number">1</span></span><br><span class="line">        docID += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> feature_matrix, labels</span><br></pre></td></tr></table></figure>
<h3 id="2-计算概率"><a href="#2-计算概率" class="headerlink" title="2 计算概率"></a>2 计算概率</h3><p>$P(word|y =spam) = P(word, spam) / P(spam)$，$P(word, spam)$即每个单词在垃圾邮件中出现的频数，$P(spam)$即垃圾邮件中所有单词的词频总和。正常邮件同理，得到概率矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计垃圾邮件中的所有词频和，以及各个词的频数</span></span><br><span class="line">spam_words = <span class="number">0</span></span><br><span class="line">non_spam_words = <span class="number">0</span></span><br><span class="line">spam_mails = []</span><br><span class="line">non_spam_mails = []</span><br><span class="line"><span class="keyword">for</span> i,line <span class="keyword">in</span> enumerate(feature_matrix):</span><br><span class="line">    <span class="keyword">if</span> labels[i] == <span class="number">1</span>:</span><br><span class="line">        non_spam_words += int(feature_matrix[i].sum())</span><br><span class="line">        non_spam_mails.append(feature_matrix[i])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        spam_mails.append(feature_matrix[i])</span><br><span class="line">        spam_words += int(feature_matrix[i].sum())</span><br><span class="line">print(<span class="string">"spam total words are &#123;&#125;, non-spam total words are &#123;&#125;"</span>.format(spam_words, non_spam_words))</span><br><span class="line"></span><br><span class="line">spam_mails = np.array(spam_mails)</span><br><span class="line">non_spam_mails = np.array(non_spam_mails)</span><br><span class="line"></span><br><span class="line">weight_matrix = np.zeros((<span class="number">2</span>,n_dim))</span><br><span class="line">weight_matrix[<span class="number">0</span>, :] = spam_mails.sum(axis = <span class="number">0</span>) / spam_words</span><br><span class="line">weight_matrix[<span class="number">1</span>, :] = non_spam_mails.sum(axis = <span class="number">0</span>) / non_spam_words</span><br></pre></td></tr></table></figure>
<p>为了避免概率矩阵中出现0的情况，使得计算概率时出现乘0，最后让权重矩阵每个分量加一个非常小的扰动</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sigma = <span class="number">1e-8</span></span><br><span class="line">weight_matrix += sigma</span><br></pre></td></tr></table></figure>
<h3 id="3-模型预测与评估"><a href="#3-模型预测与评估" class="headerlink" title="3 模型预测与评估"></a>3 模型预测与评估</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred_labels = []</span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> test_feature:</span><br><span class="line">    spam_prob = <span class="number">1.0</span></span><br><span class="line">    non_spam_prob = <span class="number">1.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i,w <span class="keyword">in</span> enumerate(feature):</span><br><span class="line">        <span class="keyword">if</span> w != <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 计算P(w|spam) P(w|non-spam)</span></span><br><span class="line">            spam_prob += np.log10(weight_matrix[<span class="number">0</span>, i])</span><br><span class="line">            non_spam_prob += np.log10(weight_matrix[<span class="number">1</span>, i])</span><br><span class="line">    print(spam_prob, non_spam_prob)</span><br><span class="line">    <span class="keyword">if</span> spam_prob &gt; non_spam_prob:</span><br><span class="line">        pred_labels.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pred_labels.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>在计算概率的时候用到了一个trick，由于非常小的数不断连乘最后会趋于0，因此利用取对数的方法将乘法化为加法（极大似然估计就是这么干的）。同时取对数时不可以为0，上面加的非常小的扰动是很有必要的。</p>
<p>模型在测试集上的准确率、召回率、f1_score如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">recall_score : <span class="number">0.9923076923076923</span></span><br><span class="line">accuracy_score : <span class="number">0.9846153846153847</span></span><br><span class="line">f1_score : <span class="number">0.9847328244274809</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>Strang教授线性代数笔记</title>
    <url>/2020/09/23/Strang%E6%95%99%E6%8E%88%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h3 id="Strang教授线性代数笔记"><a href="#Strang教授线性代数笔记" class="headerlink" title="Strang教授线性代数笔记"></a>Strang教授线性代数笔记</h3><p>具体参见：<a href="https://github.com/zyj-nova/LinearAlgebraNotes" target="_blank" rel="noopener">https://github.com/zyj-nova/LinearAlgebraNotes</a></p>
]]></content>
  </entry>
  <entry>
    <title>浅谈逻辑斯蒂回归模型</title>
    <url>/2020/08/03/%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h2 id="浅谈逻辑斯蒂回归模型"><a href="#浅谈逻辑斯蒂回归模型" class="headerlink" title="浅谈逻辑斯蒂回归模型"></a>浅谈逻辑斯蒂回归模型</h2><h3 id="sigmod函数"><a href="#sigmod函数" class="headerlink" title="sigmod函数"></a>sigmod函数</h3><p>​        $sigmod$函数及其导数形式如下：</p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">
f^\prime (x)= f(x)*(1-f(x))</script><p>​        逻辑斯蒂回归或者是$soft\max$回归，说是回归其实是在解决二分类或者多分类的问题。逻辑斯蒂模型如下：</p>
<script type="math/tex; mode=display">
y = \frac{1}{1 + e^{-（w^Tx +b)}}</script><p>​        其中，$x \in R^n, w \in R^n$，$b$为偏置bias可以设置为0，$x$为样本的特征向量，$w$即要学习的参数。在给定一批有label的训练数据集后，我们可以根据极大似然估计或者梯度下降来得到最优的参数值，之后，给定某个测试数据，当：$y &gt;= 0.5$，我们就把这个样本划为1类，否则就划为0类。</p>
<p>​        其实，完整的二项逻辑斯蒂回归模型是定义如下的条件概率分布：</p>
<script type="math/tex; mode=display">
P(Y = 1|x) = \frac{\exp(w*x + b)}{1 + \exp(w*x + b)}</script><script type="math/tex; mode=display">
P(Y = 0|x) = \frac{1}{1 + \exp(w*x + b)}</script><p>​        观察上述条件概率计算公式我们发现，当$w<em>x+b &gt; 0$的时候，$\exp(w</em>x+b)&gt;1,P(Y=1)$的概率一定高于$P(Y=0)$，我们就把其划分为1类。对于$P(Y=1)$的分布函数上下同除以分子$\exp$就得到了熟悉的逻辑斯蒂函数。</p>
<p>​        事实上，逻辑斯蒂函数是在计算样本为1类的概率，由于是二分类，不是1类就是0类。</p>
<p>​        <strong>本质上，逻辑斯蒂回归是一个线性分类器，以二维特征为例，如果不对特征进行任何组合，不使用核函数，其决策平面始终是一条直线：$w^Tx = 0$</strong>。</p>
<p><img src="/images/make_moons.png" alt=""></p>
<p>​        以make moons生成的数据集为例，不对特征进行任何处理，决策平面如下：</p>
<p><img src="/images/log_bundary2.png" alt=""></p>
<p>​        若对两维特征进行平方处理，绘制的决策平面如下：</p>
<p><img src="/images/log_bundary1.png" alt=""></p>
<p>​        谈到特征融合，决策树就可以拿来做特征融合（从根节点到叶子节点的每一条路径都是一个组合特征）。</p>
<h3 id="softmax回归模型"><a href="#softmax回归模型" class="headerlink" title="softmax回归模型"></a>softmax回归模型</h3><p>参考：<a href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" target="_blank" rel="noopener">https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/</a></p>
<p><strong>softmax函数</strong></p>
<script type="math/tex; mode=display">
\sigma (\pmb x)_i = \frac{e^{x_i}}{\sum_j^n e^{x_j}},i = 1,...,n,\pmb x=(x_1,...,x_n)</script><p>定义：</p>
<script type="math/tex; mode=display">
S_j = \frac{e^{x_j}}{\sum_{k=1}^ne^{x_k}}, j = 1,2,...,n</script><p>​        可以看到，softmax函数输入一个向量，输出是一个向量，是一个$\R^n \rightarrow \R^n$的函数。下面讨论其输入的各个分量的偏导，可以看到其输出分量对输入分量的偏导构成了一个$n \times n$的雅可比矩阵。</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{\partial S_i}{\partial x_j}</script><script type="math/tex; mode=display">
DS = \begin{pmatrix}D_1S_1&D_1S_2&...&D_1S_n\\...&...&...&...\\D_nS_1&..&...&D_nS_n\end{pmatrix}</script><p>现在计算$D_jS_i$:</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{\partial S_i}{\partial x_j} ,S_i =\frac{e^{x_i}}{\sum_{k=1}^ne^{x_k}}</script><p>$S_i$是个复合函数</p>
<script type="math/tex; mode=display">
S_i = \frac{g(x)}{h(x)},g(x) = e^{x_i},h(x)=\sum_{k=1}^ne^{x_k}</script><p>根据求导的除法法则：</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{g^\prime(x)h(x)-h^\prime(x)g(x)}{h^2(x)}</script><p>这里需要分类讨论，即$i =j$和$i\not=j$的情况：$i=j$时，$g^\prime(x_j)=e^{x_j}=e^{x_i},h^\prime(x_i)=e^{x_i} = e^{x_j}$；否则$i\not=j$，$g^\prime(x_i)=0,h^\prime(x_i)=e^{x_j}$.</p>
<p>$i=j$时带入得：</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{e^{x_i}\Sigma - e^{x_j}e^{x_i}}{\Sigma^2} = \frac{e^{x_i}}{\Sigma}\frac{\Sigma - e^{x_j}}{\Sigma} = S_i(1-S_j)</script><p>$i\not=j$时，带入得：</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{0 - e^{x_i}e^{x_j}}{\Sigma^2} = -S_iS_j</script><p>综合以上讨论，有：</p>
<script type="math/tex; mode=display">
D_jS_i = S_i(\delta_{ij}-S_j)</script><script type="math/tex; mode=display">
i = j,\delta_{ij} = 1;i \not=j,\delta_{ij}=0</script><p><strong>softmax回归模型</strong>        </p>
<p>​        值得注意的是，这里讲的softmax回归与softmax函数还是有一点不同的。softmax函数只是把给定的输入做了一个指数归一化；相比logistic回归模型的参数只是一个向量，softmax回归模型包含参数矩阵$W$，也就是每一个类别都对应一个n维向量，n为输入特征数量。</p>
<p>假设有$k$个类别，则权重矩阵：</p>
<script type="math/tex; mode=display">
W_{k \times n} = \begin{pmatrix}\pmb w_1^T\\...\\\pmb w_k^T\end{pmatrix}</script><p>输入为第$j$个类别的概率为：</p>
<script type="math/tex; mode=display">
P(y = j | \pmb x) = \frac{e^{\pmb x^T\pmb w_j}}{\sum_{i=1}^k e^{\pmb x^T\pmb w_i}}</script><p>模型针对给定输入预测类别的过程如下图所示：</p>
<p><img src="https://eli.thegreenplace.net/images/2016/softmax-layer-generic.png" alt=""></p>
<p>注：T表示类别数量。</p>
<h3 id="二者的关联"><a href="#二者的关联" class="headerlink" title="二者的关联"></a>二者的关联</h3><p>当$k = 2$的时候，</p>
<script type="math/tex; mode=display">
P(y = 1) = \frac{e^{\pmb x^T\pmb w_1}}{e^{\pmb x^T\pmb w_1} + e^{\pmb x^T\pmb w_0}} = \frac{e^{\pmb x^T(\pmb w_1 - \pmb w_0)}}{1 + e^{\pmb x^T(\pmb w_1 - \pmb w_0)}}</script><script type="math/tex; mode=display">
P(y = 0) = \frac{e^{\pmb x^T\pmb w_0}}{e^{\pmb x^T\pmb w_1} + e^{\pmb x^T\pmb w_0}} = \frac{1}{1 + e^{\pmb x^T(\pmb w_1 - \pmb w_0)}}</script><p>令$x^T(\pmb w_1 - \pmb w_0) = \pmb t$，那么：</p>
<script type="math/tex; mode=display">
P(y = 1) = \frac{e^{\pmb t}}{1 + e^{\pmb t}}</script><script type="math/tex; mode=display">
P(y = 0) = \frac{1}{1 + e^{\pmb t}}</script><p>我们可以发现，当类别数等于$2$时，softmax模型就会退化为logistic回归模型。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>近期刷题总结记录</title>
    <url>/2020/07/18/%E8%BF%91%E6%9C%9F%E5%88%B7%E9%A2%98%E6%80%BB%E7%BB%93%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h2 id="近期刷题总结记录"><a href="#近期刷题总结记录" class="headerlink" title="近期刷题总结记录"></a>近期刷题总结记录</h2><h3 id="1-Leftmost-Digit"><a href="#1-Leftmost-Digit" class="headerlink" title="1. Leftmost Digit"></a>1. Leftmost Digit</h3><p>​        杭电1060题，给你一个数字$n$，求出$n^n$最高位数字$t$，$n$不超过$10,000,000,000$(10亿)。</p>
<p>​        具体思想如下：</p>
<p>​        设$M = n^n$（n为整数），两边取10的对数则有</p>
<script type="math/tex; mode=display">
\log_{10}M = \log_{10} n^n = k</script><p>​        直觉上，如果$M$不是10的整数幂次，那么$k$是一个浮点数。</p>
<script type="math/tex; mode=display">
k = A.B</script><p>​        $A$为$k$ 的整数部分，$B$ 则为$k$ 的小数部分。有</p>
<script type="math/tex; mode=display">
\log_{10} M = k = A.B \\
10^{A.B} = M \\
10^A*10^{B} = M(B < 1)</script><p>​        其实$A$就代表了$M$的位数（三位数$10^2$、四位数$10^3$以此类推），而$10^B$相当于前面的系数（有点类似科学计数法那个形式）。</p>
<p>​        我们观察幂函数$y = 10^x$ ，在$x = 1$时$y = 10$，我们这儿$B &lt; 1$，<strong>因此$10^B$的值介于$[1,10)$之间，并且是一个浮点数</strong>。我们可以这么想，$10^A*10^{B}$把$10^B$放大了$10^A$倍后得到了$M$，即 $n$ 的 $n$ 次幂。</p>
<p>​        好了，那么$M$的最高位到底是几？很明显是$10^B$这个数字的整数部分。如何取到$B$这个小数呢，$k = n \log_{10} n = A.B$，$k$减去其取整部分即可得到$B$。$10^B$再取整就得到了最高位。</p>
<p>​        程序代码如下</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> t,n;</span><br><span class="line">	<span class="keyword">double</span> k;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;t);</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">        k = n * <span class="built_in">log10</span>(<span class="number">1.0</span> * n); <span class="comment">// k = A.B</span></span><br><span class="line">        <span class="comment">// k减去对k取整部分得到小数部分</span></span><br><span class="line">        k = k - (__int64)k;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,(<span class="keyword">int</span>)<span class="built_in">pow</span>(<span class="number">10</span>,k));</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-Euler函数"><a href="#2-Euler函数" class="headerlink" title="2. Euler函数"></a>2. Euler函数</h3><p>​        在数论中，对正整数，欧拉函数是小于或等于n的正整数中与n互质的数的数目。比如$\phi(8) = 4$，因为1、3、5、7与8互质。注：互质为两者没有除1外的公因数。</p>
<p>​        具体定义如下：</p>
<p>​        我们知道，对于任意正整数n，都可以表示为多个质数的乘积，即如下形式：</p>
<script type="math/tex; mode=display">
n = p_1^{k_1}p_2^{k2}...p_n^{k_n}</script><p>​        ，其中，$p_i$均为质数。</p>
<p>​        那么欧拉函数$\phi(n)$等于</p>
<script type="math/tex; mode=display">
\phi(n) = n\prod_{i=1}^n (1 - \frac{1}{p_i})</script><p>​        性质：</p>
<p>​        ① 欧拉函数是积性函数，$\phi(mn)=\phi(m)*\phi(n)$</p>
<p>​        ② 若p为质数，$\phi(p)=p-1$</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="comment">// 欧拉函数:统计小于等于n中与n互质的数的个数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">euler</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ans = n;</span><br><span class="line">    <span class="keyword">int</span> tmp = n;</span><br><span class="line">    <span class="comment">// n在循环过程中会不断发生变化。</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= <span class="built_in">sqrt</span>(<span class="number">1.0</span> * tmp); i++ )&#123;<span class="comment">//枚举所有能够被n整除的质因数</span></span><br><span class="line">        <span class="keyword">if</span>(n % i == <span class="number">0</span>)&#123;</span><br><span class="line">            ans = ans * (i - <span class="number">1</span>) / i;<span class="comment">// 欧拉公式，不断累乘</span></span><br><span class="line">            <span class="keyword">while</span>(n % i == <span class="number">0</span>)&#123;</span><br><span class="line">                n /= i; <span class="comment">// 消除质数因子的过程</span></span><br><span class="line">                <span class="comment">//将质数i不断的左除过去</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">         <span class="comment">// 新的n = n / p1 / p1 / p1 ... = p2*p2*p2...*pn</span></span><br><span class="line">        <span class="comment">// p1 &lt; p2 &lt; ... &lt; pn</span></span><br><span class="line">        <span class="comment">// 再次循环通过枚举找到p2....pn</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// n不为1，表明还有因子，最后一个质因子可能不在[2,sqrt(n)]之间</span></span><br><span class="line">    <span class="keyword">if</span>(n &gt; <span class="number">1</span>)&#123;</span><br><span class="line">        ans = ans / n * (n - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> t,n;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;t);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,euler(n));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-找出缺失的数字"><a href="#3-找出缺失的数字" class="headerlink" title="3. 找出缺失的数字"></a>3. 找出缺失的数字</h3><p>​        题目大意如下，首先输入$n$个不同的数字，再次输入$n-1$个数字，找出第二次输入没有输入的那个数字。输入的数字的值介于$[0,1000000000]$，$2 &lt;= n &lt;= 400000$。</p>
<p>​        可以看到数据规模较大，无论是集合还是利用高斯公式求两次和都可能数据溢出。能不能使用$O(n)$时间复杂度以及$O(1)$空间解决这个问题？</p>
<p>​        下面介绍二进制中异或（<code>^</code>）的操作的一些性质。</p>
<p>​        异或就是相同为0，不同为1。即<code>1 ^ 1 = 0,0 ^ 0 = 0,1 ^ 0 = 1,0 ^ 1 = 1</code>。异或还有如下性质</p>
<ul>
<li><p><code>0 ^ x = x</code></p>
</li>
<li><p><code>x ^ x = 0</code></p>
</li>
<li><p>异或满足交换律，即<code>1 ^ 2 ^ 4 ^ 6 =  6 ^ 2 ^ 1 ^ 4</code></p>
<p>有了这些性质，对于这个题，我们设置<code>a = 0,b = 0</code>；</p>
<p><code>a = a ^ arr1[i], i from 0 to n-1</code>，<code>b = b ^ arr2[j], j  from 0 to n-2</code>。</p>
<p>最后所求数字即为<code>a^b</code>。</p>
<p>其实很好理解：</p>
<script type="math/tex; mode=display">
a = a_1 \bigoplus a_2 \bigoplus....\bigoplus a_n \\
b = b_1 \bigoplus b_2 \bigoplus....\bigoplus b_{n-1} \\</script><p>最后<code>a^b</code>，由<strong>异或交换律</strong>这个性质等价于，将前后两次相同的数字先做异或得到0，前后两次相同的数字就都被消掉了，最后剩下的那个数字再与0异或还是这个数字，也就是第二次没有出现的那个数字。</p>
<p>同理，找出在都是偶数次出现的数字中出现次数为奇数次的数字也可以利用异或算法解决。</p>
<p><code>a = 1 ^ 2 ^ 4 ^ 6;b = 4 ^ 6 ^ 1;</code></p>
<p> <code>a ^ b =1 ^ 2 ^ 4 ^ 6 ^ 4 ^ 6 ^ 1 = 1 ^ 1 ^ 4 ^ 4 ^ 6 ^ 6 ^ 2= 0 ^ 0 ^ 0 ^ 2 = 0 ^ 2 = 2</code></p>
</li>
</ul>
<h3 id="4-Rightmost-Digit"><a href="#4-Rightmost-Digit" class="headerlink" title="4. Rightmost Digit"></a>4. Rightmost Digit</h3><p>​        杭电1061题。与第1题相反的是，本题求正整数$n^n$最左边的数字。要用到快速幂取模算法。快速幂即比较快的计算n的幂次。相比于循环n次计算的$O(n)$算法，快速幂能够在$O(\log n)$求出结果。比如计算$3^{100}$，一般我们可以循环100次计算出结果。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">long</span> <span class="keyword">long</span> ans = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++)&#123;</span><br><span class="line">    ans *= <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​        我们观察发现:</p>
<script type="math/tex; mode=display">
3^{100} = 3^{50}*3^{50}\\
3^{50} = 3^{25}*3^{25}\\
3^{25} = 3^{12}*3^{12}*3\\
3^{12} = 3^6*3^6 \\
3^6 = 3^3*3^3\\
3^3 = 3^2 * 3\\
3^2 = 3 * 3</script><p>​        这样，我们要计算$3^{100}$，就可以先计算出$3^{50}$，然后自己和自己相乘就可以得到结果，对于$3^{25}$同理，然后这样递推下去就可得到结果。按照这个方法计算，我们只做了8次乘法便得到了结果，相比100次大大减少了时间。幂次不是奇数就是偶数，偶数次幂就等于自己乘自己，奇数次幂得额外再乘以一个基底数字。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 递归计算快速幂 a^b</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(b == <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> a;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> temp = f(a, b / <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">return</span> (b % <span class="number">2</span> == <span class="number">0</span> ? <span class="number">1</span> : a) * temp * temp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 非递归</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> base = a;</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>( b != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(b % <span class="number">2</span> == <span class="number">1</span>)&#123;</span><br><span class="line">            ans = ans * base;</span><br><span class="line">        &#125;</span><br><span class="line">        base = base * base;</span><br><span class="line">        b /= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​        回到这道题，两个数相乘，二者的最后一位分别相乘贡献了结果，因此我们可以在计算快速幂过程中不断对10取模就可以得到最终结果。</p>
<p>​        插入一点其他知识：</p>
<script type="math/tex; mode=display">
(a + b) \mod p = (a\mod p + b \mod p) \mod p</script><p>​        在计算斐波那契数列时会用到。将加号改为乘号同样适用。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 快速幂取余</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> base = a;</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>( b != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(b % <span class="number">2</span> == <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="comment">// 最后两位数相乘再对10取余 ans = ans * base ans % 10 = (ans%10)*(base%10)%10</span></span><br><span class="line">            ans = (ans % <span class="number">10</span>) * (base % <span class="number">10</span>) % <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 因为求最后一位数字，每次记录最后一位数字是几就可以避免越界</span></span><br><span class="line">        base = (base % <span class="number">10</span>)* (base % <span class="number">10</span>) % <span class="number">10</span>;</span><br><span class="line">        b /= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> t,n;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; t;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,f(n,n));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5-阶乘位数"><a href="#5-阶乘位数" class="headerlink" title="5. 阶乘位数"></a>5. 阶乘位数</h3><p>​        给定一个小于等于$10,000,000$的正整数，计算其阶乘的位数。小于1000的数字的阶乘可以通过模拟乘法来计算出来，但是当数字特别大数组也无法存下。这是就要用到斯特林公式。</p>
<p>​        一个十进制数 $n$ 的位数可以表示为：</p>
<script type="math/tex; mode=display">
(int)\log_{10} n + 1</script><p>这一点可以很明显的在$y = \log_{10} x$的函数图像上观察出来。</p>
<p>​        因此，$n!$的位数即：</p>
<script type="math/tex; mode=display">
\log_{10} n! + 1</script><script type="math/tex; mode=display">
\log_{10} n! = \log_{10} n * (n-1) * ... * 1 = \log_{10}n + \log_{10}n-1 + ...</script><p>​        只要循环计算1到n的对数再求和之后取整加一就是结果。</p>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
  </entry>
  <entry>
    <title>Graph Neutral Network和Graph Embedding</title>
    <url>/2020/07/07/Graph%20Neutral%20Network%E5%92%8CGraph%20Embedding/</url>
    <content><![CDATA[<h2 id="Graph-Neutral-Network和Graph-Embedding"><a href="#Graph-Neutral-Network和Graph-Embedding" class="headerlink" title="Graph Neutral Network和Graph Embedding"></a>Graph Neutral Network和Graph Embedding</h2><h3 id="0-概述"><a href="#0-概述" class="headerlink" title="0. 概述"></a>0. 概述</h3><h4 id="0-1-图的基础知识"><a href="#0-1-图的基础知识" class="headerlink" title="0.1 图的基础知识"></a>0.1 图的基础知识</h4><p>​        通常定义一个图 $G=(V,E)$，其中 $V$为 <strong>顶点（Vertices）</strong>集合，$E$为<strong>边（Edges）</strong>集合。对于一条边$e=(u,v)$ 包含两个<strong>端点（Endpoints）</strong> u 和 v，同时 u 可以称为 v 的<strong>邻居（Neighbor）</strong>。当所有的边为有向边时，图称之为<strong>有向（Directed）</strong>图，当所有边为无向边时，图称之为<strong>无向（Undirected）</strong>图。在无向图中，对于一个顶点 v，令$d(v) $表示连接的边的数量，称之为<strong>度（Degree）</strong>；有向图中又分为<strong>入度</strong>和<strong>出度</strong>。</p>
<p>​        对于一个无向图 $G=(V,E)$，其<strong>邻接矩阵（Adjacency Matrix）</strong>$A$通常定义为：</p>
<script type="math/tex; mode=display">
A_{ij} = \left\{  
             \begin{array}{**lr**}  
             1，i\not=j&if\ vi\ is\ adjacent\ to\ vj\\
             0, &otherwise 
             \end{array}  
\right.</script><p>​        <img src="/images/graph.png" alt="image-20200707174530868"></p>
<p>​        对于上面这个无向图，其度矩阵（degree matrix）如下。度矩阵$D$是一个对角矩阵。</p>
<p><img src="/images/image-20200707174724389.png" alt="image-20200707174724389"></p>
<p>​        图的拉普拉斯矩阵$L$定义为，在图论中，作为一个图的矩阵表示。拉普拉斯矩阵是对称的（Symmetric）。</p>
<script type="math/tex; mode=display">
L = D - A</script><p>​        另外一种更为常用的的是正则化的拉普拉斯矩阵（Symmetric normalized Laplacian）。</p>
<script type="math/tex; mode=display">
L^{sym} = D^{-\frac{1}{2}}LD^{-\frac{1}{2}} = I - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script><p>​        说明：$I$为单位矩阵；$D^{-\frac{1}{2}}$表示度矩阵对角线上的元素开平方根取倒数。</p>
<h4 id="0-2-二者不同"><a href="#0-2-二者不同" class="headerlink" title="0.2 二者不同"></a>0.2 二者不同</h4><p>​        图嵌入旨在通过保留图的网络<strong>拓扑结构和节点内容信息</strong>，将图中顶点表示为低维向量，以便使用简单的机器学习算法（例如，支持向量机分类）进行处理（摘自知乎：图神经网络（Graph Neural Networks）综述，作者：苏一。<a href="https://zhuanlan.zhihu.com/p/75307407）。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/75307407）。</a></p>
<p>​        图神经网络是用于处理图数据（非欧式空间）的神经网络结构。</p>
<p>​        图嵌入和图神经网络的区别与联系。</p>
<p><img src="https://d33wubrfki0l68.cloudfront.net/3c5b07652ee4f2012f3ce1b9cf25bf8282a4f9f4/abdc9/images/cn/2020-04-11-graph-embedding-and-gnn/graph-embedding-vs-graph-neural-networks.png" alt=""></p>
<h3 id="1-Graph-Embedding"><a href="#1-Graph-Embedding" class="headerlink" title="1. Graph Embedding"></a>1. Graph Embedding</h3><p>​        Embedding在数学上是一个函数，将一个空间的点映射到另一个空间，通常是从高维抽象的空间映射到低维具象的空间，并且在低维空间保持原有的语义。早在2003年，Bengio就发表论文论述word embedding想法，将词汇映射到实数向量空间。而2013年google连发两篇论文推出word embedding的神经网咯工具包：skip-gram、cbow（连续词袋模型），使得embedding技术成为深度学习的基本操作，进而导致万物皆可embedding。</p>
<p>​        而基于图的embedding又可以分为基于顶点（vertex）和基于图（graph）。前者主要是将给定的图数据中的vertex表示为单独的向量（vector），后者将整个图进行embedding表示，之后可以进行图的分类等工作。下面分别介绍。</p>
<h4 id="1-1-Vertex-Embedding"><a href="#1-1-Vertex-Embedding" class="headerlink" title="1.1 Vertex Embedding"></a>1.1 Vertex Embedding</h4><h5 id="1-1-1-DeepWalk"><a href="#1-1-1-DeepWalk" class="headerlink" title="1.1.1 DeepWalk"></a>1.1.1 DeepWalk</h5><p>Perozzi B, Al-Rfou R, Skiena S. Deepwalk: Online learning of social representations[C]//Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014: 701-710.</p>
<p><img src="/images/image-20200708200747355.png" alt="image-20200708200747355"></p>
<p>即：基于图上的随机游走进行节点采样，之后将采样到的节点集（每个节点feature使用one-hot表示或者随机向量）输入到skip-gram模型进行训练，得到节点的embedding。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RandomWalk</span><span class="params">(node,t)</span>:</span></span><br><span class="line">    walk = [node]        <span class="comment"># Walk starts from this node</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(t<span class="number">-1</span>):</span><br><span class="line">        <span class="comment"># 应先统计adj_list[node]对应的为1表示相邻的节点索引列表，从该列表中随机选取索引</span></span><br><span class="line">        adj_nodes = np.array(adj_list[node]).nonzero()[<span class="number">0</span>]<span class="comment"># nonzero返回一个元组</span></span><br><span class="line">        </span><br><span class="line">        node = adj_list[node][adj_nodes[random.randint(<span class="number">0</span>,len(adj_nodes)<span class="number">-1</span>)]]</span><br><span class="line">        walk.append(node)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> walk</span><br></pre></td></tr></table></figure>
<h5 id="1-1-2-LINE"><a href="#1-1-2-LINE" class="headerlink" title="1.1.2 LINE"></a>1.1.2 LINE</h5><p>Tang J, Qu M, Wang M, et al. Line: Large-scale information network embedding[C]//Proceedings of the 24th international conference on world wide web. 2015: 1067-1077.</p>
<p>​        相似度定义</p>
<p><strong>first-order proximity</strong></p>
<p>​        <img src="/images/image-20200709225053851.png" alt="image-20200709225053851"></p>
<p>​        1阶相似度用于描述图中成对顶点之间的局部相似度，形式化描述为若$u,v$之间存在直连边，则边权$w_{uv}$即为两个顶点的相似度，若不存在直连边，则1阶相似度为0。 如上图，6和7之间存在直连边，且边权较大，则认为两者相似且1阶相似度较高，而5和6之间不存在直连边，则两者间1阶相似度为0。</p>
<p><strong>second-order proximity</strong></p>
<p>​        仅有1阶相似度就够了吗？显然不够，如上图，虽然5和6之间不存在直连边，但是他们有很多相同的邻居顶点(1,2,3,4)，这其实也可以表明5和6是相似的，而2阶相似度就是用来描述这种关系的。 形式化定义为，令$p_u = (w_{u,1},…,w_{u,|V|})$ 表示顶点$u$与<strong>所有其他</strong>顶点间的1阶相似度，则  $u$ 与 $v$ 的2阶相似度可以通过 $p_{u}$ 和  $p_v$ 的相似度表示（两个顶点他们的邻居集合的相似程度）。若$u$与$v$之间不存在相同的邻居顶点，则2阶相似度为0。</p>
<p><strong>目标函数</strong></p>
<p>​        <strong>1st-order</strong>（用于无向图）</p>
<p>​        对于每条边集$E$中的任一条边$(i,j)$，邻接的两个节点$p_i,p_j$的联合分布定义为：</p>
<script type="math/tex; mode=display">
p(v_i,v_j) = \frac{1}{1 + \exp(-u_i·u_j)}</script><p>​        $u_i,u_j$即节点$v_i,v_j$的低维embedding表示。同时定义经验分布$\hat p$：</p>
<script type="math/tex; mode=display">
\hat p(i,j) = \frac{w_{ij}}{W},W = \sum_{(i,j)\in E}w_{ij}</script><p>​        那么，目标函数就是尽可能地减小这两个分布的差异，衡量两个分布差异可以使用KL散度来衡量，进而：</p>
<script type="math/tex; mode=display">
O_1 = - \sum_{(i,j)\in E}w_{ij}\log p(v_i,v_j)</script><p>​        <strong>2nd-order</strong></p>
<script type="math/tex; mode=display">
O_2 = - \sum_{(i,j)\in E} w_{ij}\log p(v_j|v_i)</script><p>​        当然也可以使用二者的结合。</p>
<h5 id="1-1-3-Node2Vec"><a href="#1-1-3-Node2Vec" class="headerlink" title="1.1.3 Node2Vec"></a>1.1.3 Node2Vec</h5><p>Grover A, Leskovec J. node2vec: Scalable feature learning for networks[C]//Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. 2016: 855-864.</p>
<p>​        本质上也是基于随机游走，提出了一种新的节点采样策略，有“导向”的游走，即加入了参数来控制从某个节点到其他节点的概率。采样得到的节点集，<strong>仍使用word2vec形式进行训练</strong>。</p>
<p>​        如下图。p（return parameter）、q（in-out parameter）为超参数。</p>
<p>​        下图中，现处于节点v，上一个节点是t，那么从节点v到节点$t$、$x_1$、$x_2$、$x_3$的概率$\pi_{vx}$计算公式如下：</p>
<script type="math/tex; mode=display">
\alpha(t,x) = \frac{1}{p},d_{t,x} = 0</script><script type="math/tex; mode=display">
\alpha(t,x) = 1,d_{t,x} = 1</script><script type="math/tex; mode=display">
\alpha(t,x) = \frac{1}{q},d_{t,x} = 2</script><script type="math/tex; mode=display">
\pi_{vx}=\alpha(t,x) w_{v,x}</script><p>​        其中，$d_{tx}$表示<strong>上一节点和下一个能到达的节点的距离</strong>，$w_{vx}$即节点v与next节点连接边的权重（无向图中为1）。下图中，节点$t$与$x_1$直接有边相连接，$d$为1；$t$与$x_2$、$x_3$距离为2（不相邻）；特别的，与上一个节点距离为0，概率为$\frac{1}{p}$。</p>
<p><img src="/images/image-20200709201700829.png" alt="image-20200709201700829"></p>
<p>​        基于这种随机游走策略，使得该模型可以体现网络的同质性（homophily）或结构性（structural equivalence）。网络的“同质性”指<strong>距离相近</strong>的节点的embedding应尽量相似；“结构性”指的是<strong>结构上相似</strong>的节点的embedding应尽量相似。</p>
<p>​        为了能让graph embedding的结果能够表达网络的“结构性”，需要让游走的过程更倾向与<strong>BFS</strong>，因为BFS会更多的在当前节点的领域中游走遍历；为了表达”同质性”，则需要让游走过程倾向于<strong>DFS</strong>。</p>
<p>​        现在讨论超参数p、q，q越大，则随机游走到远方节点的可能性更大，随机游走策略就近似于DFS；反之，近似于BFS。</p>
<h5 id="1-1-4-SDNE"><a href="#1-1-4-SDNE" class="headerlink" title="1.1.4 SDNE"></a>1.1.4 SDNE</h5><p>Wang D, Cui P, Zhu W. Structural deep network embedding[C]//Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. 2016: 1225-1234.</p>
<p>​        SDNE基于LINE研究的基础上，采用deep encoder-decoder模型进行embedding。</p>
<p>​        </p>
<h4 id="1-2-Graph-Embedding"><a href="#1-2-Graph-Embedding" class="headerlink" title="1.2 Graph Embedding"></a>1.2 Graph Embedding</h4><h3 id="2-Graph-Neural-Network"><a href="#2-Graph-Neural-Network" class="headerlink" title="2. Graph Neural Network"></a>2. Graph Neural Network</h3><p>​        图神经网络目前主要的任务包括：节点分类（node classification）、图分类（graph classification）、graph representation、link predication等等。</p>
<h4 id="2-1-Neighborhood-Aggravating"><a href="#2-1-Neighborhood-Aggravating" class="headerlink" title="2.1 Neighborhood Aggravating"></a>2.1 Neighborhood Aggravating</h4><p>​        <strong>neighborhood aggravating即用节点的neighbor feature更新下一个的hidden state。</strong></p>
<p>​        给定一张图，我们可以用与与节点邻接的节点集去表示该节点。下图中，A与B、C、D相邻接，而B又与A、C邻接等等，每个节点都可以使用与其相邻接的节点进行表示（aggravating）（每个节点都有一个初始状态【特征】，当前节点状态用其他节点的上一个状态【特征】表示）。通过其邻接节点聚合，这个节点就可以学到图的结构。</p>
<p><img src="/images/g1.png" alt="image-20200707215031730" style="zoom:67%;" /></p>
<p><img src="E:\hexo\themes\ayer\source\images\image-20200707215123250.png" alt="image-20200707215123250" style="zoom:67%;" /></p>
<p><img src="/images/image-20200707215155395.png" alt="image-20200707215155395" style="zoom:67%;" /></p>
<p>​        因此，我们可以定义任意层的网络来聚合各个节点的信息。（如下图，注：并非完整）</p>
<p><img src="/images/image-20200707215535544.png" alt="image-20200707215535544" style="zoom: 67%;" /></p>
<p>​        注：图中方块表示任意的聚合函数。</p>
<p>​        数学表示如下：</p>
<script type="math/tex; mode=display">
h_v^k = \sigma(W_k\sum_{u \in N(v)}\frac{h_u^{k-1}}{|N(v)|} + B_kh_v^{k-1}),k > 0</script><script type="math/tex; mode=display">
h_v^0 = x_v</script><p>​        其中，k表示第k层（layer）；v表示节点；$\sigma$表示激活函数；$h_u$表示节点$u$的状态；$N(v)$表示节点$v$的邻接节点集合；$|N(v)|$即邻接节点数量；$W、B$分别为权重矩阵和偏置（bias）。</p>
<p>​        因此，我们只需定义一个聚合函数（sum、mean、max-pooling等），以及损失函数（比如：基于节点分类的交叉熵损失函数等），就可以开始迭代训练，最终得到各个节点的embedding向量。</p>
<h4 id="2-2-Graph-Convolution-Networks"><a href="#2-2-Graph-Convolution-Networks" class="headerlink" title="2.2 Graph Convolution Networks"></a>2.2 Graph Convolution Networks</h4><p>​        卷积网络大致分类如下图。</p>
<p><img src="/images/image-20200707223107082.png" alt="image-20200707223107082"></p>
<h5 id="2-2-1-Spatial-based"><a href="#2-2-1-Spatial-based" class="headerlink" title="2.2.1 Spatial-based"></a>2.2.1 Spatial-based</h5><p>​        空间卷积网络也是基于neighborhood aggravating的思想。</p>
<ul>
<li><p>Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.</p>
<p>这篇论文提出了著名的GCN模型，采用的图卷积网络在neighborhood aggravating上做出了一点改动。</p>
</li>
</ul>
<script type="math/tex; mode=display">
h_v^k = \sigma(W_k\sum_{u\in N(v)\cup v}\frac{h_u^{k-1}}{\sqrt{|N(u)||N(v)|}})</script><p>​        <strong>矩阵形式</strong>如下：</p>
<script type="math/tex; mode=display">
H^{(k+1)} = \sigma(D^{-\frac{1}{2}}\hat AD^{\frac{1}{2}}H^{(k)}W_k) \\
\hat A = A + I</script><p>​        其中$A$为图邻接矩阵，$D$为度矩阵，$I$为单位矩阵。</p>
<p>​        下面看一下具体的矩阵形式</p>
<p><img src="/images/g2.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面这张图的邻接矩阵如下</span></span><br><span class="line">&gt; A = torch.tensor(np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]]))</span><br><span class="line"><span class="comment"># 度矩阵 D，即邻接矩阵每一行求和</span></span><br><span class="line">&gt; D = torch.diag(torch.sum(torch.tensor(adj),<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">&gt; X = torch.tensor(np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]))</span><br><span class="line"><span class="comment"># 每个节点的邻居节点聚合等价于A*X</span></span><br><span class="line">&gt; torch.mm(A,X)</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]], dtype=torch.int32)</span><br><span class="line"><span class="comment"># 从结果可以看出相当于对每个节点的所有邻居节点特征求和</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义权重矩阵 W，将input维度映射到output维度，本例input=3</span></span><br><span class="line">&gt; out = <span class="number">4</span></span><br><span class="line">&gt; W = W = torch.randn(<span class="number">3</span>,out)</span><br><span class="line"><span class="comment"># 完成聚合 AXW</span></span><br><span class="line">&gt; output = torch.mm(torch.mm(A,X).type_as(torch.FloatTensor()),W)</span><br><span class="line"><span class="comment"># 经过激活函数,得到这一阶段的hidden（下一阶段的输入）</span></span><br><span class="line">&gt; hidden = F.relu(output)</span><br><span class="line"><span class="comment"># hidden [num_of_vertex, hidden_dims]</span></span><br></pre></td></tr></table></figure>
<p>​        但是Kipf等人在他的论文中，对邻接矩阵$A$进行了标准化。即上文采用的公式。下面实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义图卷积</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCNConv</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, A, in_channels, out_channels)</span>:</span></span><br><span class="line">        super(GCNConv, self).__init__()</span><br><span class="line">        self.A_hat = A+torch.eye(A.size(<span class="number">0</span>))</span><br><span class="line">        self.D     = torch.diag(torch.sum(A,<span class="number">1</span>))</span><br><span class="line">        self.D     = self.D.inverse().sqrt() <span class="comment"># D是对角矩阵，对角线元素开根号</span></span><br><span class="line">        self.A_hat = torch.mm(torch.mm(self.D, self.A_hat), self.D)</span><br><span class="line">        self.W     = nn.Parameter(torch.rand(in_channels,out_channels, requires_grad=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        out = torch.relu(torch.mm(torch.mm(self.A_hat, X), self.W))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="comment"># 定义GCN  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,A, input_dims, nhid, out_dims)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = GCNConv(A,input_dims, nhid)</span><br><span class="line">        self.conv2 = GCNConv(A,nhid, out_dims)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,X)</span>:</span></span><br><span class="line">        H  = self.conv1(X)</span><br><span class="line">        H2 = self.conv2(H)</span><br><span class="line">        <span class="keyword">return</span> H2</span><br></pre></td></tr></table></figure>
<h5 id="2-2-2-Spectral-based"><a href="#2-2-2-Spectral-based" class="headerlink" title="2.2.2 Spectral-based"></a>2.2.2 Spectral-based</h5><p>​         谱图卷积网络基于图的信号处理，即图的拉普拉斯矩阵进行傅里叶变化（Fourier Transform）与逆变化（Inverse Fourier Transform），</p>
<p>​    <strong>图上的傅里叶变换</strong></p>
<p>​        傅里叶变换可以将<strong>时域</strong>信号转为<strong>频域信号</strong>。时域即信号大小随时间而改变，其图像在二维平面中，横轴为时间，纵轴为信号大小，其图像是连续的；而频域图像，横轴代表频率大小，纵轴代表信号大小，其图像是离散的。频域图像本质上描述了一段信号中包含的具体成分（不同频率信号的叠加）如何。</p>
<p>​        而图上的傅里叶变换用到了图的拉普拉斯矩阵$L$，因为$L$是半正定矩阵，因此其特征值都非负。对其进行特征值分解有：</p>
<script type="math/tex; mode=display">
L = U \Lambda U^T</script><p>，其中$U$为特征向量，$\Lambda$为特征值矩阵，是一个对角矩阵，$\lambda_1,\lambda_2…$即特征值，且$\lambda_1&lt;=\lambda_2&lt;=…&lt;=\lambda_n$。$\lambda$也代表了图上的频率。</p>
<p>​        下面直接给出图上傅里叶变换公式，具体推导略去。</p>
<p>​        给定图中某顶点$v$，其对应的信号（特征）为$x$，那么其傅里叶变换即：</p>
<script type="math/tex; mode=display">
\hat x = U^Tx</script><p>​        可知$\hat x$为频域上信号，要重新换变换顶点域，就需要逆傅里叶变换即：</p>
<script type="math/tex; mode=display">
y = U\hat x</script><p><strong>谱图卷积</strong></p>
<p>​        要进行卷积，就需要在频域上进行卷积，即将顶点上的信号进行傅里叶变换后，定义对应的滤波器（滤波函数）$g_\theta(\lambda)$对其进行处理，再利用逆傅里叶变换还原为顶点域中。这里的滤波器或者说关于$\lambda$的滤波函数就是我们要通过训练学习的。说是关于$\lambda$的函数，就是说根据不同的$\lambda$给出不同的<strong>相应</strong>$\theta$。</p>
<p>​        由此，图上信号卷积即：</p>
<script type="math/tex; mode=display">
x_{*\mathcal{G}}f=U((U^Tf)\otimes(U^Tx))</script><p>$f$即要学习的filter，$\otimes$即点积符号。因为最后$y$还是会和$U$运算，直接学习$U^Ty$并表示为：</p>
<script type="math/tex; mode=display">
y = U\hat x = Ug_{\theta}(\Lambda)U^Tx</script><p>，$g_{\theta}$是一个对角矩阵，就是要学习的参数。</p>
<script type="math/tex; mode=display">
g_{\theta} = diag\{\theta_1,...,\theta_n\}</script><p>​        但上面这样定义仍存在几个问题，一是当图太大时矩阵分解计算复杂度太高（a. 矩阵分解复杂度$O(n^3)$，b. 向量与矩阵相乘），二是这并非是局部的（localized）。进而又提出了ChebNet模型（Defferrard M, Bresson X, Vandergheynst P. Convolutional neural networks on graphs with fast localized spectral filtering[C]. Advances in neural information processing systems. 2016: 3844-3852.）。</p>
<p>​        第一类切比雪夫多项式定义如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
T_0(x) &= 1 \\
T_1(x) &= x \\
T_n(x) &= 2xT_{n-1}(x) - T_{n-2}(x)
\end{aligned}</script><script type="math/tex; mode=display">
g_{\theta}(\Lambda) =\sum_{k=0}^{K-1} \theta_{k}\Lambda^k</script><p>$\Lambda^k$表示对角矩阵的$k$次幂。</p>
<p>代码参考：<a href="https://github.com/dovelism/Traffic_Data_Projection_Using_GCN/" target="_blank" rel="noopener">https://github.com/dovelism/Traffic_Data_Projection_Using_GCN/</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChebConv</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    The ChebNet convolution operation.</span></span><br><span class="line"><span class="string">    :param in_c: int, number of input channels.</span></span><br><span class="line"><span class="string">    :param out_c: int, number of output channels.</span></span><br><span class="line"><span class="string">    :param K: int, the order of Chebyshev Polynomial.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, out_c, K, bias=True, normalize=True)</span>:</span></span><br><span class="line">        super(ChebConv, self).__init__()</span><br><span class="line">        self.normalize = normalize</span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(K + <span class="number">1</span>, <span class="number">1</span>, in_c, out_c))  <span class="comment"># [K+1, 1, in_c, out_c]</span></span><br><span class="line">        init.xavier_normal_(self.weight)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.Tensor(<span class="number">1</span>, <span class="number">1</span>, out_c))</span><br><span class="line">            init.zeros_(self.bias)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">"bias"</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        self.K = K + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, graph)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param inputs: the input data, [B, N, C]</span></span><br><span class="line"><span class="string">        :param graph: the graph structure, [N, N]</span></span><br><span class="line"><span class="string">        :return: convolution result, [B, N, D]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        L = ChebConv.get_laplacian(graph, self.normalize)  <span class="comment"># [N, N]</span></span><br><span class="line">        mul_L = self.cheb_polynomial(L).unsqueeze(<span class="number">1</span>)   <span class="comment"># [K, 1, N, N]</span></span><br><span class="line"></span><br><span class="line">        result = torch.matmul(mul_L, inputs)  <span class="comment"># [K, B, N, C]</span></span><br><span class="line">        result = torch.matmul(result, self.weight)  <span class="comment"># [K, B, N, D]</span></span><br><span class="line">        result = torch.sum(result, dim=<span class="number">0</span>) + self.bias  <span class="comment"># [B, N, D]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cheb_polynomial</span><span class="params">(self, laplacian)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute the Chebyshev Polynomial, according to the graph laplacian.</span></span><br><span class="line"><span class="string">        :param laplacian: the graph laplacian, [N, N].</span></span><br><span class="line"><span class="string">        :return: the multi order Chebyshev laplacian, [K, N, N].</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        N = laplacian.size(<span class="number">0</span>)  <span class="comment"># [N, N]</span></span><br><span class="line">        multi_order_laplacian = torch.zeros([self.K, N, N], device=laplacian.device, dtype=torch.float)  <span class="comment"># [K, N, N]</span></span><br><span class="line">        multi_order_laplacian[<span class="number">0</span>] = torch.eye(N, device=laplacian.device, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.K == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> multi_order_laplacian</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            multi_order_laplacian[<span class="number">1</span>] = laplacian</span><br><span class="line">            <span class="keyword">if</span> self.K == <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">return</span> multi_order_laplacian</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>, self.K):</span><br><span class="line">                    multi_order_laplacian[k] = <span class="number">2</span> * torch.mm(laplacian, multi_order_laplacian[k<span class="number">-1</span>]) - multi_order_laplacian[k<span class="number">-2</span>]</span><br><span class="line">        <span class="keyword">return</span> multi_order_laplacian</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_laplacian</span><span class="params">(graph, normalize)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        return the laplacian of the graph.</span></span><br><span class="line"><span class="string">        :param graph: the graph structure without self loop, [N, N].</span></span><br><span class="line"><span class="string">        :param normalize: whether to used the normalized laplacian.</span></span><br><span class="line"><span class="string">        :return: graph laplacian.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> normalize:</span><br><span class="line">            D = torch.diag(torch.sum(graph, dim=<span class="number">-1</span>) ** (<span class="number">-1</span> / <span class="number">2</span>))</span><br><span class="line">            L = torch.eye(graph.size(<span class="number">0</span>), device=graph.device, dtype=graph.dtype) - torch.mm(torch.mm(D, graph), D)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            D = torch.diag(torch.sum(graph, dim=<span class="number">-1</span>))</span><br><span class="line">            L = D - graph</span><br><span class="line">        <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChebNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, hid_c, out_c, K)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param in_c: int, number of input channels.</span></span><br><span class="line"><span class="string">        :param hid_c: int, number of hidden channels.</span></span><br><span class="line"><span class="string">        :param out_c: int, number of output channels.</span></span><br><span class="line"><span class="string">        :param K:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ChebNet, self).__init__()</span><br><span class="line">        self.conv1 = ChebConv(in_c=in_c, out_c=hid_c, K=K)</span><br><span class="line">        self.conv2 = ChebConv(in_c=hid_c, out_c=out_c, K=K)</span><br><span class="line">        self.act = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, data, device)</span>:</span></span><br><span class="line">        graph_data = data[<span class="string">"graph"</span>].to(device)[<span class="number">0</span>]  <span class="comment"># [N, N]</span></span><br><span class="line">        flow_x = data[<span class="string">"flow_x"</span>].to(device)  <span class="comment"># [B, N, H, D]</span></span><br><span class="line"></span><br><span class="line">        B, N = flow_x.size(<span class="number">0</span>), flow_x.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        flow_x = flow_x.view(B, N, <span class="number">-1</span>)  <span class="comment"># [B, N, H*D]</span></span><br><span class="line"></span><br><span class="line">        output_1 = self.act(self.conv1(flow_x, graph_data))</span><br><span class="line">        output_2 = self.act(self.conv2(output_1, graph_data))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_2.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, hid_c, out_c)</span>:</span></span><br><span class="line">        super(GCN, self).__init__()</span><br><span class="line">        self.linear_1 = nn.Linear(in_c, hid_c)</span><br><span class="line">        self.linear_2 = nn.Linear(hid_c, out_c)</span><br><span class="line">        self.act = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, data, device)</span>:</span></span><br><span class="line">        graph_data = data[<span class="string">"graph"</span>].to(device)[<span class="number">0</span>]  <span class="comment"># [N, N]</span></span><br><span class="line">        graph_data = GCN.process_graph(graph_data)</span><br><span class="line">        flow_x = data[<span class="string">"flow_x"</span>].to(device)  <span class="comment"># [B, N, H, D]</span></span><br><span class="line">        B, N = flow_x.size(<span class="number">0</span>), flow_x.size(<span class="number">1</span>)</span><br><span class="line">        flow_x = flow_x.view(B, N, <span class="number">-1</span>)  <span class="comment"># [B, N, H*D]  H = 6, D = 1</span></span><br><span class="line"></span><br><span class="line">        output_1 = self.linear_1(flow_x)  <span class="comment"># [B, N, hid_C]</span></span><br><span class="line">        output_1 = self.act(torch.matmul(graph_data, output_1))  <span class="comment"># [N, N], [B, N, Hid_C]</span></span><br><span class="line"></span><br><span class="line">        output_2 = self.linear_2(output_1)</span><br><span class="line">        output_2 = self.act(torch.matmul(graph_data, output_2))  <span class="comment"># [B, N, 1, Out_C]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_2.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_graph</span><span class="params">(graph_data)</span>:</span></span><br><span class="line">        N = graph_data.size(<span class="number">0</span>)</span><br><span class="line">        matrix_i = torch.eye(N, dtype=graph_data.dtype, device=graph_data.device)</span><br><span class="line">        graph_data += matrix_i  <span class="comment"># A~ [N, N]</span></span><br><span class="line"></span><br><span class="line">        degree_matrix = torch.sum(graph_data, dim=<span class="number">-1</span>, keepdim=<span class="literal">False</span>)  <span class="comment"># [N]</span></span><br><span class="line">        degree_matrix = degree_matrix.pow(<span class="number">-1</span>)</span><br><span class="line">        degree_matrix[degree_matrix == float(<span class="string">"inf"</span>)] = <span class="number">0.</span>  <span class="comment"># [N]</span></span><br><span class="line"></span><br><span class="line">        degree_matrix = torch.diag(degree_matrix)  <span class="comment"># [N, N]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.mm(degree_matrix, graph_data)  <span class="comment"># D^(-1) * A = \hat(A)</span></span><br></pre></td></tr></table></figure>
<p>​        在推出ChebNet后，Kipf等人进一步将公式化简，推出一阶（k=1）ChebNet模型（Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.），这就有了上面我们模型中采用的那个公式。</p>
<script type="math/tex; mode=display">
H^{(k+1)} = \sigma(D^{-\frac{1}{2}}\hat AD^{\frac{1}{2}}H^{(k)}W_k) \\
\hat A = A + I</script><h4 id="2-2-Graph-Attention-Networks"><a href="#2-2-Graph-Attention-Networks" class="headerlink" title="2.2 Graph Attention Networks"></a>2.2 Graph Attention Networks</h4><p>Veličković P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017.</p>
<p>​        attention通俗的讲就是输入两个向量，然后输出一个分数；是一种思想，可以有不同的实现。</p>
<p>​        GAT在空间卷积的基础上，引入了注意力机制，注意力机制也赋予了模型一定的可解释性。加入了attention后，我们aggravating的时候就需要计算当前节点和邻接节点的权重，然后进行聚合。</p>
<p>​        节点v第k个状态更新公式如下：</p>
<script type="math/tex; mode=display">
h_v^k = \sigma (\sum_{u \in N(v)}\alpha_{vu}h_u)</script><p>​        其中$\alpha$就是计算得到的attention权重。</p>
<p><img src="/images/image-20200707230726126.png" alt="image-20200707230726126" style="zoom:80%;" /></p>
<h4 id="2-3-Graph-SAGE"><a href="#2-3-Graph-SAGE" class="headerlink" title="2.3 Graph SAGE"></a>2.3 Graph SAGE</h4><p>Hamilton W, Ying Z, Leskovec J. Inductive representation learning on large graphs[C]. Advances in neural information processing systems. 2017: 1024-1034.</p>
<p>SAGE即Sample and Aggregate。GCN（Kipf.2016）模型存在如下缺点：</p>
<ul>
<li>对于静态图有效，当图结构发生改变（比如：节点的删除和更新）时，需重新训练模型。</li>
<li>当训练数据集非常时，内存无法全部加载图数据。</li>
</ul>
<p>该篇论文提出了名为inductive learning的模型，能更好的适应动态图，同时针对新加入的节点也可以快速的生成node embedding。具体思想是不是去学习一个固定的embedding，而是学习一个聚合函数（aggregator），学习到的聚合函数需满足对称性（symmetry property），因为对称性确保了模型可以被训练而且可以应用于任意顺序的顶点邻居特征集合上。常见的聚合函数有：mean、lstm、pooling等。</p>
<p><img src="/images/sage.png" alt=""></p>
<p>可以看出模型针对每个节点每次聚合固定数量的邻居，而不必把所有数据加载到内存中。</p>
<p><em>K**</em>是网络的层数，也代表着每个顶点能够聚合的邻接点的跳数，因为每增加一层，可以聚合更远的一层邻居的信息**。</p>
<p><img src="/images/khops.png" alt="image-20200920183029770" style="zoom:67%;" /></p>
<p>当新的节点加入到图中时，已经训练好的聚合函数（参数固定了）会聚合新节点的邻居然后生成该节点的embedding。</p>
<p>值得注意的是，当图中大量的节点有更新行为，图结构改变较大时，已经训练好的模型也需要重新训练，即模型退化为静态模型了。</p>
<p>具体实现代码参见：<a href="https://github.com/dsgiitr/graph_nets，具体架构就是三个组件：Aggregator、Encoder、SupervisedModel。Aggregator用来聚合邻居节点的信息，给定节点列表以及对应的邻居节点，首先进行采样然后使用feature函数进行聚合；Encoder包含图的一系列信息，比如邻接矩阵、采样大小等，根据GraphSAGE论文中提出的模型，需要concatenate自身节点信息与Aggregator聚合的邻居节点信息，然后矩阵相乘经过非线性激活函数完成对整个图节点的embedding；SupervisedModel即最终模型，包含一个encoder以及损失函数，完成最终的任务。代码中定义了2个aggregator和2个encoder，实际上是2层的一个编码网络（hops=2）。具体示意图如下：" target="_blank" rel="noopener">https://github.com/dsgiitr/graph_nets，具体架构就是三个组件：Aggregator、Encoder、SupervisedModel。Aggregator用来聚合邻居节点的信息，给定节点列表以及对应的邻居节点，首先进行采样然后使用feature函数进行聚合；Encoder包含图的一系列信息，比如邻接矩阵、采样大小等，根据GraphSAGE论文中提出的模型，需要concatenate自身节点信息与Aggregator聚合的邻居节点信息，然后矩阵相乘经过非线性激活函数完成对整个图节点的embedding；SupervisedModel即最终模型，包含一个encoder以及损失函数，完成最终的任务。代码中定义了2个aggregator和2个encoder，实际上是2层的一个编码网络（hops=2）。具体示意图如下：</a></p>
<p><img  src="/images/GraphSAGE.png" style="zoom:50%;" /></p>
<p>参考资料：</p>
<p>李宏毅，深度学习</p>
<p><a href="http://snap.stanford.edu/proj/embeddings-www/" target="_blank" rel="noopener">http://snap.stanford.edu/proj/embeddings-www/</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/56478167" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/56478167</a></p>
<p><a href="https://github.com/shenweichen/GraphEmbedding" target="_blank" rel="noopener">https://github.com/shenweichen/GraphEmbedding</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>交叉熵</title>
    <url>/2020/07/01/%E4%BA%A4%E5%8F%89%E7%86%B5/</url>
    <content><![CDATA[<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><ul>
<li><p>熵与交叉熵的定义：</p>
<p>信息论中熵的定义：</p>
<script type="math/tex; mode=display">
H(X) = -\sum_i p(x_i)\log p(x_i)</script><p>，其中$X$表示一个分布，$x_i$为该分布中的样本，$p(x_i)$即该样本的概率。</p>
<p><strong>在信息论中，熵是表示信息不确定性的度量。熵越大，表明信息的不确定性越大。</strong></p>
<p>两个分布$p$、$q$的交叉熵定义如下：</p>
<script type="math/tex; mode=display">
H(p,q) = -\sum_i p_i\log(q_i)</script></li>
<li><p>多分类问题中的交叉熵损失函数</p>
<p>在神经网路多分类任务时，最后一层采用$soft\max$层输出预测概率。</p>
<script type="math/tex; mode=display">
soft\max(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}</script><p>输出概率向量表示为$p$，训练$ont-hot$标签向量为$L$，交叉熵定义为$H(L,p)$。</p>
<p>则分类的损失函数定义为</p>
<script type="math/tex; mode=display">
J = \frac{1}{N}\sum_{i=1}^N H(L_i,p_i)</script><p>因为$L_i$、$q_i$表示一个样本，所以计算交叉熵即计算$-L_ilog(q_i)$。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">p = np.array([<span class="number">0.2</span>,<span class="number">0.5</span>,<span class="number">0.3</span>])</span><br><span class="line">target = np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">H = <span class="number">-1</span> * np.sum(target * np.log(p)) / <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>当分类为<strong>二分类时</strong>，最后一层通常采用$sigmod$函数（逻辑斯蒂”回归”）。</p>
<script type="math/tex; mode=display">
sigmod(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">
J = -\frac{1}{N}\sum_{i=1}^N [y_i\log(\hat y_i) +(1-y_i)\log(1-\hat y_i)]</script><script type="math/tex; mode=display">
\hat y(x) = \frac{1}{1 + e^{-W*x}}</script><p>即为交叉熵损失函数的一个特殊形式。</p>
<p>采用交叉熵损失函数的原因是该损失函数为凸函数，从而可以进行凸优化。</p>
</li>
<li><p>KL（Kullback–Leibler）散度（Divergence）和JS（Jensen–Shannon）散度</p>
<p>KL散度用来衡量两个概率分布$P$、$Q$的<strong>差异</strong>。</p>
<script type="math/tex; mode=display">
D_{KL}(P||Q)= \sum P\log(\frac{P}{Q}) = -H(P) + H(P,Q)</script><p>注意：$D_{KL}(P||Q) \not= D_{KL}(Q||P)$。KL散度是不对称的！</p>
<p>JS散度定义如下：</p>
<script type="math/tex; mode=display">
JSD(P||Q) = \frac{1}{2}D_{KL}(P||M) + \frac{1}{2}D_{KL}(Q||M)</script><script type="math/tex; mode=display">
M = \frac{1}{2}(P + Q)</script><p>JS散度是对称的。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>Dropout理解与实现</title>
    <url>/2020/06/30/Dropout%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h2 id="Dropout理解与实现"><a href="#Dropout理解与实现" class="headerlink" title="Dropout理解与实现"></a>Dropout理解与实现</h2><ul>
<li><p>作用：</p>
<p>正则化的一种手段，训练过程中避免神经网络的过拟合。</p>
<p>在某一层中，随机使一部分神经元失活（输出为0），<strong>导致这部分神经元对下一层输入的贡献为0</strong>。</p>
<p>数学表达如下：</p>
<p>第$l + 1$层的输入：</p>
<script type="math/tex; mode=display">
z_i^{(l+1)} = w_i^{(l+1)}y^l + b_i^{(l+1)}</script><script type="math/tex; mode=display">
y_i^{(l+1)} = f(z_i^{(l+1)})</script><p>$f$为激活函数。</p>
<p>若在该层使用dropout，则：</p>
<script type="math/tex; mode=display">
r^{(l+1)} = Bernouli(p)</script><script type="math/tex; mode=display">
\hat y^{(l+1)} = r^{(l+1)} * y_i^{(l+1)}</script><p>$r^{(l+1)}$是一个mask向量，只包含0、1，其中为0表示该神经元失活了。$\hat y^{(l+1)}$即作为下一层的输入。</p>
<p><img src="/images/dropout.png" alt=""></p>
<p>即等价于：</p>
<p><img src="https://pic2.zhimg.com/v2-64930dc0337f42bcdbec488fd5337e95_r.jpg" alt=""></p>
</li>
<li><p>实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">	x: 输入</span></span><br><span class="line"><span class="string">	keep_prob: 留存概率</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, keep_prob)</span>:</span></span><br><span class="line">    mask = np.random.binomial(<span class="number">1</span>, keep_prob, size=x.shape)</span><br><span class="line">    x *= mask</span><br><span class="line">    x = x / keep_prob <span class="comment"># 对余下的神经元进行rescale</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, W1, W2, W3, training=False)</span>:</span></span><br><span class="line">    z1 = np.dot(x, W1)</span><br><span class="line">    y1 = np.tanh(z1)</span><br><span class="line">    z2 = np.dot(y1, W2)</span><br><span class="line">    y2 = np.tanh(z2)</span><br><span class="line">    <span class="comment"># Dropout in layer 2 </span></span><br><span class="line">    <span class="keyword">if</span> training:</span><br><span class="line">        m2 = np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>, size=z2.shape) <span class="comment"># 生成mask</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        m2 = <span class="number">0.5</span> <span class="comment"># 训练中没有rescale，测试时需要平衡训练中失活的神经元数量</span></span><br><span class="line">    y2 *= m2 <span class="comment"># 乘以mask，为0即代表失活</span></span><br><span class="line">    z3 = np.dot(y2, W3)</span><br><span class="line">    y3 = z3 <span class="comment"># linear output</span></span><br><span class="line">    <span class="keyword">return</span> y1, y2, y3, m2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, W1, W2, W3, training=False)</span>:</span></span><br><span class="line">    z1 = np.dot(x, W1)</span><br><span class="line">    y1 = np.tanh(z1)</span><br><span class="line">    z2 = np.dot(y1, W2)</span><br><span class="line">    y2 = np.tanh(z2)</span><br><span class="line">    <span class="comment"># Dropout in layer 2</span></span><br><span class="line">    <span class="keyword">if</span> training:</span><br><span class="line">   		y2 = dropout(y2,<span class="number">0.5</span>) <span class="comment"># 训练阶段已经进行了rescale</span></span><br><span class="line">    z3 = np.dot(y2, W3)</span><br><span class="line">    y3 = z3 <span class="comment"># linear output</span></span><br><span class="line">    <span class="keyword">return</span> y1, y2, y3, m2</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意：</p>
<p>测试中不需要dropout。dropout一般用于全连接层之前，对卷积层的效果一般。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>Flink流式处理框架学习</title>
    <url>/2020/06/29/Flink%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="Flink流式处理框架学习入门"><a href="#Flink流式处理框架学习入门" class="headerlink" title="Flink流式处理框架学习入门"></a>Flink流式处理框架学习入门</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><h3 id="2-安装与部署"><a href="#2-安装与部署" class="headerlink" title="2. 安装与部署"></a>2. 安装与部署</h3><h3 id="3-Source与Sink"><a href="#3-Source与Sink" class="headerlink" title="3. Source与Sink"></a>3. Source与Sink</h3><h3 id="4-DataStream编程模型API"><a href="#4-DataStream编程模型API" class="headerlink" title="4. DataStream编程模型API"></a>4. DataStream编程模型API</h3><h3 id="5-Checkpoint与State状态管理"><a href="#5-Checkpoint与State状态管理" class="headerlink" title="5. Checkpoint与State状态管理"></a>5. Checkpoint与State状态管理</h3><h3 id="6-Flink中的Windows"><a href="#6-Flink中的Windows" class="headerlink" title="6. Flink中的Windows"></a>6. Flink中的Windows</h3><h3 id="7-Flink中的Time"><a href="#7-Flink中的Time" class="headerlink" title="7.  Flink中的Time"></a>7.  Flink中的Time</h3>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>Kafka入门教程</title>
    <url>/2020/06/25/Kafka%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="Kafka入门教程"><a href="#Kafka入门教程" class="headerlink" title="Kafka入门教程"></a>Kafka入门教程</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><h4 id="1-1-消息队列"><a href="#1-1-消息队列" class="headerlink" title="1.1 消息队列"></a>1.1 消息队列</h4><p>​        总的来说，消息队列可以分为点对点模式、发布订阅模式</p>
<p>​        （1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）</p>
<p>​        点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息， 而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者 接收处理，即使有多个消息监听者也是如此。</p>
<p>​        （2）发布订阅模式（一对多，数据生产后，推送给所有订阅者）</p>
<p>​        发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅 者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即 使当前订阅者不可用，处于离线状态。</p>
<h4 id="1-2-什么是Kafka"><a href="#1-2-什么是Kafka" class="headerlink" title="1.2 什么是Kafka"></a>1.2 什么是Kafka</h4><p>​        在流式计算中，Kafka 一般用来缓存数据，作为大数据处理系统中的一个中间件。</p>
<p>​        （1）Apache Kafka 是一个开源消息系统，由 Scala 写成。是由 Apache 软件基金会开发的 一个开源消息系统项目。</p>
<p>​        （2）Kafka 最初是由 LinkedIn 公司开发，并于 2011 年初开源。2012 年 10 月从 Apache Incubator 毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。</p>
<p>​        （3））Kafka 是一个分布式消息队列。Kafka 对消息保存时根据 Topic 进行归类，发送消息 者称为 Producer，消息接受者称为 Consumer，此外 Kafka 集群有多个 Kafka 实例组成，每个实例(server)称为 broker。</p>
<p>​        （4）无论是 Kafka 集群，还是 consumer 都依赖于 zookeeper 集群保存一些 meta 信息， 来保证系统可用性。</p>
<h4 id="1-3-Kafka架构"><a href="#1-3-Kafka架构" class="headerlink" title="1.3 Kafka架构"></a>1.3 Kafka架构</h4><p>​        （1）Producer ：消息生产者，就是向 Kafka broker 发消息的客户端；</p>
<p>​        （2）Consumer ：消息消费者，向 Kafka broker 取消息的客户端； </p>
<p>​        （3）Topic ：可以理解为一个队列；</p>
<p>​        （4）Consumer Group：这是Kafka用来实现一个topic消息的广播（发给所有的consumer） 和单播（发给任意一个 consumer）的手段。一个 topic 可以有多个 CG。topic 的消息会复制 （不是真的复制，是概念上的）到所有的 CG，但每个 partition 只会把消息发给该 CG 中的一 个 consumer。如果需要实现广播，只要每个 consumer 有一个独立的 CG 就可以了。要实现 单播只要所有的 consumer 在同一个 CG。用 CG 还可以将 consumer 进行自由的分组而不需 要多次发送消息到不同的 topic；</p>
<p>​        （5）Broker ：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker 可以容纳多个 topic；</p>
<p>​        （6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。partition 中的每条消息 都会被分配一个有序的 id（offset）。Kafka 只保证按一个 partition 中的顺序将消息发给 consumer，不保证一个 topic 的整体（多个 partition 间）的顺序； </p>
<p>​        （7）Offset：Kafka 的存储文件都是按照 offset.kafka 来命名，用 offset 做名字的好处是方便查 找。例如你想找位于 2049 的位置，只要找到 2048.kafka 的文件即可。当然 the first offset 就 是 00000000000.kafka。</p>
<p>​    <img src="C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200702161839173.png" alt="image-20200702161839173"></p>
<h3 id="2-Kafka集群部署"><a href="#2-Kafka集群部署" class="headerlink" title="2. Kafka集群部署"></a>2. Kafka集群部署</h3><p>​        Kafka集群依赖ZooKeeper，因此，启动Kafka集群前，应启动zookeeper集群。否则会报错。</p>
<h4 id="2-1-集群部署"><a href="#2-1-集群部署" class="headerlink" title="2.1 集群部署"></a>2.1 集群部署</h4><p>​        （1）修改配置文件</p>
<p>​        下载好对应的tar包并解压后，找到<code>config</code>目录下的<code>server.propertites</code>文件，修改<code>log.dirs</code>配置项，即配置Kafka运行日志的存放路径。</p>
<p>​        （2）启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties &amp;</span><br></pre></td></tr></table></figure>
<p><code>&amp;</code>指后台启动。        </p>
<p>​        （3）关闭</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure>
<p>​        （4）发生错误日志检查</p>
<p>​        当启动或运行时发生故障后，可以检查<code>logs/server.log</code>文件进行故障排查。</p>
<h4 id="2-2-Kafka命令行操作"><a href="#2-2-Kafka命令行操作" class="headerlink" title="2.2 Kafka命令行操作"></a>2.2 Kafka命令行操作</h4><p>​        （1）查看当前服务器中所有topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --list</span><br></pre></td></tr></table></figure>
<p>​        （2）创建topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 \</span><br><span class="line">--create --replication-factor 3 --partitions 1 --topic first</span><br></pre></td></tr></table></figure>
<p>选项说明：</p>
<p>​        —topic 定义 topic 名 </p>
<p>​        —replication-factor 定义副本数</p>
<p>​        —partitions 定义分区数</p>
<p>​        （3）删除topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic first</span><br></pre></td></tr></table></figure>
<p>注意：需要<code>server.properties</code>中设置<code>delete.topic.enable=true</code>否则只是标记删除或者直接重启。</p>
<p>​        （4）发送（生产）消息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> bin/kafka-console-producer.sh \</span><br><span class="line">--broker-list localhost:9092 --topic first</span><br></pre></td></tr></table></figure>
<p>​        （5）消费消息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> bin/kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server localhost:2181 --from-beginning --topic first</span><br></pre></td></tr></table></figure>
<p>​        （6）查看某个topic的详情</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic first</span><br></pre></td></tr></table></figure>
<h3 id="3-Kafka工作流程分析"><a href="#3-Kafka工作流程分析" class="headerlink" title="3. Kafka工作流程分析"></a>3. Kafka工作流程分析</h3><p>​        </p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title>SpringMVC源码浅析</title>
    <url>/2020/06/24/SpringMVC%E6%BA%90%E7%A0%81%E6%B5%85%E6%9E%90/</url>
    <content><![CDATA[<h2 id="SpringMVC源码浅析"><a href="#SpringMVC源码浅析" class="headerlink" title="SpringMVC源码浅析"></a>SpringMVC源码浅析</h2>]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>生成对抗网络</title>
    <url>/2020/06/22/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h2 id="生成对抗网络（GAN）"><a href="#生成对抗网络（GAN）" class="headerlink" title="生成对抗网络（GAN）"></a>生成对抗网络（GAN）</h2><h3 id="1-入门简介"><a href="#1-入门简介" class="headerlink" title="1. 入门简介"></a>1. 入门简介</h3><p>​        gan整体的损失函数</p>
<script type="math/tex; mode=display">
\min_{G}\max_{D} V(G,D) = E_{x-P_{data}}\log D(x) + E_{z-P_z}\log (1-D(G(z)))</script><p>​        训练时，先训练Discriminator、然后训练Generator，迭代直至目标函数收敛。</p>
<p>​        需要注意的是，一切损失计算都是在D（判别器）输出处产生的，而D的输出一般是fake/true的判断，所以整体上采用的是二分类交叉熵函数。</p>
<p>​        首先看一下maxD部分，因为训练一般是先保持G（生成器）不变训练D的。D的训练目标是正确区分fake/true，如果我们以1/0代表true/fake，则对第一项E因为输入采样自真实数据所以我们期望D(x)趋近于1，也就是第一项更大。同理第二项E输入采样自G生成数据，所以我们期望D(G(z))趋近于0更好，也就是说第二项又是更大。所以是这一部分是期望训练使得整体更大了，也就是<code>maxD</code>的含义了。</p>
<p>　　第二部分<strong>保持D不变，训练G</strong>，这个时候只有第二项E有用了，<strong>关键来了，因为我们要迷惑D，所以这时将label设置为1(我们知道是fake，所以才叫迷惑)，希望D(G(z))输出接近于1，也就是这一项越小越好，这就是minG。当然判别器D哪有这么好糊弄，所以这个时候判别器就会产生比较大的误差，误差会更新G，那么G就会变得更好了，这次没有骗过你，只能下次更努力了</strong>。</p>
<p>​        Discriminator的损失函数</p>
<script type="math/tex; mode=display">
\max_D \log [D(x)] + \log [1 - D(G(z))]</script><p>​        Generator的损失函数</p>
<script type="math/tex; mode=display">
\min_G \log[1-D(G(z))]</script><p>​        <strong>在（近似）最优判别器下，最小化生成器的loss等价于最小化$P_r$与$P_g$之间的JS散度</strong>。</p>
<p>​        下图中可以发现，所有的loss都是由判别器产生的。如果没有D，G不知道自己生成的结果如何，便得不到权重更新。</p>
<p>​    <img src="E:\hexo\themes\ayer\source\images\gan-train.png" alt="image-20200625214641466"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"><span class="comment">#数据集的加载</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Lambda(<span class="keyword">lambda</span> x: x.repeat(<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">    transforms.Normalize(mean=(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), std=(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">'./MNIST_data/'</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">False</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">'./MNIST_data/'</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_dims, output_dims)</span>:</span></span><br><span class="line">        super(Generator,self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dims,<span class="number">256</span>)</span><br><span class="line">        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*<span class="number">2</span>)</span><br><span class="line">        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*<span class="number">2</span>)</span><br><span class="line">        self.fc4 = nn.Linear(self.fc3.out_features, output_dims)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.leaky_relu(self.fc1(x),<span class="number">0.2</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc2(x),<span class="number">0.3</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc3(x),<span class="number">0.4</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.tanh(self.fc4(x))</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_dim)</span>:</span></span><br><span class="line">        super(Discriminator,self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, <span class="number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//<span class="number">2</span>)</span><br><span class="line">        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//<span class="number">2</span>)</span><br><span class="line">        self.fc4 = nn.Linear(self.fc3.out_features, <span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.leaky_relu(self.fc1(x), <span class="number">0.2</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.3</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc2(x), <span class="number">0.2</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.3</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc3(x), <span class="number">0.2</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.3</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(self.fc4(x))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 二分类交叉熵损失函数</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">D_optimizer = optim.Adam(D.parameters(), lr = lr)</span><br><span class="line">G_optimizer = optim.Adam(G.parameters(), lr = lr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">G_train</span><span class="params">(x)</span>:</span></span><br><span class="line">    G.zero_grad()</span><br><span class="line">    z = Variable(torch.randn(batch_size,z_dim).to(device))</span><br><span class="line">    <span class="comment"># label全为1</span></span><br><span class="line">    y = Variable(torch.ones(batch_size,<span class="number">1</span>).to(device))</span><br><span class="line">    </span><br><span class="line">    G_output = G(z)</span><br><span class="line">    D_output = D(G_output)</span><br><span class="line">    </span><br><span class="line">    G_loss = criterion(D_output, y)</span><br><span class="line">    G_loss.backward()</span><br><span class="line">    </span><br><span class="line">    G_optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> G_loss.data.item()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">D_train</span><span class="params">(x)</span>:</span></span><br><span class="line">    D.zero_grad()</span><br><span class="line">    <span class="comment"># x原来的shape [batch_size,3,28,28]</span></span><br><span class="line">    <span class="comment"># 3个通道都是一样的，取一个通道就行</span></span><br><span class="line">    x = x[:,<span class="number">0</span>,:,:]</span><br><span class="line">    x_real, y_real = x.view(<span class="number">-1</span>, mnist_dim), torch.ones(batch_size, <span class="number">1</span>)</span><br><span class="line">    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))</span><br><span class="line">    </span><br><span class="line">    D_output = D(x_real)</span><br><span class="line">    D_real_loss = criterion(D_output, y_real)</span><br><span class="line">    <span class="comment">#D_real_score = D_output</span></span><br><span class="line">    </span><br><span class="line">    z = Variable(torch.randn(batch_size, z_dim).to(device))</span><br><span class="line">    x_fake, y_fake = G(z), Variable(torch.zeros(batch_size, <span class="number">1</span>).to(device))</span><br><span class="line"></span><br><span class="line">    D_output = D(x_fake)</span><br><span class="line">    D_fake_loss = criterion(D_output, y_fake)</span><br><span class="line">    <span class="comment">#D_fake_score = D_output</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># gradient backprop &amp; optimize ONLY D's parameters</span></span><br><span class="line">    D_loss = D_real_loss + D_fake_loss</span><br><span class="line">    D_loss.backward()</span><br><span class="line">    D_optimizer.step()</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span>  D_loss.data.item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">n_epoch = <span class="number">200</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epoch):</span><br><span class="line">    D_losses, G_losses = [], []</span><br><span class="line">    <span class="keyword">for</span> index,(x,_) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        D_losses.append(D_train(x))</span><br><span class="line">        G_losses.append(G_train(x))</span><br><span class="line">    print(<span class="string">'[%d/%d]: loss_d: %.3f, loss_g: %.3f'</span> % (</span><br><span class="line">            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用训练好的GAN生成图片</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    test_z = Variable(torch.randn(batch_size, z_dim).to(device))</span><br><span class="line">    generated = G(test_z)</span><br><span class="line">    save_image(generated.view(generated.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), <span class="string">'./samples/sample_'</span> + <span class="string">'.png'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-各式各样的GAN"><a href="#2-各式各样的GAN" class="headerlink" title="2. 各式各样的GAN"></a>2. 各式各样的GAN</h3><h4 id="2-1DCGAN"><a href="#2-1DCGAN" class="headerlink" title="2.1DCGAN"></a>2.1DCGAN</h4><p>​        深度卷积生成对抗网络，在生成器中，对输入的一维向量不断进行转置卷积（上采样）最终生成对应的图像。在判别器中，则将输入的图像经过多层卷积最后经过sigmod函数进行二分类，判断这是原始数据图片还是生成器产生的图片。</p>
<p><img src="/images/dcgan.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        m.weight.data.normal_(<span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        m.weight.data.normal_(<span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        m.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    input (N, in_dim)</span></span><br><span class="line"><span class="string">    output (N, 3, 64, 64)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim, dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dconv_bn_relu</span><span class="params">(in_dim, out_dim)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(in_dim, out_dim, <span class="number">5</span>, <span class="number">2</span>,</span><br><span class="line">                                   padding=<span class="number">2</span>, output_padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(out_dim),</span><br><span class="line">                nn.ReLU())</span><br><span class="line">        self.l1 = nn.Sequential(</span><br><span class="line">            nn.Linear(in_dim, dim * <span class="number">8</span> * <span class="number">4</span> * <span class="number">4</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm1d(dim * <span class="number">8</span> * <span class="number">4</span> * <span class="number">4</span>),</span><br><span class="line">            nn.ReLU())</span><br><span class="line">        self.l2_5 = nn.Sequential(</span><br><span class="line">            dconv_bn_relu(dim * <span class="number">8</span>, dim * <span class="number">4</span>),</span><br><span class="line">            dconv_bn_relu(dim * <span class="number">4</span>, dim * <span class="number">2</span>),</span><br><span class="line">            dconv_bn_relu(dim * <span class="number">2</span>, dim),</span><br><span class="line">            nn.ConvTranspose2d(dim, <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>, padding=<span class="number">2</span>, output_padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh())</span><br><span class="line">        self.apply(weights_init)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = self.l1(x)</span><br><span class="line">        y = y.view(y.size(<span class="number">0</span>), <span class="number">-1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">        y = self.l2_5(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    input (N, 3, 64, 64)</span></span><br><span class="line"><span class="string">    output (N, )</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim, dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">conv_bn_lrelu</span><span class="params">(in_dim, out_dim)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_dim, out_dim, <span class="number">5</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                nn.BatchNorm2d(out_dim),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>))</span><br><span class="line">        self.ls = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_dim, dim, <span class="number">5</span>, <span class="number">2</span>, <span class="number">2</span>), nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            conv_bn_lrelu(dim, dim * <span class="number">2</span>),</span><br><span class="line">            conv_bn_lrelu(dim * <span class="number">2</span>, dim * <span class="number">4</span>),</span><br><span class="line">            conv_bn_lrelu(dim * <span class="number">4</span>, dim * <span class="number">8</span>),</span><br><span class="line">            nn.Conv2d(dim * <span class="number">8</span>, <span class="number">1</span>, <span class="number">4</span>),</span><br><span class="line">            nn.Sigmoid())</span><br><span class="line">        self.apply(weights_init)        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = self.ls(x)</span><br><span class="line">        y = y.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h4 id="2-2-Conditional-GAN"><a href="#2-2-Conditional-GAN" class="headerlink" title="2.2 Conditional GAN"></a>2.2 Conditional GAN</h4><p>CGAN的目标函数与原始的并无太大不同，只不过加了一个限定条件。</p>
<script type="math/tex; mode=display">
\min_G \max_D V(D,G) = E_{x-p_{data}}[\log(D(x|y))] + E_{z-p_z}[\log[1 - D(G(z|y))]]</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># G(z)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># initializers</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(generator, self).__init__()</span><br><span class="line">        self.fc1_1 = nn.Linear(<span class="number">100</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc1_1_bn = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        <span class="comment"># 处理label one-hot向量的</span></span><br><span class="line">        self.fc1_2 = nn.Linear(<span class="number">10</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc1_2_bn = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        </span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc2_bn = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">512</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc3_bn = nn.BatchNorm1d(<span class="number">1024</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">1024</span>, <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># weight_init</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_init</span><span class="params">(self, mean, std)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self._modules:</span><br><span class="line">            normal_init(self._modules[m], mean, std)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, label)</span>:</span></span><br><span class="line">        x = F.relu(self.fc1_1_bn(self.fc1_1(input)))</span><br><span class="line">        y = F.relu(self.fc1_2_bn(self.fc1_2(label)))</span><br><span class="line">        <span class="comment"># 把两个向量进行合并</span></span><br><span class="line">        x = torch.cat([x, y], <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.fc2_bn(self.fc2(x)))</span><br><span class="line">        x = F.relu(self.fc3_bn(self.fc3(x)))</span><br><span class="line">        x = F.tanh(self.fc4(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># initializers</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(discriminator, self).__init__()</span><br><span class="line">        self.fc1_1 = nn.Linear(<span class="number">784</span>, <span class="number">1024</span>)</span><br><span class="line">        <span class="comment"># 处理label one-hot向量 batch_size * 10</span></span><br><span class="line">        self.fc1_2 = nn.Linear(<span class="number">10</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">2048</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc2_bn = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc3_bn = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># weight_init</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_init</span><span class="params">(self, mean, std)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self._modules:</span><br><span class="line">            normal_init(self._modules[m], mean, std)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, label)</span>:</span></span><br><span class="line">        x = F.leaky_relu(self.fc1_1(input), <span class="number">0.2</span>)</span><br><span class="line">        y = F.leaky_relu(self.fc1_2(label), <span class="number">0.2</span>)</span><br><span class="line">        </span><br><span class="line">        x = torch.cat([x, y], <span class="number">1</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc2_bn(self.fc2(x)), <span class="number">0.2</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc3_bn(self.fc3(x)), <span class="number">0.2</span>)</span><br><span class="line">        x = F.sigmoid(self.fc4(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_init</span><span class="params">(m, mean, std)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Linear):</span><br><span class="line">        m.weight.data.normal_(mean, std)</span><br><span class="line">        m.bias.data.zero_()</span><br></pre></td></tr></table></figure>
<p>结合介绍的两种，可以定义<code>cDCNGAN</code>模型（就是把Linear全连接层换为了ConvTranspose2d或Conv2d卷积层）。</p>
<h4 id="2-3-Bidirectional-GAN"><a href="#2-3-Bidirectional-GAN" class="headerlink" title="2.3 Bidirectional GAN"></a>2.3 Bidirectional GAN</h4><p>讲述$BiGAN$的两篇论文分别为：</p>
<p>Donahue, Jeff, Philipp Krähenbühl, and Trevor Darrell. “Adversarial feature learning.” <em>arXiv preprint arXiv:1605.09782</em> (2016).</p>
<p>Dumoulin, Vincent, et al. “Adversarially learned inference.” <em>arXiv preprint arXiv:1606.00704</em> (2016).</p>
<ul>
<li><p>网络架构</p>
<p><img src="/images/bigan.png" alt="image-20200625104051977"></p>
</li>
</ul>
<ul>
<li>目标函数<script type="math/tex; mode=display">
\min_{G,E}\max_D V(D,E,G)</script><img src="/images/image-20200625104357166.png" alt="image-20200625104357166"></li>
</ul>
<p>代码参考：<a href="https://github.com/fmu2/Wasserstein-BiGAN" target="_blank" rel="noopener">https://github.com/fmu2/Wasserstein-BiGAN</a></p>
<h4 id="2-4-WGAN"><a href="#2-4-WGAN" class="headerlink" title="2.4 WGAN"></a>2.4 WGAN</h4><p>Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017). Wasserstein gan. <em>arXiv preprint arXiv:1701.07875</em>.（gradient clipping）</p>
<p>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., &amp; Courville, A. C. (2017). Improved training of wasserstein gans. In <em>Advances in neural information processing systems</em> (pp. 5767-5777).（gradient penalty）</p>
<p>​        参考：<a href="https://zhuanlan.zhihu.com/p/25071913（令人拍案叫绝的WGAN）。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25071913（令人拍案叫绝的WGAN）。</a></p>
<p>​        $Wasserstein$距离也被称为$Earth  mover’s$距离（推土机距离）。<strong>Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近。</strong></p>
<p>​        <strong>我们可以构造一个含参数$w$、最后一层不是非线性激活层的判别器网络$f_w$，在限制$w$不超过某个范围的条件下，使得</strong></p>
<script type="math/tex; mode=display">
L = E_{x-P_r}[f_w(x)] - E_{x-P_G}[f_w(x)]</script><p><strong>尽可能取到最大，此时$L$就会近似真实分布与生成分布之间的Wasserstein距离（忽略常数倍数$K$）。</strong></p>
<p><img src="https://pic1.zhimg.com/v2-6be6e2ef3d15c4b10c2a943e9bf4db70_r.jpg" alt=""></p>
<p><img src="/images/wgan-code.png" alt="image-20200622215347951"></p>
<p>注：判别器要迭代训练多次。而生成器只训练一次。</p>
<p><img src="/images/wgan-g.png" alt="image-20200622220223959"></p>
<p>WGAN在原生的GAN做出的改进：</p>
<ol>
<li>G和D的损失函数不用对数</li>
<li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li>
<li>D最后一层去掉$sigmod$二分类函数</li>
<li>采用gradient clipping和gradient penalty（改进）</li>
</ol>
<p>原始GAN存在的问题：</p>
<ul>
<li>判别器越好，生成器越容易产生梯度消失。</li>
<li>训练不稳定，容易导致$collapse mode$。</li>
</ul>
<h4 id="2-5-StackGAN由文本生成高分辨率图像"><a href="#2-5-StackGAN由文本生成高分辨率图像" class="headerlink" title="2.5 StackGAN由文本生成高分辨率图像"></a>2.5 StackGAN由文本生成高分辨率图像</h4><p>Zhang, Han, et al. “Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks.” <em>Proceedings of the IEEE international conference on computer vision</em>. 2017.</p>
<h4 id="2-6-GANomaly异常检测"><a href="#2-6-GANomaly异常检测" class="headerlink" title="2.6 GANomaly异常检测"></a>2.6 GANomaly异常检测</h4><ul>
<li>网络架构：</li>
</ul>
<p><img src="/images/ganomaly.png" alt="image-20200623103433011"></p>
<p>​        可以看出，模型包含两个encoder、一个decoder（相当于生成器）和一个判别器。模型划分为三个部分：第一部分为一个自动编码器，包含一个encoder（$G_E$）、一个decoder（$G_D$），这一部分被记为$G$；第二部分为一个encoder，记为$E$；第三部分为一个判别器网络，记为$D$。前两部分也被称为G-Net。</p>
<p>​        输入图片数据$x$经过一个encoder（$G_E$）编码为向量$z$，decoder（$G_D$）将向量$z$还原为原尺寸图像数据$\hat x$，另一个encoder（$E$）将$\hat x$又编码为向量$\hat z$。将$x$和$\hat x$输入判别器网络（$D$）判断图片是原始图片还是生成器生成的图片。</p>
<ul>
<li>损失函数</li>
</ul>
<p>​        损失函数共分为三部分，第一部分是$Enocder Loss$，衡量两个encoder编码向量的损失；第二部分是$Contextual Loss$，衡量原图像与生成器生成图像的损失，第三部分是$Adversial  Loss$，是常规的GAN中判别网络的损失，这里采用的是二分类的交叉熵损失。</p>
<p>​        优化D-net，采用$Adversial  Loss$。</p>
<p>​        优化G-net时，采用三部分损失函数的加权和。</p>
<ul>
<li>异常检测</li>
</ul>
<p>​        原理：由于训练输入的都是正常数据，第一个encoder学习到的是正常数据的分布，经过生成器的重建后再经过encoder编码差异不会很大，当输入异常数据时，encoder编码后会损失部分信息，经过生成器重建后再编码会与原来的数据差异很大，从而进行异常检测。</p>
<script type="math/tex; mode=display">
A(\hat x) = ||G_E(\hat x) - E(G(\hat x))||_1</script><p>​        当异常得分$A$大于某一阈值时，模型就会判定该数据为异常数据。（异常检测并没有用到判别器）。</p>
<h4 id="2-7-DiscoGAN关联分析"><a href="#2-7-DiscoGAN关联分析" class="headerlink" title="2.7 DiscoGAN关联分析"></a>2.7 DiscoGAN关联分析</h4><p><img src="/images/discogan.png" alt="image-20200623120348746"></p>
<p>​        模型主要由两个生成器和两个判别器构成。</p>
<ul>
<li><p>$G_{AB}$：输入A领域（domain）图片，生成B领域图片</p>
</li>
<li><p>$G_{BA}$：输入B领域图片，生成A领域图片</p>
</li>
<li><p>$D_A$：判别A领域原始图像和$G_{BA}$生成的A领域图像</p>
</li>
<li><p>$D_B$：判别B领域原始图像和$G_{AB}$生成的B领域图像</p>
</li>
</ul>
<p><img src="/images/disco-gan-loss.png" alt="image-20200623231830797"></p>
<p><img src="/images/disco-gan-loss2.png" alt="image-20200623231914928"></p>
<p><img src="/images/disco-gan-loss3.png" alt="image-20200623231950959"></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>ZooKeeper基础教程</title>
    <url>/2020/06/22/ZooKeeper%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="ZooKeeper基础教程"><a href="#ZooKeeper基础教程" class="headerlink" title="ZooKeeper基础教程"></a><code>ZooKeeper</code>基础教程</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><h4 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h4><p>​        <code>Zookeeper</code> 是一个基于Java的分布式协调服务的开源框架。主要用来解决分布式集群中应用系统的一致性问题，例如怎样避免同时操作同一数据造成脏读的问题。<code>ZooKeeper</code>本质上是一个分布式的<strong>小文件存储系统</strong>。提供基于类似于文件系统的目录树方式的数据存储，并且可以对树中的节点进行有效管理。从而用来维护和监控你存储的数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理。诸如:统一命名服务、分布式配置管理、分布式消息队列、分布式锁、分布式协调等功能。</p>
<h4 id="1-2-集群"><a href="#1-2-集群" class="headerlink" title="1.2 集群"></a>1.2 集群</h4><p>​        <code>ZooKeeper</code>集群包含两种角色，Leader和Follower。</p>
<ul>
<li><p>Leader</p>
<p><code>Zookeeper</code>集群工作的核心，事务请求（写操作）的唯一调度和处理者，保证集群事务处理的顺序性；</p>
<p>集群内部各个服务器的调度者。 对于 <code>create，setData，delete</code> 等有写操作的请求，则需要统一转发给 leader处理，leader需要决定编号、执行操作，这个过程称为一个事务。</p>
</li>
<li><p>Follower</p>
<p>处理客户端非事务（读操作）请求，转发事务请求给 Leader； 参与集群 Leader 选举投票。 此外，针对访问量比较大的 <code>ZooKeeper</code> 集群，还可新增观察者角色。</p>
</li>
</ul>
<ul>
<li><p>Observer: </p>
<p>观察者角色，观察<code>Zookeeper</code> 集群的最新状态变化并将这些状态同步过 来，其对于非事务请求可以进行独立处理，对于事务请求，则会转发给Leader服务器进行处理。 </p>
<p>不会参与任何形式的投票只提供非事务服务，通常用于在不影响集群事务处理能力的前提下提升集群的非事务处理能力。</p>
</li>
</ul>
<h4 id="1-3-集群搭建"><a href="#1-3-集群搭建" class="headerlink" title="1.3 集群搭建"></a>1.3 集群搭建</h4><p>​        集群通常由<strong>2n+1</strong>台 servers 组成。这是因为为了保证 Leader 选举（基于<code>Paxos</code>算法的实现）能过得到多数的支持，所以<code>ZooKeeper</code>集群的数量一般为奇数。</p>
<p>​        <code>Zookeeper</code>运行需要<code>java</code>环境，所以需要提前安装 。对于安装 leader+follower 模式的集群，大致过程如下</p>
<ul>
<li><p>配置主机名称到IP地址映射配置</p>
</li>
<li><p>修改 <code>ZooKeeper</code> 配置文件（<code>/conf/zoo.cfg</code>文件）</p>
<p>配置data暂存目录，各个服务器地址ip</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">dataDir=/usr/local/zookeeper-3.4.14/zkData</span><br><span class="line"></span><br><span class="line">clientPort=2181</span><br><span class="line"></span><br><span class="line">server.0=192.168.10.1:2888:3888</span><br><span class="line">server.1=192.168.10.2:2888:3888</span><br><span class="line">server.2=192.168.10.3:2888:3888</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p><code>server.A=B:C:D</code>解释：</p>
<p>A：其中 A 是一个数字，表示这个是服务器的编号；</p>
<p>B：是这个服务器的IP地址；</p>
<p>C：<code>Leader</code>选举的端口；</p>
<p>D：<code>Zookeeper</code>服务器之间的通信端口。</p>
</li>
<li><p>远程复制分发安装文件</p>
</li>
<li><p>设置<code>myid</code></p>
<p>在 上一步<code>dataDir</code> 指定的目录下，创建<code>myid</code>文件。在对应的服务器上写入对应序号。</p>
<p>比如<code>192.168.10.1</code>对应<code>server.0</code>，那么在<code>myid</code>中写入0即可。</p>
</li>
<li><p>启动集群</p>
<p>启动命令：<code>zkServer.sh start</code></p>
<p>停止命令：<code>zkServer.sh stop</code></p>
<p>重启命令：<code>zkServer.sh restart</code></p>
<p>查看集群结点状态：<code>zkServer.sh status</code></p>
</li>
</ul>
<h3 id="2-Shell操作"><a href="#2-Shell操作" class="headerlink" title="2 Shell操作"></a>2 Shell操作</h3><p><code>bin/zkCli.sh</code>命令进入命令行界面。</p>
<h4 id="2-1-创建结点"><a href="#2-1-创建结点" class="headerlink" title="2.1 创建结点"></a>2.1 创建结点</h4><p><code>ZooKeeper</code>中的结点类型分为<strong>永久节点和临时结点(-e)</strong>。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create [-s] [-e] path data acl</span><br></pre></td></tr></table></figure>
<p>其中，-s 或-e 分别指定节点特性，顺序或临时节点，若不指定，则表示持 久节点；<code>acl</code>用来进行权限控制。</p>
<p>例子：</p>
<p>创建顺序节点，结点值为123：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create -s /test 123</span><br></pre></td></tr></table></figure>
<p>创建临时结点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create -e /test-tmp 123</span><br></pre></td></tr></table></figure>
<p>创建永久节点：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create /test-per 123p</span><br></pre></td></tr></table></figure>
<h4 id="2-2-读取节点"><a href="#2-2-读取节点" class="headerlink" title="2.2 读取节点"></a>2.2 读取节点</h4><pre><code>     与读取相关的命令有`ls`命令和 `get` 命令，`ls` 命令可以列出 `Zookeeper `指 定节点下的所有子节点，只能查看指定节点下的第一级的所有子节点；`get` 命令 可以获取 `Zookeeper` 指定节点的数据内容和属性信息。
</code></pre><p>查看根目录下所有结点：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /</span><br><span class="line">[cluster, brokers, zookeeper, admin, isr_change_notification, log_dir_event_notification, node1, controller_epoch, servers, nefu, test0000000015, consumers, latest_producer_id_block, config]</span><br></pre></td></tr></table></figure>
<p>获取<code>/nefu</code>结点的信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] get /nefu</span><br><span class="line">ssss  #属性值</span><br><span class="line">cZxid = 0x12</span><br><span class="line">ctime = Thu Jun 04 16:51:44 CST 2020</span><br><span class="line">mZxid = 0x12</span><br><span class="line">mtime = Thu Jun 04 16:51:44 CST 2020</span><br><span class="line">pZxid = 0x12</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 4</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure>
<h4 id="2-3-更新与删除结点"><a href="#2-3-更新与删除结点" class="headerlink" title="2.3 更新与删除结点"></a>2.3 更新与删除结点</h4><h5 id="2-3-1-更新结点"><a href="#2-3-1-更新结点" class="headerlink" title="2.3.1 更新结点"></a>2.3.1 更新结点</h5><p><code>set path data [version]</code> data 就是要更新的新内容，version 表示数据版本。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 5] set /nefu ilovenefu     </span><br><span class="line">cZxid = 0x12</span><br><span class="line">ctime = Thu Jun 04 16:51:44 CST 2020</span><br><span class="line">mZxid = 0xd7</span><br><span class="line">mtime = Mon Jun 22 11:37:18 CST 2020</span><br><span class="line">pZxid = 0x12</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 1 #注意这儿的版本号</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 9</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure>
<h5 id="2-3-2-删除节点"><a href="#2-3-2-删除节点" class="headerlink" title="2.3.2 删除节点"></a>2.3.2 删除节点</h5><p><code>delete path [version]</code></p>
<p>注意：若删除节点存在子节点，那么无法删除该节点，必须先删除子节点，再删除父节点。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Rmr path</span><br></pre></td></tr></table></figure>
<h3 id="3-ZooKeeper数据模型"><a href="#3-ZooKeeper数据模型" class="headerlink" title="3. ZooKeeper数据模型"></a>3. <code>ZooKeeper</code>数据模型</h3><p>​        <code>ZooKeeper</code>的数据模型，在结构上和标准文件系统的非常相似，拥有一个层次的命名空间，都是采用树形层次结构，<code>ZooKeeper</code>树中的每个节点被称为一个<code>Znode</code>。和文件系统的目录树一样，<code>ZooKeeper</code>树中的每个节点可以拥有子节点。</p>
<p><img src="/images/zookeeper.png" alt=""></p>
<p>​        图中的每个节点称为一个<code>Znode</code> 。每个<code>Znode</code>由 3 部分组成:</p>
<p> ① stat：此为状态信息, 描述该 <code>Znode</code> 的版本, 权限等信息</p>
<p> ② data：与该 <code>Znode</code> 关联的数据 </p>
<p> ③ children：该<code>Znode</code>下的子节点</p>
<p>​        <code>Znode</code>有两种，分别为临时节点和永久节点。 </p>
<p>​        节点的类型在创建时即被确定，并且不能改变。 </p>
<p>​        <strong>临时节点</strong>：该节点的生命周期依赖于创建它们的会话。一旦<strong>会话结束</strong>，临时节点将被自动删除，当然可以也可以手动删除。临时节点不允许拥有子节点。 </p>
<p>​        <strong>永久节点</strong>：该节点的生命周期不依赖于会话，并且只有在客户端显示执行删 除操作的时候，他们才能被删除。 </p>
<p>​        <code>Znode</code>还有一个序列化的特性，如果创建的时候指定的话，该<code>Znode</code>的名字 后面会自动追加一个不断增加的序列号。序列号对于此节点的父节点来说是唯一 的，这样便会记录每个子节点创建的先后顺序。它的格式为“%10d”(10 位数字， 没有数值的数位用 0 补充，例如“0000000001”)。</p>
<h3 id="4-Watcher"><a href="#4-Watcher" class="headerlink" title="4. Watcher"></a>4. Watcher</h3><p>​        ZooKeeper 提供了分布式数据发布/订阅功能，一个典型的发布/订阅模型系统定义了一种一对多的订阅关系，能让多个订阅者同时监听某一个主题对象，当 这个主题对象自身状态变化时，会通知所有订阅者，使他们能够做出相应的处理。 </p>
<p>​        ZooKeeper 中，引入了 Watcher 机制来实现这种分布式的通知功能。 ZooKeeper 允许客户端向服务端注册一个 Watcher 监听，当服务端的一些事件触 发了这个 Watcher，那么就会向指定客户端发送一个事件通知来实现分布式的通知功能。</p>
<p>​         触发事件种类很多，如：节点创建，节点删除，节点改变，子节点改变等。 </p>
<p>​        总的来说可以概括 Watcher 为以下三个过程：客户端向服务端注册 Watcher、 服务端事件发生触发 Watcher、客户端回调 Watcher 得到触发事件情况。</p>
<h3 id="5-Java-API编程"><a href="#5-Java-API编程" class="headerlink" title="5. Java API编程"></a>5. Java API编程</h3><p>首先导入<code>pom</code>依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.nefu<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.10<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.logging.log4j/log4j-core --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>简单的连接集群和结点操作</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.zookeeper.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.Stat;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestZookeeper</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String connectionString = <span class="string">"139.129.100.28:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zkClient;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        zkClient = <span class="keyword">new</span> ZooKeeper(connectionString,<span class="number">2000</span>, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line">                List&lt;String&gt; children ;</span><br><span class="line">                <span class="keyword">try</span>&#123;</span><br><span class="line">                    System.out.println(<span class="string">"------------start----------------"</span>);</span><br><span class="line">                    children = zkClient.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</span><br><span class="line">                    System.out.println(children);</span><br><span class="line">                    System.out.println(<span class="string">"-------------end------------------"</span>);</span><br><span class="line">                &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCreateNode</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        String path = zkClient.create(<span class="string">"/nefu"</span>,<span class="string">"i wanna go back"</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">        System.out.println(path);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getChildren</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        zkClient.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        Stat stat = zkClient.exists(<span class="string">"/nefu"</span>, <span class="keyword">false</span>);</span><br><span class="line">        System.out.println(stat);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="6-ZooKeeper典型应用"><a href="#6-ZooKeeper典型应用" class="headerlink" title="6. ZooKeeper典型应用"></a>6. ZooKeeper典型应用</h3><h4 id="6-1-数据发布与订阅（配置中心）"><a href="#6-1-数据发布与订阅（配置中心）" class="headerlink" title="6.1 数据发布与订阅（配置中心）"></a>6.1 数据发布与订阅（配置中心）</h4><p>​        发布与订阅模型，即所谓的配置中心，顾名思义就是发布者将数据发布到ZK节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。</p>
<p>​         应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个 Watcher， 这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从来达 到获取最新配置信息的目的。</p>
<p>​        比如： 分布式搜索服务中，索引的元信息和服务器集群机器的节点状态存放在 ZK 的一些指定节点，供各个客户端订阅使用。 </p>
<p>​        注意：适合数据量很小的场景，这样数据更新可能会比较快。</p>
<h4 id="6-2-命名服务-Naming-Service"><a href="#6-2-命名服务-Naming-Service" class="headerlink" title="6.2 命名服务(Naming Service)"></a>6.2 命名服务(Naming Service)</h4><p>​        在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取 资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提 供的服务地址，远程对象等等——这些我们都可以统称他们为名字（Name）。其 中较为常见的就是一些分布式服务框架中的服务地址列表。通过调用 ZK 提供的 创建节点的 API，能够很容易创建一个全局唯一的 path，这个 path 就可以作为 一个名称。</p>
<p>​        阿里巴巴集团开源的分布式服务框架 <code>Dubbo</code> 中使用 <code>ZooKeeper</code>来作为其命 名服务，维护全局的服务地址列表。</p>
<h4 id="6-3-分布式锁"><a href="#6-3-分布式锁" class="headerlink" title="6.3 分布式锁"></a>6.3 分布式锁</h4><p>​        分布式锁，这个主要得益于 ZooKeeper 保证了数据的强一致性。锁服务可以 分为两类，一个是保持独占，另一个是控制时序。</p>
<p>​        所谓保持独占，就是所有试图来获取这个锁的客户端，最终只有一个可以成 功获得这把锁。通常的做法是把 zk 上的一个 znode 看作是一把锁，通过 create znode 的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功 创建的那个客户端也即拥有了这把锁。</p>
<p>​        控制时序，就是所有试图来获取这个锁的客户端，最终都是会被安排执行， 只是有个全局时序了。做法和上面基本类似，只是这里 /distribute_lock 已经 预先存在，客户端在它下面创建临时有序节点（这个可以通过节点的属性控制： CreateMode.EPHEMERAL_SEQUENTIAL 来指定）。Zk 的父节点（/distribute_lock） 维持一份 sequence,保证子节点创建的时序性，从而也形成了每个客户端的全局 时序。</p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>LRU缓存机制</title>
    <url>/2020/06/11/LRU%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h2 id="LRU缓存机制"><a href="#LRU缓存机制" class="headerlink" title="LRU缓存机制"></a>LRU缓存机制</h2><p>LRU即最近最久未使用。这是Leetcode 147号问题，主要采用了双向链表和哈希表这两种数据结构。双向链表相比单向链表，插入和删除的时间复杂度为O(1)，而哈希表在获取key的时间复杂度为O(1)。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> key, val;</span><br><span class="line">    <span class="keyword">public</span> Node prev, next;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(<span class="keyword">int</span> key, <span class="keyword">int</span> val)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.val = val;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 双向链表</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubleList</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Node head, tail;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> length = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DoubleList</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        head = <span class="keyword">new</span> Node(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">        tail = <span class="keyword">new</span> Node(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">        head.next = tail;</span><br><span class="line">        tail.prev = head;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addFirst</span><span class="params">(Node n)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 双向链表头部添加节点</span></span><br><span class="line">        n.next = head.next;</span><br><span class="line">        n.prev = head;</span><br><span class="line"></span><br><span class="line">        head.next.prev = n;</span><br><span class="line">        head.next = n;</span><br><span class="line">        length++;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">(Node x)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 删除链表中的结点</span></span><br><span class="line">        x.prev.next = x.next;</span><br><span class="line">        x.next.prev = x.prev;</span><br><span class="line">        length--;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Node <span class="title">removeLast</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 首先判断链表是否为空</span></span><br><span class="line">        <span class="keyword">if</span> (head.next == tail) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        Node res = tail.prev;</span><br><span class="line">        remove(res);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.length;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LRUCache</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Map&lt;Integer, Node&gt; map;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> DoubleList cache;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> capacity;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LRUCache</span><span class="params">(<span class="keyword">int</span> capacity)</span> </span>&#123;</span><br><span class="line">        map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        cache = <span class="keyword">new</span> DoubleList();</span><br><span class="line">        <span class="keyword">this</span>.capacity = capacity;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">get</span><span class="params">(<span class="keyword">int</span> key)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!map.containsKey(key)) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> val = map.get(key).val;</span><br><span class="line">        put(key, val);</span><br><span class="line">        <span class="keyword">return</span> val;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(<span class="keyword">int</span> key, <span class="keyword">int</span> value)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Node x = <span class="keyword">new</span> Node(key, value);</span><br><span class="line">        <span class="keyword">if</span> (map.containsKey(key)) &#123;</span><br><span class="line">            <span class="comment">// map中存在key，更新值</span></span><br><span class="line">            cache.remove(map.get(key));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.capacity == cache.size()) &#123;</span><br><span class="line">                <span class="comment">// 删除最久未使用的结点 -- 最后一个结点</span></span><br><span class="line">                Node last = cache.removeLast();</span><br><span class="line">                map.remove(last.key);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        cache.addFirst(x);</span><br><span class="line">        map.put(key, x);<span class="comment">// if else 中都有的逻辑，提取出来</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
  </entry>
  <entry>
    <title>数据分析之Pandas</title>
    <url>/2020/06/10/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BPandas/</url>
    <content><![CDATA[<h2 id="数据分析之Pandas"><a href="#数据分析之Pandas" class="headerlink" title="数据分析之Pandas"></a>数据分析之Pandas</h2><p>​    Pandas中两大数据结构：DataFrame、Series。</p>
<ol>
<li><p>数据文件的读取。可以读取excel、csv、tsv等数据文件。下面以csv文件为例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">'https://bitly.com/imdbratings'</span>)</span><br></pre></td></tr></table></figure>
<p>具体可以指定的参数有：</p>
<ul>
<li><p>encoding，文件编码方式，如：<code>GBK/ANSI/UTF-8</code></p>
</li>
<li><p>names，指定数据文件的各个列的名称，传入的是list类型</p>
</li>
<li><p>seq，指定文件的分隔符</p>
</li>
</ul>
</li>
<li><p>查看数据相关信息</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<p><img src="/images/pandas-1.png" alt=""></p>
</li>
</ol>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.info()</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">core</span>.<span class="title">frame</span>.<span class="title">DataFrame</span>'&gt;</span></span><br><span class="line"><span class="class"><span class="title">RangeIndex</span>:</span> <span class="number">979</span> entries, <span class="number">0</span> to <span class="number">978</span></span><br><span class="line">Data columns (total <span class="number">6</span> columns):</span><br><span class="line"> <span class="comment">#   Column          Non-Null Count  Dtype  </span></span><br><span class="line">---  ------          --------------  -----  </span><br><span class="line"> <span class="number">0</span>   star_rating     <span class="number">979</span> non-null    float64</span><br><span class="line"> <span class="number">1</span>   title           <span class="number">979</span> non-null    object </span><br><span class="line"> <span class="number">2</span>   content_rating  <span class="number">976</span> non-null    object </span><br><span class="line"> <span class="number">3</span>   genre           <span class="number">979</span> non-null    object </span><br><span class="line"> <span class="number">4</span>   duration        <span class="number">979</span> non-null    int64  </span><br><span class="line"> <span class="number">5</span>   actors_list     <span class="number">979</span> non-null    object </span><br><span class="line">dtypes: float64(<span class="number">1</span>), int64(<span class="number">1</span>), object(<span class="number">4</span>)</span><br><span class="line">memory usage: <span class="number">46.0</span>+ KB</span><br></pre></td></tr></table></figure>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.describe()</span><br><span class="line">	  star_rating	duration</span><br><span class="line">count	<span class="number">979.000000</span>	<span class="number">979.000000</span></span><br><span class="line">mean	<span class="number">7.889785</span>	<span class="number">120.979571</span></span><br><span class="line">std		<span class="number">0.336069</span>	<span class="number">26.218010</span></span><br><span class="line">min		<span class="number">7.400000</span>	<span class="number">64.000000</span></span><br><span class="line"><span class="number">25</span>%		<span class="number">7.600000</span>	<span class="number">102.000000</span></span><br><span class="line"><span class="number">50</span>%		<span class="number">7.800000</span>	<span class="number">117.000000</span></span><br><span class="line"><span class="number">75</span>%		<span class="number">8.100000</span>	<span class="number">134.000000</span></span><br><span class="line">max		<span class="number">9.300000</span>	<span class="number">242.000000</span></span><br></pre></td></tr></table></figure>
<ol>
<li><p>获取某一列或多个列的值</p>
<p>通常将要获取的多个列名包装到list中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#获取评分这一列的值，返回一个Series对象</span></span><br><span class="line">df[<span class="string">'star_rating'</span>]</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">df.star_rating</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取评分、电影时长两列的值</span></span><br><span class="line">df[[<span class="string">'star_rating'</span>,<span class="string">'duration'</span>]]</span><br></pre></td></tr></table></figure>
<p>注意：不能这么写：<code>df[[0,1,2,3]]</code></p>
</li>
<li><p>获取某一行或多行的数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#直接切片获取 </span></span><br><span class="line">df[:<span class="number">1</span>]</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">star_rating                                                     <span class="number">9.3</span></span><br><span class="line">title                                      The Shawshank Redemption</span><br><span class="line">content_rating                                                    R</span><br><span class="line">genre                                                         Crime</span><br><span class="line">duration                                                        <span class="number">142</span></span><br><span class="line">actors_list       [<span class="string">u'Tim Robbins'</span>, <span class="string">u'Morgan Freeman'</span>, <span class="string">u'Bob Gunt...</span></span><br><span class="line"><span class="string">Name: 0, dtype: object</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#获取前10部电影的时长</span></span><br><span class="line"><span class="string">df[:10]['</span>duration<span class="string">'] </span></span><br><span class="line"><span class="string">df[:10][['</span>star_rating<span class="string">','</span>duration<span class="string">']]</span></span><br></pre></td></tr></table></figure>
<p>还是推荐使用loc或者iloc来进行行的获取。这样更加直观。</p>
</li>
<li><p><strong>iloc和loc</strong></p>
<p>使用形式：<code>loc[,]/iloc[,]</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取指定索引的数据，比如获取第10部电影的数据</span></span><br><span class="line">df.loc[<span class="number">10</span>] <span class="comment"># 默认取全部列（不建议这么写） 相当于 ） </span></span><br><span class="line">df.loc[<span class="number">10</span>,:] <span class="comment">#（推荐）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取第1、5、10部电影</span></span><br><span class="line">df.loc[[<span class="number">1</span>,<span class="number">5</span>,<span class="number">10</span>], :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># loc函数支持切片, 取前10部电影, 注意是：前闭后闭的</span></span><br><span class="line">df.loc[:<span class="number">10</span>,:] <span class="comment">#获取前10行的数据，返回一个DataFrame</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面讲解列操作</span></span><br><span class="line"><span class="comment"># 取前10部电影的时长</span></span><br><span class="line">df.loc[:<span class="number">10</span>,<span class="string">'duration'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取前10部电影的时长和名字</span></span><br><span class="line">df.loc[:<span class="number">10</span>,[<span class="string">'star_rating'</span>,<span class="string">'title'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取前10部电影评分到时长列</span></span><br><span class="line">df.loc[:<span class="number">10</span>,[<span class="string">'star_rating'</span>:<span class="string">'duration'</span>]]                   </span><br><span class="line">              </span><br><span class="line"><span class="comment"># 下面全部是错误的写法！</span></span><br><span class="line">df[:<span class="number">10</span>][<span class="string">'star_rating'</span>:<span class="string">'duration'</span>]   </span><br><span class="line">df.loc[:,<span class="number">0</span>:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<p><strong>注意：获取某一特定index行，不可以直接<code>df[index]</code>，需借助<code>df.loc[index,:]</code>函数。</strong></p>
<p>loc不仅可以传入索引、列表、切片获得指定数据，还可以传入条件进行筛选，具体参见下一小节。</p>
<p>iloc在列的选取上与loc不同，loc传入的是列名；而iloc传入的是列的索引。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#介绍df.iloc使用， iloc是按位置选取的,传入的参数都是整数</span></span><br><span class="line"><span class="comment"># 传入的切片是前闭后开的！</span></span><br><span class="line"><span class="comment">#选取索引为1、4、5的电影</span></span><br><span class="line">df.iloc[[<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>],:]</span><br><span class="line"></span><br><span class="line">df.iloc[:<span class="number">100</span>,:<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择第0、2列</span></span><br><span class="line">df.iloc[[<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">0</span>,<span class="number">2</span>]]</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取满足特定条件的行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取电影时长在120min以上的电影，返回一个DataFrame</span></span><br><span class="line">df[df[<span class="string">'duration'</span>] &gt; <span class="number">120</span>]</span><br><span class="line">df[df[<span class="string">'duration'</span>] &gt; <span class="number">120</span>].duration</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取电影时长在120min以上的电影且评分高于8的电影,多条件要用括号括起来</span></span><br><span class="line">df[(df[<span class="string">'duration'</span>] &gt; <span class="number">120</span>) &amp; (df[<span class="string">'star_rating'</span>] &gt; <span class="number">8</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">df.loc[(df[<span class="string">'duration'</span>] &gt; <span class="number">120</span>) &amp; (df[<span class="string">'star_rating'</span>] &gt; <span class="number">8</span>), :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取电影时长在120min以上的电影且评分高于8的电影的名字</span></span><br><span class="line">df.loc[(df[<span class="string">'duration'</span>] &gt; <span class="number">120</span>) &amp; (df[<span class="string">'star_rating'</span>] &gt; <span class="number">8</span>),<span class="string">'title'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取电影时长在120min以上的电影且评分高于8的电影的名字和评分</span></span><br><span class="line">df.loc[(df[<span class="string">'duration'</span>] &gt; <span class="number">120</span>) &amp; (df[<span class="string">'star_rating'</span>] &gt; <span class="number">8</span>),[<span class="string">'title'</span>,<span class="string">'star_rating'</span>]]</span><br><span class="line"></span><br><span class="line">df.loc[(df[<span class="string">'duration'</span>] &gt; <span class="number">120</span>) &amp; (df[<span class="string">'star_rating'</span>] &gt; <span class="number">8</span>),<span class="string">'star_rating'</span>:<span class="string">'genre'</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看不同分级的电影数量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取电影具体的分级级别</span></span><br><span class="line">df[<span class="string">'content_rating'</span>].unique()</span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">array([<span class="string">'R'</span>, <span class="string">'PG-13'</span>, <span class="string">'NOT RATED'</span>, <span class="string">'PG'</span>, <span class="string">'UNRATED'</span>, <span class="string">'APPROVED'</span>, <span class="string">'PASSED'</span>,</span><br><span class="line">       <span class="string">'G'</span>, <span class="string">'X'</span>, nan, <span class="string">'TV-MA'</span>, <span class="string">'GP'</span>, <span class="string">'NC-17'</span>], dtype=object)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同分级类别的电影数量</span></span><br><span class="line">df[<span class="string">'content_rating'</span>].value_counts()</span><br><span class="line"><span class="comment"># Output</span></span><br><span class="line">R            <span class="number">460</span></span><br><span class="line">PG<span class="number">-13</span>        <span class="number">189</span></span><br><span class="line">PG           <span class="number">123</span></span><br><span class="line">NOT RATED     <span class="number">65</span></span><br><span class="line">APPROVED      <span class="number">47</span></span><br><span class="line">UNRATED       <span class="number">38</span></span><br><span class="line">G             <span class="number">32</span></span><br><span class="line">PASSED         <span class="number">7</span></span><br><span class="line">NC<span class="number">-17</span>          <span class="number">7</span></span><br><span class="line">X              <span class="number">4</span></span><br><span class="line">GP             <span class="number">3</span></span><br><span class="line">TV-MA          <span class="number">1</span></span><br><span class="line">Name: content_rating, dtype: int64</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以柱状图形式展示</span></span><br><span class="line">df[<span class="string">'content_rating'</span>].value_counts().plot(kind=<span class="string">'bar'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/content-rating.png" alt=""></p>
</li>
<li><p>groupby的应用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看不同题材的电影的平均时长</span></span><br><span class="line">df.groupby(<span class="string">'genre'</span>).duration.mean()</span><br></pre></td></tr></table></figure>
</li>
<li><p>apply、map、applymap</p>
<p>apply传入一个函数，会将该函数应用于选中的列</p>
<p>applymap是指将函数应用于DataFrame的所有元素</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">df[<span class="string">'int_rating'</span>] = df.star_rating.apply(np.ceil) <span class="comment"># 默认为 0</span></span><br><span class="line">df.loc[:<span class="number">5</span>,[<span class="string">'int_rating'</span>,<span class="string">'star_rating'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">	int_rating	star_rating</span><br><span class="line"><span class="number">0</span>			<span class="number">10.0</span> 		<span class="number">9.3</span></span><br><span class="line"><span class="number">1</span>			<span class="number">10.0</span>		<span class="number">9.2</span></span><br><span class="line"><span class="number">2</span>			<span class="number">10.0</span>		<span class="number">9.1</span></span><br><span class="line"><span class="number">3</span>			<span class="number">9.0</span>			<span class="number">9.0</span></span><br><span class="line"><span class="number">4</span>			<span class="number">9.0</span>			<span class="number">8.9</span></span><br><span class="line"><span class="number">5</span>			<span class="number">9.0</span>			<span class="number">8.9</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#获取电影主演的第一个演员的名字</span></span><br><span class="line">df.actors_list.apply(<span class="keyword">lambda</span> x:eval(x)[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#Outputs</span></span><br><span class="line"><span class="number">0</span>          Tim Robbins</span><br><span class="line"><span class="number">1</span>        Marlon Brando</span><br><span class="line"><span class="number">2</span>            Al Pacino</span><br><span class="line"><span class="number">3</span>       Christian Bale</span><br><span class="line"><span class="number">4</span>        John Travolta</span><br></pre></td></tr></table></figure>
<p>下面引入一个新的数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">drinks = pd.read_csv(<span class="string">'http://bit.ly/drinksbycountry'</span>)</span><br><span class="line">drinks.head()</span><br></pre></td></tr></table></figure>
<p><img src="/images/pandas-2.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在DataFrame中，可通过axis指定appply作用在行或者列上</span></span><br><span class="line">res = drinks.loc[:,<span class="string">'beer_servings'</span>:<span class="string">'wine_servings'</span>].apply(np.argmax,axis=<span class="number">0</span>)</span><br><span class="line">res</span><br><span class="line"><span class="comment"># Outputs</span></span><br><span class="line">beer_servings      <span class="number">117</span></span><br><span class="line">spirit_servings     <span class="number">68</span></span><br><span class="line">wine_servings       <span class="number">61</span></span><br><span class="line">dtype: int64</span><br><span class="line">    </span><br><span class="line">drinks.loc[res,<span class="string">'country'</span>]</span><br><span class="line"><span class="number">117</span>    Namibia</span><br><span class="line"><span class="number">68</span>     Grenada</span><br><span class="line"><span class="number">61</span>      France</span><br><span class="line"><span class="comment"># 即beer_servings 最大的国家是Namibia，spirit_servings大的国家Grenada等等</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在DataFrame中删除行或者列</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删除一列或多列</span></span><br><span class="line">df.drop([<span class="string">'int_rating'</span>],axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除一行或多行</span></span><br><span class="line">df.drop([<span class="number">0</span>,<span class="number">1</span>], axis = <span class="number">0</span>, inplace = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>在DataFrame中给某一列重命名</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取所有列名</span></span><br><span class="line">df.columns</span><br><span class="line"><span class="comment"># 替换全部列名</span></span><br><span class="line">df_cols = [<span class="string">'Star rating'</span>,<span class="string">'Title'</span>, <span class="string">'Content rating'</span>,<span class="string">'Genre'</span>, <span class="string">'Duration'</span>, <span class="string">'Actors list'</span>]</span><br><span class="line">df.columns = df_cols</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改指定列名</span></span><br><span class="line">df.rename(columns = &#123;<span class="string">'star_rating'</span>:<span class="string">'star rating'</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>也可以通过在read数据时，指定names参数来指定列名（<code>names = df_cols</code>）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 也可以.str来使用Series中封装的字符串方法</span></span><br><span class="line">df.columns.str.[replace(<span class="string">'_'</span>,<span class="string">' '</span>)/lower()/upper()]</span><br></pre></td></tr></table></figure>
</li>
<li><p>给某一列进行排序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pandas.Series.sort_values(ascending=True, inplace=False) 默认升序,且不改变原Series结果</span></span><br><span class="line"><span class="comment"># 按照电影时长从大倒小排序</span></span><br><span class="line">df.duration.sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># pandas.DataFrame.sort_values(by='columns_name')</span></span><br><span class="line">df.sort_values(by=<span class="string">'duration'</span>,ascending=<span class="literal">False</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照电影评分从大到小排列，若评分相同，则时长长的在前</span></span><br><span class="line">df.sort_values([<span class="string">'star_rating'</span>,<span class="string">'duration'</span>],ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>发现并删除重复行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pandas.Series.duplicated() 返回bool的series</span></span><br><span class="line">df.title.duplicated().sum()</span><br><span class="line"></span><br><span class="line">df.duplicated()</span><br><span class="line"></span><br><span class="line">df.drop_duplicates(keep=<span class="string">'first'</span>)</span><br><span class="line"><span class="comment"># keep = first即保存第一个重复的元素，keep = False，不保留重复元素</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>Java中多态和方法的重载和重写</title>
    <url>/2020/06/06/Java%E4%B8%AD%E5%A4%9A%E6%80%81%E5%92%8C%E6%96%B9%E6%B3%95%E7%9A%84%E9%87%8D%E8%BD%BD%E5%92%8C%E9%87%8D%E5%86%99/</url>
    <content><![CDATA[<h2 id="Java中多态和方法的重载和重写"><a href="#Java中多态和方法的重载和重写" class="headerlink" title="Java中多态和方法的重载和重写"></a>Java中多态和方法的重载和重写</h2><p>重载（Overload）：指在同一个类中，方法名相同，而<strong>参数列表</strong>不同，参数列表指参数的<strong>顺序、类型、个数</strong>。重载与方法的返回类型、可见类型无关（例1）。</p>
<p>具体例子：</p>
<p><img src="/images/ex1.png" alt="例1"></p>
<p><img src="/images/ex1-1.png" alt="image-20200608221031279"></p>
<center>
    例1 重载与方法返回类型无关
</center>



<p>重写（Overwrite）：指的是在继承关系中，子类定义了与父类的同名方法且参数列表与父类方法完全一致。注意：子类的方法名、参数列表必须与父类（接口）定义的完全一致，否则不能称为覆写（例7）；确认重写关系后，当返回类型为基本类型时（<code>int、double、char、short、byte、float、bool、long、String</code>）子类覆写时<strong>不可以改变</strong>返回类型，当返回类型为引用型，子类返回的引用型只能为，父类返回类型的子类或相同类型，不可以为无关的引用类型。覆写不可以缩小父类方法的可见性！（例5）。</p>
<p>总结起来就是，首先确认子类方法名和参数列表是否与父方法<strong>完全一致</strong>，如果是，那么就是重写，语法要求方法返回类型务必和父类方法相同，且不可以减小该方法可见性；如果不是，则不是重写。</p>
<p>具体例子</p>
<p><img src="/images/ex2.png" alt="例2-1"></p>
<p><img src="/images/ex2-2.png" alt="例2-2"></p>
 <center> 
     例2 重写时，返回类型务必相同<center>
 </center>



<p><img src="/images/ex3.png" alt="image-20200608221852911"></p>
<center>
    例3 重写时，子类返回类型为父类方法返回类型的子类或实现类，参数列务必表完全相同
</center>



<p><img src="/images/ex4.png" alt="image-20200608221930946"></p>
<center>
    例4 重写时，子类返回类型不允许为与父类方法返回类型毫无关系的类型
</center>



<p><img src="/images/ex5.png" alt="image-20200608222225893"></p>
<center>
    例5 重写时，子类不允许减小父方法可见性
</center>



<p><img src="/images/ex6.png" alt="image-20200608223224765"></p>
<center>
    例6 重写时，子类重写的方法返回类型不能为
</center>



<p><img src="/images/ex7-1.png" alt="image-20200608223631425"></p>
<p><img src="/images/ex7-2.png" alt="image-20200608223608764"></p>
<center>
    例7 当参数列表不同时，这两个方法不存在覆写关系，加上注解就会报错
</center>

<p>顺便提一下，多态指的是动态绑定方法，而非类内变量。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Father</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> i + <span class="number">10</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Son</span> <span class="keyword">extends</span> <span class="title">Father</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i = <span class="number">10</span>;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		Father son = <span class="keyword">new</span> Son();</span><br><span class="line">		System.out.println(son.test()); <span class="comment">// 10 子类没有定义test，找到父类方法，由于多态不动态绑定变量，因此i取得是父类的</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Father</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> getI() + <span class="number">10</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getI</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> i;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Son</span> <span class="keyword">extends</span> <span class="title">Father</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i = <span class="number">10</span>;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getI</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> i;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		Father son = <span class="keyword">new</span> Son();</span><br><span class="line">		Son son2 = <span class="keyword">new</span> Son();</span><br><span class="line">		</span><br><span class="line">		System.out.println(son.test()); <span class="comment">// 20</span></span><br><span class="line">		System.out.println(son2.test());<span class="comment">// 20</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>归一化和标准化</title>
    <url>/2020/05/23/%E5%BD%92%E4%B8%80%E5%8C%96%E5%92%8C%E6%A0%87%E5%87%86%E5%8C%96/</url>
    <content><![CDATA[<h2 id="归一化和标准化"><a href="#归一化和标准化" class="headerlink" title="归一化和标准化"></a>归一化和标准化</h2><p>​        注：本文内容转载自:<a href="https://www.jianshu.com/p/4c3081d40ca6" target="_blank" rel="noopener">https://www.jianshu.com/p/4c3081d40ca6</a></p>
<h2 id="1-先说是什么，再说为什么"><a href="#1-先说是什么，再说为什么" class="headerlink" title="1. 先说是什么，再说为什么"></a>1. 先说是什么，再说为什么</h2><ul>
<li><p>归一化：</p>
<p>就是将训练集中某一列<strong>数值</strong>特征（假设是第i列）的值缩放到<strong>0和1</strong>之间。方法如下所示：</p>
<script type="math/tex; mode=display">
\frac{x_i - \min(x)}{\max(x) - \min(x)}</script></li>
<li><p>标准化：</p>
<p>就是将训练集中某一列<strong>数值</strong>特征（假设是第i列）的值缩放成<strong>均值为0，方差为1</strong>的状态。如下所示：</p>
<script type="math/tex; mode=display">
\frac{x_i - \bar{x}}{std(x)}</script></li>
</ul>
<p>  sklearn中<code>MinmaxScaler</code>对应着归一化，<code>StandardScaler</code>对应标准化。</p>
<p>  神经网络中Batch Normalization对应标准化，即把样本拉回到均值为0，方差为1的分布上。</p>
<ul>
<li><p>进一步明确二者含义：</p>
<p>归一化和标准化的相同点都是对<strong>某个特征（column）</strong>进行缩放（scaling）而不是对某个样本的特征向量（row）进行缩放。对特征向量进行缩放是毫无意义的（<strong>暗坑1</strong>），比如三列特征：身高、体重、血压。每一条样本（row）就是三个这样的值，对这个row无论是进行标准化还是归一化都是好笑的，因为你不能将身高、体重和血压混到一起去！<br> 在线性代数中，将一个向量除以向量的长度，也被称为标准化，不过这里的标准化是将向量变为长度为1的单位向量，它和我们这里的标准化不是一回事儿，不要搞混哦（<strong>暗坑2</strong>）。</p>
</li>
</ul>
<h2 id="2-标准化和归一化的对比分析"><a href="#2-标准化和归一化的对比分析" class="headerlink" title="2. 标准化和归一化的对比分析"></a>2. 标准化和归一化的对比分析</h2><p>首先明确，在机器学习中，标准化是更常用的手段，归一化的应用场景是有限的。我总结原因有两点：</p>
<ul>
<li>1、标准化更好保持了样本间距。当样本中有异常点时，归一化有可能将正常的样本“挤”到一起去。比如三个样本，某个特征的值为1,2,10000，假设10000这个值是异常值，用归一化的方法后，正常的1,2就会被“挤”到一起去。如果不幸的是1和2的分类标签还是相反的，那么，当我们用梯度下降来做分类模型训练时，模型会需要更长的时间收敛，因为将样本分开需要更大的努力！而标准化在这方面就做得很好，至少它不会将样本“挤到一起”。</li>
<li>2、标准化更符合统计学假设<br> 对一个数值特征来说，很大可能它是服从正态分布的。标准化其实是基于这个隐含假设，只不过是略施小技，将这个正态分布调整为均值为0，方差为1的标准正态分布而已。</li>
</ul>
<h2 id="3-逻辑回归是否需要标准化"><a href="#3-逻辑回归是否需要标准化" class="headerlink" title="3. 逻辑回归是否需要标准化"></a>3. 逻辑回归是否需要标准化</h2><p>真正的答案是，这取决于我们的逻辑回归是不是用正则。</p>
<p><strong>如果你不用正则，那么，标准化并不是必须的，如果你用正则，那么标准化是必须的。（暗坑3）</strong></p>
<p>因为不用正则时，我们的损失函数<strong>只是仅仅</strong>在度量<strong>预测与真实的差距</strong>，加上正则后，我们的损失函数除了要度量上面的差距外，还要度量<strong>参数值</strong>是否足够小。而<strong>参数值的大小程度或者说大小的级别是与特征的数值范围</strong>相关的。举例来说，我们用体重预测身高，体重用kg衡量时，训练出的模型是：<br>身高 = 体重*x，x就是我们训练出来的参数。</p>
<p>当我们的体重用吨来衡量时，x的值就会扩大为原来的1000倍。<br>在上面两种情况下，都用L1正则的话，显然对模型的训练影响是不同的。</p>
<p>假如不同的特征的数值范围不一样，有的是0到0.1，有的是100到10000，那么，每个特征对应的参数大小级别也会不一样，在L1正则时，我们是简单将参数的绝对值相加，因为它们的大小级别不一样，就会导致L1最后只会对那些级别比较大的参数有作用，那些小的参数都被忽略了。</p>
<p>如果你回答到这里，面试官应该基本满意了，但是他可能会进一步考察你，如果不用正则，那么标准化对逻辑回归有什么好处吗？</p>
<p>答案是有好处，进行标准化后，我们得出的参数值的大小可以反应出不同特征对样本label的<strong>贡献度</strong>，方便我们进行特征筛选。如果不做标准化，是不能这样来筛选特征的。</p>
<p>答到这里，有些厉害的面试官可能会继续问，做标准化有什么注意事项吗？</p>
<p><strong>最大的注意事项就是先拆分出test集，不要在整个数据集上做标准化，因为那样会将test集的信息引入到训练集中，这是一个非常容易犯的错误！</strong></p>
<h2 id="4-决策树需要标准化吗"><a href="#4-决策树需要标准化吗" class="headerlink" title="4. 决策树需要标准化吗"></a>4. 决策树需要标准化吗</h2><p>答案是不需要标准化。因为决策树中的切分依据，信息增益、信息增益比、Gini指数都是基于概率得到的，和值的大小没有关系。另外同属概率模型的朴素贝叶斯，隐马尔科夫也不需要标准化。</p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><ol>
<li><p><strong>搞清楚标准化和归一化的具体定义和区别</strong></p>
</li>
<li><p><strong>标准化要在train_test_split后，在训练集、测试集上分别进行标准化</strong></p>
</li>
<li><p><strong>决策树、朴素贝叶斯等概率模型不需要标准化</strong></p>
</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>Vuex使用</title>
    <url>/2020/05/23/Vuex%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="Vuex使用"><a href="#Vuex使用" class="headerlink" title="Vuex使用"></a>Vuex使用</h2><p>​        <code>Vuex</code>是一个状态管理库，使用单一状态树管理应用内的全局状态。</p>
<p>​        <code>index.js</code>文件如下：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> Vue <span class="keyword">from</span> <span class="string">"vue"</span>;</span><br><span class="line"><span class="keyword">import</span> Vuex <span class="keyword">from</span> <span class="string">"vuex"</span>;</span><br><span class="line"><span class="keyword">import</span> cart <span class="keyword">from</span> <span class="string">"./cart"</span></span><br><span class="line">Vue.use(Vuex);</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> store = <span class="keyword">new</span> Vuex.Store(&#123;</span><br><span class="line">  <span class="comment">//全局的state</span></span><br><span class="line">  <span class="comment">// 如何调用：this.$store.state.状态名 / this.$store.state.模块名.状态名</span></span><br><span class="line">  state: &#123;</span><br><span class="line">    isTabbarShow: <span class="literal">true</span>,</span><br><span class="line">    itemList: [],</span><br><span class="line">  &#125;,</span><br><span class="line">  mutations: &#123;</span><br><span class="line">    <span class="comment">//修改state操作，state为第一个参数，vue会自动注入该函数</span></span><br><span class="line">    <span class="comment">//如何调用：this.$store.commit("函数名",[data])</span></span><br><span class="line">    HideTabbar(state, data) &#123;</span><br><span class="line">      state.isTabbarShow = data;</span><br><span class="line">    &#125;,</span><br><span class="line">    ShowTabbar(state, data) &#123;</span><br><span class="line">      state.isTabbarShow = data;</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;,</span><br><span class="line">  actions: &#123;</span><br><span class="line">    <span class="comment">//主要执行异步操作,</span></span><br><span class="line">    <span class="comment">// 如何调用：this.$store.dispatch("模块名/函数名称",[data])</span></span><br><span class="line">    test(store, data) &#123;</span><br><span class="line">      <span class="comment">//使用commit通过mutation中方法修改state</span></span><br><span class="line">      store.commit(<span class="string">""</span>, data);</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;,</span><br><span class="line">  getters: &#123;</span><br><span class="line">    <span class="comment">// store中的计算属性</span></span><br><span class="line">    <span class="comment">// 调用方式 $store.getters."函数名字"</span></span><br><span class="line">    getItemListTop3(state) &#123;</span><br><span class="line">      <span class="keyword">return</span> state.itemList.slice(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;,</span><br><span class="line">  modules: &#123;</span><br><span class="line">    <span class="comment">//不同模块中可以定义state、mutations、actions，然后包装成一个对象导出，在这儿进行导入，注意在子模块中不需要使用Vuex.Store构造函数。</span></span><br><span class="line">      cart,</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> store;</span><br></pre></td></tr></table></figure>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">cart/index.js</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> state = &#123;</span><br><span class="line">  items: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">const</span> getters = &#123;</span><br><span class="line">  getOddItem(state) &#123;</span><br><span class="line">    <span class="keyword">return</span> state.items.filter(<span class="function">(<span class="params">x</span>) =&gt;</span> x % <span class="number">2</span> == <span class="number">0</span>);</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">const</span> mutations = &#123;</span><br><span class="line">  incre(state) &#123;</span><br><span class="line">    state.items = state.items.map(<span class="function">(<span class="params">x</span>) =&gt;</span> x + <span class="number">2</span>);</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> &#123;<span class="comment">//开启了命名空间</span></span><br><span class="line">  namespaced: <span class="literal">true</span>,</span><br><span class="line">  state,</span><br><span class="line">  getters,</span><br><span class="line">  mutations,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>在<code>main.js</code>中：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> store <span class="keyword">from</span> <span class="string">"@/store"</span>;</span><br><span class="line"></span><br><span class="line">Vue.config.productionTip = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">new</span> Vue(&#123;</span><br><span class="line">  store,</span><br><span class="line">  render: <span class="function">(<span class="params">h</span>) =&gt;</span> h(App),</span><br><span class="line">&#125;).$mount(<span class="string">"#app"</span>);</span><br></pre></td></tr></table></figure>
<p>​        <strong>注意：</strong></p>
<p>​        默认情况下，模块内部的 <strong>action、mutation 和 getter</strong> 是注册在<strong>全局命名空间</strong>的——这样使得多个模块能够对同一 mutation 或 action 作出响应。</p>
<p>​        <strong>即不论是在那个模块定义的，都可以使用</strong><code>this.$store.getters.名字</code>进行调用。如果希望你的模块具有更高的封装度和复用性，你可以通过添加 <code>namespaced: true</code> 的方式使其成为带命名空间的模块。当模块被注册后，它的所有 getter、action 及 mutation 都会自动根据模块注册的路径调整命名。即：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">$store.getters[<span class="string">"cart/getOddItem"</span>]</span><br><span class="line">$store.dispatch(<span class="string">'cart/login'</span>)</span><br><span class="line">$store.commit(<span class="string">'cart/incre)</span></span><br></pre></td></tr></table></figure>
<p>​        可以看到，在vuex中，只允许mutation操作修改state，构成单向数据流。示意图如下：</p>
<p><img src="https://vuex.vuejs.org/vuex.png" alt=""></p>
<p><strong>补充：</strong></p>
<p>vue中计算属性（computed）和方法（methods）的区别：</p>
<p>计算属性以方法形式定义，使用时使用属性的方式进行调用；计算属性只会计算一次，结果缓存在内存中，当其相关联的数据发生改变时才会重新计算。而方法调用一次就会被执行一次。当有关数据处理逻辑较为复杂，或页面中需要多处使用时可以使用计算属性。</p>
]]></content>
      <categories>
        <category>JS</category>
      </categories>
  </entry>
  <entry>
    <title>ES6模块的导出和导入</title>
    <url>/2020/05/22/ES6%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AF%BC%E5%87%BA%E5%92%8C%E5%AF%BC%E5%85%A5/</url>
    <content><![CDATA[<h2 id="ES6模块的导出和导入"><a href="#ES6模块的导出和导入" class="headerlink" title="ES6模块的导出和导入"></a>ES6模块的导出和导入</h2><p>​    es6采用export、import进行模块的导出和导入。</p>
<p>​    假如有如下三个文件，处于相同目录下</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">A.js</span><br><span class="line"><span class="keyword">export</span> <span class="function"><span class="keyword">function</span> <span class="title">a1</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">"a1 run"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="function"><span class="keyword">function</span> <span class="title">a2</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">"a2 run"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">const</span> a3 = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//或者</span></span><br><span class="line"><span class="comment">// export &#123;a1,a2,a3&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">B.js</span><br><span class="line"><span class="keyword">let</span> b1 = <span class="function"><span class="keyword">function</span>(<span class="params">data</span>)</span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(data)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">b2</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">"b2 run"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">export</span> <span class="keyword">const</span> b3 = <span class="number">6</span></span><br><span class="line"><span class="comment">//只可以使用一次 export default</span></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span><br><span class="line">    b1,b2,b3</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">export</span> &#123;</span><br><span class="line">	b1,b3</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">main.js</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> &#123;a3, a2 <span class="keyword">as</span> myfunc&#125; <span class="keyword">from</span> <span class="string">"./A.js"</span></span><br><span class="line"><span class="keyword">import</span> MyB <span class="keyword">from</span> <span class="string">"./B.js"</span> </span><br><span class="line"><span class="keyword">import</span> &#123;b1,b3&#125; <span class="keyword">from</span> <span class="string">"./B.js"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">console</span>.log(a3) <span class="comment">// 5</span></span><br><span class="line">myfunc() <span class="comment">// a2 run</span></span><br><span class="line"></span><br><span class="line">MyB.b2() <span class="comment">// b2 run</span></span><br><span class="line">MyB.b1(<span class="string">"hello world"</span>) <span class="comment">// hello world</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">console</span>.log(b3); <span class="comment">// 6</span></span><br><span class="line">b1(<span class="string">"word"</span>) <span class="comment">// word</span></span><br></pre></td></tr></table></figure>
<p><strong>说明：</strong></p>
<ol>
<li><p><code>export default</code>意味着将大括号中的函数、变量封装成default对象全部导出，且该语句在一个js文件中只可以使用一次。在另一个文件中导入时，导入名字可以随便写。</p>
</li>
<li><p><code>export</code> + 变量/常量/函数，部分导出；在另一个文件进行导入时，导入名称必须和模块中定义的相同，但是可以使用<code>as</code>进行重命名，注意导入时需要使用大括号（<code>ES6</code>中对象的解构赋值）。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span> p = &#123;<span class="attr">name</span>:<span class="string">"bill"</span>, <span class="attr">age</span>: <span class="number">25</span>&#125;</span><br><span class="line"><span class="keyword">let</span> &#123;name,age&#125; = p <span class="comment">//变量名务必和对象属性名相同，否则会得到undefined</span></span><br><span class="line"><span class="built_in">console</span>.log(name + <span class="string">","</span> + age) <span class="comment">// bill 25</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> &#123;log&#125; = <span class="built_in">console</span></span><br><span class="line">log(<span class="string">"hello world"</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>JS</category>
      </categories>
  </entry>
  <entry>
    <title>Pytorch学习笔记之卷积神经网络</title>
    <url>/2020/05/18/Pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h2 id="Pytorch学习笔记之卷积神经网络"><a href="#Pytorch学习笔记之卷积神经网络" class="headerlink" title="Pytorch学习笔记之卷积神经网络"></a>Pytorch学习笔记之卷积神经网络</h2><h3 id="1-模型构建"><a href="#1-模型构建" class="headerlink" title="1 模型构建"></a>1 模型构建</h3><p>在<code>pytorch</code>中，我们构建的模型都继承自<code>torch.nn.Module</code>这个类，并且要重写其<code>forward</code>方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">12</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">12</span> * <span class="number">4</span> * <span class="number">4</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">60</span>)</span><br><span class="line">        self.out = nn.Linear(in_features=<span class="number">60</span>, out_features=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, t)</span>:</span></span><br><span class="line">        <span class="comment"># implement the forward pass</span></span><br><span class="line">        <span class="comment"># (1) input layer</span></span><br><span class="line">		t = t</span><br><span class="line">        <span class="comment"># (2) hidden conv layer</span></span><br><span class="line">        t = self.conv1(t)</span><br><span class="line">        t = F.relu(t)</span><br><span class="line">        t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (3) hidden conv layer</span></span><br><span class="line">        t = self.conv2(t) <span class="comment"># [batch_size, channels, width, height]</span></span><br><span class="line">        </span><br><span class="line">        t = F.relu(t)</span><br><span class="line">        t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (4) hidden linear layer</span></span><br><span class="line">        t = t.reshape(<span class="number">-1</span>, <span class="number">12</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">        t = self.fc1(t)</span><br><span class="line">        t = F.relu(t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (5) hidden linear layer</span></span><br><span class="line">        t = self.fc2(t)</span><br><span class="line">        t = F.relu(t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (6) output layer</span></span><br><span class="line">        t = self.out(t)</span><br><span class="line">        <span class="comment">#t = F.softmax(t, dim=1)</span></span><br><span class="line">        <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line">&gt; network = Network()</span><br><span class="line">&gt; print(network)</span><br><span class="line">Network(</span><br><span class="line">  (conv1): Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (conv2): Conv2d(<span class="number">6</span>, <span class="number">12</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (fc1): Linear(in_features=<span class="number">192</span>, out_features=<span class="number">120</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc2): Linear(in_features=<span class="number">120</span>, out_features=<span class="number">60</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (out): Linear(in_features=<span class="number">60</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line">&gt; network.conv1.weight.shape <span class="comment"># [channels,kernel_nums, kernel_size,kernel_size]</span></span><br><span class="line">torch.Size([<span class="number">6</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">&gt; network.conv2.weight.shape</span><br><span class="line">torch.Size([<span class="number">12</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>总的来说，要经过 3 个步骤：</p>
<ol>
<li><p>继承<code>nn.Moudle</code></p>
</li>
<li><p>在<code>__init__(self)</code>方法中定义神经网络中的<code>layer</code>作为类的属性</p>
</li>
<li>实现<code>forward()</code>方法。</li>
</ol>
<p><strong>tips</strong>：一个多通道卷积与<strong>feature maps</strong>作卷积，结果是一个数值（每个通道卷积最后加起来），因此，<code>output_channels</code>的数量<strong>取决于卷积核的数量</strong>。每一层卷积的参数 = <code>channels * kernel_nums * size</code>。</p>
<p>在定义好模型结构后，就可以输入训练集进行迭代训练直至收敛。</p>
<p>上述网络结构中，<code>feature maps</code>尺寸变化如下：</p>
<p><img src="/images/feature——maps.png" alt=""></p>
<p><strong><code>feature_map</code>变换公式</strong>：</p>
<ul>
<li>图片大小 $W$</li>
<li>卷积核<code>filter</code>大小 $F$</li>
<li>步长<code>stride</code> 大小 $S$</li>
<li>填充<code>padding</code>大小 $P$</li>
<li><code>max_pooling</code>层 核大小 $f$</li>
<li><code>max_pooling</code>层步长 <code>stride</code> $s$</li>
</ul>
<p>经过卷积输出大小 $N$ 有：</p>
<script type="math/tex; mode=display">
N = \frac{W - F + 2P}{S} + 1</script><p>再经过<code>max_pooling</code> ,最终大小 $M$ ,有:</p>
<script type="math/tex; mode=display">
M = \frac{N-f}{s} + 1</script><p>注：<code>nn.Conv2d</code>输出维度：<code>[batch_size, channels, width, height]</code></p>
<h3 id="2-训练你的模型"><a href="#2-训练你的模型" class="headerlink" title="2 训练你的模型"></a>2 训练你的模型</h3><p><strong>训练流程</strong>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. Get batch from the training set.</span><br><span class="line">2. Pass batch to network.</span><br><span class="line">3. Calculate the loss (difference between the predicted values and the true values).</span><br><span class="line">4. Calculate the gradient of the loss function w.r.t the network&#39;s weights.</span><br><span class="line">5. Update the weights using the gradients to reduce the loss.（反向传播）</span><br><span class="line">6. Repeat steps 1-5 until one epoch is completed.</span><br><span class="line">7. Repeat steps 1-6 for as many epochs required to reach the minimum loss.</span><br></pre></td></tr></table></figure>
<p><strong>batch</strong>：每次输入神经网络中的数据集数量。</p>
<p><strong>epoch</strong>：遍历完整个数据集称之为一个<code>epoch</code>。</p>
<p>整个流程代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="comment">#from plotcm import plot_confusion_matrix</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"></span><br><span class="line">torch.set_printoptions(linewidth=<span class="number">120</span>)</span><br><span class="line"><span class="comment">#训练数据集的加载</span></span><br><span class="line">train_set = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">'./data'</span></span><br><span class="line">    ,train=<span class="literal">True</span></span><br><span class="line">    ,download=<span class="literal">True</span></span><br><span class="line">    ,transform=transforms.Compose([</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ])</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set</span><br><span class="line">    ,batch_size=<span class="number">1000</span></span><br><span class="line">    ,shuffle=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_num_correct</span><span class="params">(preds, labels)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> preds.argmax(dim=<span class="number">1</span>).eq(labels).sum().item()</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练</span></span><br><span class="line">network = Network()</span><br><span class="line">torch.set_grad_enabled(<span class="literal">True</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">100</span>)</span><br><span class="line">optimizer = optim.Adam(network.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 要遍历10次整个数据集</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    total_correct = <span class="number">0</span></span><br><span class="line">	<span class="comment"># batchsize = 1000</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader: <span class="comment"># Get Batch</span></span><br><span class="line">        images, labels = batch</span><br><span class="line"></span><br><span class="line">        preds = network(images) <span class="comment"># Pass Batch</span></span><br><span class="line">        loss = F.cross_entropy(preds, labels) <span class="comment"># Calculate Loss</span></span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward() <span class="comment"># Calculate Gradients</span></span><br><span class="line">        optimizer.step() <span class="comment"># Update Weights</span></span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        total_correct += get_num_correct(preds, labels)</span><br><span class="line"></span><br><span class="line">    print(</span><br><span class="line">        <span class="string">"epoch"</span>, epoch,</span><br><span class="line">        <span class="string">"total_correct:"</span>, total_correct,</span><br><span class="line">        <span class="string">"loss:"</span>, total_loss</span><br><span class="line">    )</span><br><span class="line">total_correct/len(train_set) <span class="comment"># 0.8858833333333334</span></span><br></pre></td></tr></table></figure>
<p>最后可以看出，在训练集上的准确率为 $88\%$ 左右。</p>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epoch <span class="number">0</span> total_correct: <span class="number">51138</span> loss: <span class="number">238.7455054372549</span></span><br><span class="line">epoch <span class="number">1</span> total_correct: <span class="number">52016</span> loss: <span class="number">216.4094382673502</span></span><br><span class="line">epoch <span class="number">2</span> total_correct: <span class="number">52269</span> loss: <span class="number">206.6615267843008</span></span><br><span class="line">epoch <span class="number">3</span> total_correct: <span class="number">52513</span> loss: <span class="number">201.51278421282768</span></span><br><span class="line">epoch <span class="number">4</span> total_correct: <span class="number">52504</span> loss: <span class="number">197.78098802268505</span></span><br><span class="line">epoch <span class="number">5</span> total_correct: <span class="number">52791</span> loss: <span class="number">192.42419914901257</span></span><br><span class="line">epoch <span class="number">6</span> total_correct: <span class="number">52802</span> loss: <span class="number">193.69900572299957</span></span><br><span class="line">epoch <span class="number">7</span> total_correct: <span class="number">53002</span> loss: <span class="number">187.62913002073765</span></span><br><span class="line">epoch <span class="number">8</span> total_correct: <span class="number">53087</span> loss: <span class="number">183.71526048332453</span></span><br><span class="line">epoch <span class="number">9</span> total_correct: <span class="number">53153</span> loss: <span class="number">182.18467965722084</span></span><br></pre></td></tr></table></figure>
<p>在测试集上的表现如何？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_set = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">'./data'</span>,</span><br><span class="line">    train= <span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">False</span>,</span><br><span class="line">    transform=transforms.Compose([</span><br><span class="line">        transforms.ToTensor()])</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_set,batch_size=<span class="number">500</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> correct</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> test_loader:</span><br><span class="line"></span><br><span class="line">        images,labels = batch</span><br><span class="line">        preds = network(images)</span><br><span class="line">        preds = preds.argmax(dim = <span class="number">1</span>)</span><br><span class="line">        correct += (preds == labels).sum()</span><br><span class="line">        print(correct)</span><br><span class="line">print(correct.item() *<span class="number">1.0</span> / len(test_set)) <span class="comment">#0.8631</span></span><br></pre></td></tr></table></figure>
<p>在测试集上的准确率为 $86.31\%$。仍然有很大改善空间。</p>
<p>下面观察以下经过<code>conv1</code>、<code>conv2</code>后的<code>feature maps</code>。（没有经过最大池化）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sets = iter(train_loader)</span><br><span class="line">batch = next(sets)</span><br><span class="line">images,labels = batch</span><br><span class="line">input = images[<span class="number">0</span>].unsqueeze(<span class="number">0</span>) <span class="comment">#[1, 1, 28, 28] 升维，增加一个维度 batch</span></span><br><span class="line">network = Network()</span><br><span class="line">feature1 = network.conv1(input)</span><br><span class="line">feature2 = network.conv2(feature1)</span><br><span class="line">feature1_ = feature1.squeeze() <span class="comment">#[6,24,24]</span></span><br><span class="line">feature2_ = feature2.squeeze() <span class="comment">#[12,20,20]</span></span><br><span class="line"></span><br><span class="line">ch1 = feature1_[<span class="number">5</span>]</span><br><span class="line">ch1_ = ch1.data.numpy()</span><br><span class="line">ch2 = feature2_[<span class="number">5</span>]</span><br><span class="line">ch2_ = ch2.data.numpy()</span><br><span class="line"></span><br><span class="line">plt.imshow(ch1_)</span><br><span class="line">plt.show()</span><br><span class="line">plt.imshow(ch2_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/f1_6.png" alt=""></p>
<center>conv1后第6通道的feature</center>

<p><img src="/images/f2_6.png" alt=""></p>
<center>conv2后第6通道的feature</center>

<h3 id="3-记点其他的知识"><a href="#3-记点其他的知识" class="headerlink" title="3 记点其他的知识"></a>3 记点其他的知识</h3><h4 id="3-1-Batch-Normalization"><a href="#3-1-Batch-Normalization" class="headerlink" title="3.1 Batch Normalization"></a>3.1 Batch Normalization</h4><p>在<code>Pytorch</code>中，位于<code>torch.nn</code>包下。[ <a href="https://pytorch.org/docs/stable/nn.html#batchnorm2d" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#batchnorm2d</a> ]</p>
<p><strong>提出原因</strong>：解决梯度消失问题， 加速训练收敛过程 。</p>
<p><strong>原理</strong>：将隐层的输入分布变为均值为 $0$ ，方差为 $1$ 的正态分布。</p>
<p>设$X = \{x_1,x_2,…,x_m\}$表示$X$的维度。$m$ 代表 $batch-size$.</p>
<script type="math/tex; mode=display">
x = Wu + b</script><p>计算样本每个维度均值：</p>
<script type="math/tex; mode=display">
\mu_B = \frac{1}{m} \sum_{i = 1}^{m} x_i</script><p>计算样本每个维度的方差：</p>
<script type="math/tex; mode=display">
\sigma_B^2 = \frac{1}{m} \sum_{i = 1}^{m} (x_i - \mu_B)^2</script><script type="math/tex; mode=display">
\hat x_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}</script><script type="math/tex; mode=display">
y_i = \gamma\hat x_i + \beta = BN_{\gamma,\beta}(x_i)</script><p>其中，$\gamma$、$\beta$ 是通过学习得到的。</p>
<p><strong>如何操作</strong>：即将<code>input</code>与<code>weight matrix</code>相乘后，再作为激活函数的输入。在<code>CNN</code>中，是做完卷积操作后，激活函数之前。在训练时，均值、方差都是训练时<code>batch</code>的统计数据，可以记下然后做加权平均得到测试时使用的均值和方差，在测试时使用。</p>
<p><img src="/images/20160522210927345.png" alt=""></p>
<center>全连接层，bn层的位置</center>

<p><img src="/images/cnn-bn.png" style="zoom:70%;" /></p>
<center>一般卷积操作中，bn层的位置</center>

<p><strong>进一步的理解</strong>：</p>
<p>在原论文<code>3.2</code>节，作者提到 $BN$ 通常加入到<strong>非线性单元（激活函数）之前</strong>，对 $Wu + b$ 进行<code>normalizing</code>，而不是 $u$，作者是这样解释的：$u$ 可能是上一个非线性单元的输出，非线性单元的输出分布形状会在训练过程中变化，归一化无法消除他的方差偏移。</p>
<p>而对于 $Wu+b$ （线性变换，卷积操作也是线性变换）这种变换的输出一般是一个对称，非稀疏的一个分布，更加类似高斯分布，对他们进行归一化会产生更加稳定的分布。</p>
<p>看一下 $W u + b$ 经过$BN$后，参数 $b$ 的变化：</p>
<script type="math/tex; mode=display">
\mu = \frac{1}{m} \sum_{i = 1}^{m} Wu_i + b = \frac{1}{m}\sum_{i=1}^{m}Wu_i + \frac{bm}{m}</script><script type="math/tex; mode=display">
\hat x_i = \frac{Wu_i + b- \mu}{\sqrt{\sigma^2 + \epsilon}} = \frac{Wu_i - \frac{1}{m}\sum_{i=1}^{m}Wu_i}{\sqrt{\sigma^2 + \epsilon}}</script><p>可以看到参数$b$最后被消掉了，可有可无。因此可以表示为：</p>
<script type="math/tex; mode=display">
z = g(BN(Wu + b)) \rightarrow z = g(BN(Wu))</script><p><strong>We could have also normalized the layer inputs u, but since u is likely the output of another nonlinearity, the shape of its distribution is likely to change during training, and constraining its first and second moments would not eliminate the co-variate shift.</strong></p>
<p><strong>In contrast, Wu + b is more likely to have a symmetric, non-sparse distribution, that is “more Gaussian”; normalizing it is likely to produce activations with a stable distribution.</strong></p>
<p><strong>经过卷积后怎么 BN</strong></p>
<p>通常经过卷积操作后，会产生多个<code>channel</code>。将每个<code>channel</code>视为一个维度，统计<code>channel</code>内所有样本的均值和方差，这样每个<code>channel</code>对应一对参数$\gamma,\beta$。比如：<code>[batch_size,channel,width,height]</code>，每个<code>channel</code>内，<code>batch_size * width * height</code>的均值和方差。</p>
<h4 id="3-2-Dropout"><a href="#3-2-Dropout" class="headerlink" title="3.2 Dropout"></a>3.2 Dropout</h4><p>在<code>Pytorch</code>中，位于<code>torch.nn</code>包下。</p>
<p><strong>提出原因</strong>：解决过拟合问题。</p>
<h4 id="3-3-卷积数学定义"><a href="#3-3-卷积数学定义" class="headerlink" title="3.3 卷积数学定义"></a>3.3 卷积数学定义</h4><p><strong>离散卷积</strong></p>
<script type="math/tex; mode=display">
(f*g)(n) = \sum_{\tau = -m}^{m} f(\tau)g(n-\tau)</script><p><strong>连续卷积</strong></p>
<script type="math/tex; mode=display">
(f*g)(n) = \int_{\tau = -m}^{\tau = m} f(\tau)g(n-\tau)</script><p>事实上，我们在二维卷积的时候，使用的卷积核<script type="math/tex">(g)</script>是经过反转后的，为了方便计算。在进行一维卷积时，也需要对核进行反转（左右反转）。</p>
<p>参考：<a href="https://www.cnblogs.com/itmorn/p/11177439.html" target="_blank" rel="noopener">https://www.cnblogs.com/itmorn/p/11177439.html</a></p>
<p>计算：<a href="https://www.nowcoder.com/questionTerminal/0a3fc6ff7d89441db100fdd00ce22132?orderByHotValue=1&amp;page=1&amp;onlyReference=false" target="_blank" rel="noopener">https://www.nowcoder.com/questionTerminal/0a3fc6ff7d89441db100fdd00ce22132?orderByHotValue=1&amp;page=1&amp;onlyReference=false</a></p>
<h4 id="3-4-Attention-机制"><a href="#3-4-Attention-机制" class="headerlink" title="3.4 Attention 机制"></a>3.4 Attention 机制</h4><p><strong>传统 Encoder-Decoder 模型</strong>：</p>
<p> 在传统的模型中， 我们仅使用 Encoder 的最后一个隐状态作为 Decoder 的初始隐状态，Encoder 最后的隐状态被称为<strong>context vector</strong>向量，因为他对整个输入的 sentence 做了一个编码。在之后的 Decoder 模型中，<code>Decoder</code>初始<code>input</code>为<code>SOS(start of string) token</code>，初始隐状态为这个 context vector，然后接受上一次的<code>output</code>作为<code>input</code>迭代完成训练。</p>
<p><img src="/images/encoder-decoder.png" alt=""></p>
<p><strong>加入了 Attention 机制的 Encoder-Decoder 模型</strong>：</p>
<p> 标准$seq2seq$模型通常无法准确处理长输入序列，因为只有编码器的最后一个隐藏状态被用作解码器的上下文向量。 另一方面，注意力机制在解码过程中保留并利用了输入序列的所有隐藏状态(context vector)，因此直接解决了此问题。 它通过在解码器输出的每个时间步长到所有编码器隐藏状态之间创建唯一的映射来实现此目的。 这意味着，对于解码器产生的每个输出，它都可以访问整个输入序列，并且可以从该序列中有选择地选择特定元素以产生输出。 相对于传统 LSTM 记忆网络处理长度较长的序列，加入 Attention 后参数减少。</p>
<p><img src="/images/1152PYf.png" alt=""></p>
<p> Attention 机制分为不同的种类，但总的来说大概分为以下几步：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> Calculating Alignment Scores</span><br><span class="line"><span class="number">2.</span> Softmaxing alignment scores to get Attention weights（归一化）</span><br><span class="line"><span class="number">3.</span> Multiplying the Attention weights <span class="keyword">with</span> encoder outputs/all hidden states to get the context vector(加权求和)</span><br><span class="line"><span class="number">4.</span> Concatenating context vector <span class="keyword">with</span> embedded input word</span><br></pre></td></tr></table></figure>
<p>具体实现参考：</p>
<p><a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" target="_blank" rel="noopener">https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html</a></p>
<p><a href="https://blog.floydhub.com/attention-mechanism/" target="_blank" rel="noopener">https://blog.floydhub.com/attention-mechanism/</a></p>
<p><img src="/images/attention-decoder-network.png" alt=""></p>
<p> 下面介绍<strong>NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</strong>论文中提到的 Attention 机制。</p>
<ul>
<li><p>首先将序列经过 Encoder 模型，产生所有隐状态$H_{Encoder}$</p>
</li>
<li><p>计算 Alignment Scores:</p>
<p>在 Decoder 模型中，时间 $i$ 对应的前一个隐状态为$s_{i-1}$，</p>
<script type="math/tex; mode=display">
e_{ij} = \alpha(s_{i-1},h_j),j=1,2,3,...,T_x</script><p>表示不同$h_j$对$s_{i}$的影响程度。即将 $s_{i-1}$ 与每一个 Encoder 的隐状态经过一个函数得到输出，这个函数的参数通过学习得到。</p>
</li>
<li><p>$soft\max$归一化，得到 attention weights</p>
<script type="math/tex; mode=display">
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_X} \exp(e_{ik})}</script></li>
</ul>
<ul>
<li><p>计算 context vector:</p>
<script type="math/tex; mode=display">
c_i = \sum_{j=1}^{T_x} \alpha_{ij}h_j</script><p><strong>注意</strong>：此时$c_i$仍是一个多维向量，相加的时候是各个维度分别相加。</p>
</li>
<li><p>将 context vector 与$t-1$的 output 作为输入（ _concatenated_ ），与$s_{i-1}$传入 decoder 模型中得到输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 所有encoder隐状态 共有3个隐态，维度为4</span></span><br><span class="line">hidden_states = np.array([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">-1</span>,<span class="number">3</span>,<span class="number">6</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得出的attention权重</span></span><br><span class="line">weights = np.array([[<span class="number">0.6</span>,<span class="number">0.3</span>,<span class="number">0.1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># weights和hidden_states 相乘</span></span><br><span class="line">np.dot(weights,hidden_states)</span><br><span class="line"><span class="comment">#各个维度相加得到context_vector</span></span><br><span class="line">array([[<span class="number">1.2</span>, <span class="number">2.</span> , <span class="number">3.6</span>, <span class="number">5.1</span>]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="/images/attn_mem.jpeg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BahdanauDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size, output_size, n_layers=<span class="number">1</span>, drop_prob=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    super(BahdanauDecoder, self).__init__()</span><br><span class="line">    self.hidden_size = hidden_size</span><br><span class="line">    self.output_size = output_size</span><br><span class="line">    self.n_layers = n_layers</span><br><span class="line">    self.drop_prob = drop_prob</span><br><span class="line"></span><br><span class="line">    self.embedding = nn.Embedding(self.output_size, self.hidden_size)</span><br><span class="line"></span><br><span class="line">    self.fc_hidden = nn.Linear(self.hidden_size, self.hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">  </span><br><span class="line">    self.fc_encoder = nn.Linear(self.hidden_size, self.hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment">#也可以用nn.Linear代替</span></span><br><span class="line">    self.weight = nn.Parameter(torch.FloatTensor(<span class="number">1</span>, hidden_size))</span><br><span class="line">    self.attn_combine = nn.Linear(self.hidden_size * <span class="number">2</span>, self.hidden_size)</span><br><span class="line">    self.dropout = nn.Dropout(self.drop_prob)</span><br><span class="line">    self.lstm = nn.LSTM(self.hidden_size*<span class="number">2</span>, self.hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">    self.classifier = nn.Linear(self.hidden_size, self.output_size)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, hidden, encoder_outputs)</span>:</span></span><br><span class="line">    encoder_outputs = encoder_outputs.squeeze()</span><br><span class="line">    <span class="comment"># Embed input words</span></span><br><span class="line">    embedded = self.embedding(inputs).view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    embedded = self.dropout(embedded)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculating Alignment Scores</span></span><br><span class="line">    x = torch.tanh(self.fc_hidden(hidden[<span class="number">0</span>])+self.fc_encoder(encoder_outputs))</span><br><span class="line">    alignment_scores = x.bmm(self.weight.unsqueeze(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Softmaxing alignment scores to get Attention weights</span></span><br><span class="line">    attn_weights = F.softmax(alignment_scores.view(<span class="number">1</span>,<span class="number">-1</span>), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Multiplying the Attention weights with encoder outputs to get the context vector</span></span><br><span class="line">    context_vector = torch.bmm(attn_weights.unsqueeze(<span class="number">0</span>),</span><br><span class="line">                             encoder_outputs.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Concatenating context vector with embedded input word</span></span><br><span class="line">    output = torch.cat((embedded, context_vector[<span class="number">0</span>]), <span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># Passing the concatenated vector as input to the LSTM cell</span></span><br><span class="line">    output, hidden = self.lstm(output, hidden)</span><br><span class="line">    <span class="comment"># Passing the LSTM output through a Linear layer acting as a classifier</span></span><br><span class="line">    output = F.log_softmax(self.classifier(output[<span class="number">0</span>]), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> output, hidden, attn_weights</span><br></pre></td></tr></table></figure>
<h3 id="4-卷积网络之电影评论情感分类"><a href="#4-卷积网络之电影评论情感分类" class="headerlink" title="4. 卷积网络之电影评论情感分类"></a>4. 卷积网络之电影评论情感分类</h3><p>​        参考：<a href="https://github.com/bentrevett/pytorch-sentiment-analysis" target="_blank" rel="noopener">https://github.com/bentrevett/pytorch-sentiment-analysis</a></p>
<p>​        采用<code>IMDB</code>数据集，使用<code>glove.6B.100d</code>预训练好的词向量。label总共有两类：消极评价：<code>neg</code>，积极评价：<code>pos</code>。</p>
<p>​        使用卷积网络用来扫描词向量组成的矩阵，即使用过滤器（filters）扫描embedding矩阵，这里介绍的模型使用三个大小不同的filter，每个filter共100个，filter的宽度和embedding_dim相同，高度分为3、4、5，即filter每次扫描的单词个数分别为3、4、5，filter每次移动1个单词距离。最后的向量shape为<code>[lenth_of_the_word  - height_of_the_filter + 1, 1]</code>。将经过不同filter后的向量拼接起来然后经过全连接得到最终的预测结果。</p>
<p><img src="/images/sentiment.png" alt="filter示意图"></p>
<p><img src="/images/网络结构.jpg" alt=""></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch学习笔记之循环神经网络</title>
    <url>/2020/05/18/PyTorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h2 id="PyTorch学习笔记之循环神经网络"><a href="#PyTorch学习笔记之循环神经网络" class="headerlink" title="PyTorch学习笔记之循环神经网络"></a>PyTorch学习笔记之循环神经网络</h2><p>循环神经网络的提出是为了解决序列数据。如：翻译、语言识别、时间序列问题等。</p>
<h3 id="1-基本结构"><a href="#1-基本结构" class="headerlink" title="1 基本结构"></a>1 基本结构</h3><p>循环神经网络（Recurrent Neutral Network）不同于卷积网络，刚开始接触时个人感觉比较抽象。</p>
<p>在每个 cell 中，激活函数输入的不仅包含这个时序内输入的数据$x^{<t>}$，而且还包括上个时序的输出 $a^{t-1}$(隐藏态)。因此，可以表示为：</p>
<script type="math/tex; mode=display">
a^{\langle t \rangle} = g(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)</script><p>其中，$W_{aa}$表示 $t-1$ 时段输出的权重，$W_{ax}$表示 $t$ 时段输入的权重。之后，$a^{\langle t \rangle}$再向 $t+1$ 时段传播，或者经过$soft\max$函数，作为 $t$ 时段的输出。</p>
<p><img src="/images/RNN.png" alt=""></p>
<p><img src="/images/LSTM1.png" alt=""></p>
<center>基本RNN结构</center>

<p><strong>多层 RNN 结构</strong>：</p>
<p><img src="/images/multi_rnn.png" alt=""></p>
<p><strong>双向RNN结构</strong></p>
<p><img src="https://d2l.ai/_images/birnn.svg" alt=""></p>
<p>需要注意的是：前向和后向的隐藏态互不干扰和影响，每个时间步的输出由前向和后向的隐藏态共同决定。</p>
<p><strong>RNN 的特点</strong>：</p>
<ol>
<li>RNNs 主要用于处理序列数据。对于传统神经网络模型，从输入层到隐含层再到输出层，层与层之间一般为全连接，每层之间神经元是无连接的。但是传统神经网络无法处理数据间的前后关联问题。例如，为了预测句子的下一个单词，一般需要该词之前的语义信息。这是因为一个句子中前后单词是存在语义联系的。</li>
<li>RNNs 中当前单元的输出与之前步骤输出也有关，因此称之为循环神经网络。具体的表现形式为当前单元会对之前步骤信息进行储存并应用于当前输出的计算中。隐藏层之间的节点连接起来，隐藏层当前输出由当前时刻输入向量和之前时刻隐藏层状态共同决定。</li>
<li>标准的 RNNs 结构图，图中每个箭头代表做一次变换，也就是说箭头连接带有权值。</li>
<li>在标准的 RNN 结构中，隐层的神经元之间也是带有权值的，且权值共享。</li>
</ol>
<h3 id="2-LSTM-和-GRU"><a href="#2-LSTM-和-GRU" class="headerlink" title="2 LSTM 和 GRU"></a>2 LSTM 和 GRU</h3><h4 id="2-1-LSTM"><a href="#2-1-LSTM" class="headerlink" title="2.1 LSTM"></a>2.1 LSTM</h4><p><strong>三个门控：遗忘门、输入门、输出门</strong></p>
<p><img src="/images/LSTM.png" alt=""></p>
<p><img src="/images/LSTM2.png" alt=""></p>
<p><img src="/images/lstm-simple.jpg" alt=""></p>
<p><img src="/images/lstm-normal.jpg" alt=""></p>
<p><img src="/images/lstm-peephole.jpg" alt=""></p>
<center>基本LSTM结构</center>

<p><strong>提出原因</strong>：</p>
<p>$RNN$ 在处理长期依赖（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，会造成<strong>梯度消失或者梯度膨胀</strong>的现象。为了解决该问题，研究人员提出了许多解决办法，例如 ESN（Echo State Network），增加有漏单元（Leaky Units）等等。其中最成功应用最广泛的就是门限 RNN（Gated RNN），而 LSTM 就是门限 RNN 中最著名的一种。有漏单元通过设计连接间的权重系数，从而允许 RNN 累积距离较远节点间的长期联系；而门限 RNN 则泛化了这样的思想，允许在不同时刻改变该系数，且允许网络忘记当前已经累积的信息。</p>
<p>在<code>torch.nn.LSTM</code>中，返回两个值，一个是<code>cell</code>，一个是<code>hidden</code>。cell即记忆值，hidden即最后一个时间步的隐藏态。</p>
<h4 id="2-2-GRU"><a href="#2-2-GRU" class="headerlink" title="2.2 GRU"></a>2.2 GRU</h4><p><strong>提出原因</strong></p>
<p>解决长距离梯度消失的问题</p>
<p><strong>具体结构</strong></p>
<p><strong>两个门：update gate和reset gate</strong></p>
<p><strong>Update gate</strong></p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*9aIpAyB1Jlubq0ktVEmKmw.png" alt=""></p>
<p><strong>Reset gate</strong></p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*ifB-DfmokJKQqNzMjDpMQQ.png" alt=""></p>
<p><code>torch.nn.GRU</code> 具体参数和<code>nn.RNN</code>类似</p>
<h3 id="3-Pytorch-构建-RNN"><a href="#3-Pytorch-构建-RNN" class="headerlink" title="3 Pytorch 构建 RNN"></a>3 Pytorch 构建 RNN</h3><script type="math/tex; mode=display">
a^{\langle t \rangle} = g(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)</script><p>根据计算公式，为了提高运算速度，在具体实现经常采用矩阵相乘的形式。一般来说，可以分解为如下形式：</p>
<script type="math/tex; mode=display">
\left[
 \begin{matrix}
 a^{\langle t-1 \rangle} & x^{\langle t \rangle}
  \end{matrix}
\right]*
\left[
 \begin{matrix}
  W_{aa} \\
  W_{ax}
  \end{matrix}
\right]</script><p>即，将$W_{aa},W_{ax}$按照<strong>列</strong>进行拼接，将$a^{\langle t-1 \rangle},x^{\langle t \rangle}$ 按照<strong>行</strong>进行拼接。下面确定两个权重矩阵的维度。</p>
<p>如果假设输入($input$)的维度为 $x_m$，$hidden$ 的维度为$h_n$。可知$W_{aa},W_{ax}$ 的列维度一定是相同的，否则不能按$1*D$的大小，经过激活函数成为隐藏态，那么$D$的大小必然要和$hidden$的维度相同。因此$W_{aa},W_{ax}$的维度分别为：</p>
<script type="math/tex; mode=display">
W_{aa} \rightarrow \left[h_n, h_n\right],W_{ax} \rightarrow [x_m, h_n]</script><p>在<code>Pytorch</code>中，使用<code>torch.nn.RNN</code>来完成<code>RNN</code>的设计。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt; <span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">&gt; rnn = nn.RNN(input_size=<span class="number">5</span>,hidden_size=<span class="number">10</span>,num_layers=<span class="number">1</span>)</span><br><span class="line">&gt; rnn.weight_hh_l0.shape <span class="comment">#hidden to hidden 即W_aa</span></span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">&gt; rnn.weight_ih_l0.shape <span class="comment"># input to hidden 即W_ax</span></span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">5</span>])</span><br><span class="line">&gt; input = torch.randn(<span class="number">6</span>,<span class="number">12</span>,<span class="number">5</span>) <span class="comment"># 构造一个批次为10，input_dim为5,序列长度为6的矩阵，即每个序列长6，共有12个这样的序列，序列中每个时间步的dim为5</span></span><br><span class="line"><span class="comment"># input [src_len, batch, dim]</span></span><br><span class="line">&gt; output,hidden = rnn(input)</span><br><span class="line">&gt; output.shape</span><br><span class="line">(torch.Size([<span class="number">6</span>, <span class="number">12</span>, <span class="number">10</span>])</span><br><span class="line"> <span class="comment">#[src_len, batch, num_directions * hidden_size]</span></span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>在循环神经网络中，</p>
<script type="math/tex; mode=display">
h_t = \tanh(w_{ih}x_t + b_{ih} + w_{hh}h_{t-1}+b_{hh})</script><p>每个时间步都会输出一个<code>hid</code>向量，<code>output</code>就是所有<code>hid</code>向量形成的矩阵，而<code>hidden</code>向量是最后一个时间步产生的<code>hid</code>向量。将<code>hid</code>向量经过全连接或<code>tanh</code>等函数就会得到这个时间步的输出。</p>
<p>要构建双向循环神经网络，将<code>bidirectional</code>参数设为<code>True</code>即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt; input = torch.randn(<span class="number">20</span>,<span class="number">128</span>,<span class="number">12</span>) <span class="comment">#[src_len, batch_size, embedded_dim]</span></span><br><span class="line">&gt; gru = nn.GRU(<span class="number">12</span>, <span class="number">512</span>, bidirectional = <span class="literal">True</span>)</span><br><span class="line">&gt; outputs,hidden = gru(input)</span><br><span class="line">&gt; outputs.shape,hidden.shape</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">128</span>, <span class="number">1024</span>]),torch.Size([<span class="number">2</span>, <span class="number">128</span>, <span class="number">512</span>])</span><br></pre></td></tr></table></figure>
<p><strong>输出维度解释</strong>：</p>
<p><code>output</code>：<code>[src len, batch size, hid dim * num directions]</code>，注意第三个维度，因为是双向，因此从前向后会产生hidden，而从后向前也会产生hidden，即一个时间步由两个hidden产生，第三个维度就是<code>[hid*2]</code>。单向的话就是<code>[hid*1]</code>。</p>
<p><code>output[:,:,0]</code>代表前向产生的hidden，<code>output[:,:,1]代表后向产生的hidden</code>。</p>
<p><code>hidden</code>：<code>[n layers * num directions, batch size, hid dim]</code>。</p>
<p><code>hidden[0,:,:]</code>所有前向产生的最后一个hidden向量，即到了序列的最后一个state，<code>hidden[1,:,:]</code>所有后向产生的第一个hidden向量，即到了序列的第一个state。</p>
<p>同<code>nn.Conv2d</code>不同的是，<code>rnn</code>的<code>batch_size</code>是第2个维度。</p>
<p>下面实际看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt; a = torch.randint(<span class="number">10</span>,(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))<span class="comment"># 假设这是hidden hid dim = 4, batch = 3, 前向3个hidden </span></span><br><span class="line">&gt; a</span><br><span class="line">tensor([[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">8</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">2</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">         [<span class="number">7</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">6</span>]]])</span><br><span class="line">&gt; a[<span class="number">0</span>,:,:] <span class="comment"># 前向产生的hidden</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">8</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>]])</span><br><span class="line">&gt; torch.cat((a[<span class="number">-2</span>,:,:], a[<span class="number">-1</span>,:,:]), dim = <span class="number">1</span>).shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">8</span>])</span><br><span class="line">&gt; torch.cat((a[<span class="number">-2</span>,:,:], a[<span class="number">-1</span>,:,:]), dim = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 即将每个句子的前向和后向合并成一个vector，经过合并后的hidden经过线性变换和激活函数可以传入decoder作为context向量或计算attention权重</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">8</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<p>attention权重生成的一种方式：</p>
<script type="math/tex; mode=display">
weights = soft\max(W_1\tanh(W_2[h_{decoder\_hidden},h_{encoder\_hiddens}]))</script><p>$W_1$、$W_2$可以是全连接层得到（<code>nn.Linear</code>）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, enc_hid_dim, dec_hid_dim)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        </span><br><span class="line">        self.attn = nn.Linear((enc_hid_dim * <span class="number">2</span>) + dec_hid_dim, dec_hid_dim)</span><br><span class="line">        self.v = nn.Linear(dec_hid_dim, <span class="number">1</span>, bias = <span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden, encoder_outputs)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#hidden = [batch size, dec hid dim]</span></span><br><span class="line">        <span class="comment">#encoder_outputs = [src len, batch size, enc hid dim * 2]</span></span><br><span class="line">        </span><br><span class="line">        batch_size = encoder_outputs.shape[<span class="number">1</span>]</span><br><span class="line">        src_len = encoder_outputs.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#repeat decoder hidden state src_len times</span></span><br><span class="line">        hidden = hidden.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, src_len, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        encoder_outputs = encoder_outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#hidden = [batch size, src len, dec hid dim]</span></span><br><span class="line">        <span class="comment">#encoder_outputs = [batch size, src len, enc hid dim * 2]</span></span><br><span class="line">        </span><br><span class="line">        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = <span class="number">2</span>))) </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#energy = [batch size, src len, dec hid dim]</span></span><br><span class="line"></span><br><span class="line">        attention = self.v(energy).squeeze(<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#attention= [batch size, src len]</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> F.softmax(attention, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#在每一行上进行softmax，即每一个单词的权重</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>微信小程序开发入门指南</title>
    <url>/2020/05/13/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<h2 id="微信小程序开发入门指南"><a href="#微信小程序开发入门指南" class="headerlink" title="微信小程序开发入门指南"></a>微信小程序开发入门指南</h2><h3 id="0-概述"><a href="#0-概述" class="headerlink" title="0. 概述"></a>0. 概述</h3><p>​        小程序是典型的前后端分离开发的架构，前端界面使用微信官方推出的<code>wxml</code>、<code>wxss</code>、<code>js</code>进行开发，发布时这些代码被微信官方托管，而后端开发可以使用任意语言，如：<code>Node.js</code>、<code>Python</code>、<code>PHP</code>、<code>Java</code>等，只需提供数据交互的<code>api</code>接口即可。需要注意的是，后端服务器必须有合法的域名才可以与前端界面提供数据交（微信强制要求），具体配置在微信公众平台 （小程序）=&gt; 开发 =&gt; 开发设置 =&gt; 服务器域名中，进行配置。</p>
<h3 id="1-入门"><a href="#1-入门" class="headerlink" title="1. 入门"></a>1. 入门</h3><p>​        小程序的项目架构大致如下：</p>
<p><img src="/images/project-structure.png" alt="image-20200517000550738"></p>
<p>pages主要存放各个页面的<code>wxss/wxml/js/json</code>文件，pages下一个文件夹即代表一个具体页面，文件命名应和页面文件夹名称相同（如：<code>login-&gt;login.wxml,login.js,login.json,login.wxss</code>）。</p>
<p>images存放系统所需图片资源。</p>
<p><code>app.js/app.json/app.wxss</code>都是创建项目时自动生成的，<code>app.json</code>用于配置前端路由，小程序整体设置等信息（例如tab bar、导航栏颜色和标题等），<code>app.wxss</code>用于配置全局样式。</p>
<p><strong>前端组件</strong>：详情参见微信官方文档</p>
<p><strong>数据绑定</strong>：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">index.js:</span><br><span class="line"></span><br><span class="line">Page(&#123;</span><br><span class="line">    data:&#123;</span><br><span class="line">        msg:<span class="string">"hello world"</span>,</span><br><span class="line">        number: <span class="number">5</span>,</span><br><span class="line">        person:&#123;</span><br><span class="line">            name:<span class="string">'bill'</span>,</span><br><span class="line">            age:<span class="string">'25'</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    func1()&#123;&#125;,</span><br><span class="line">    func2()&#123;&#125;</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">index.wxml:</span><br><span class="line">&lt;!--编译后页面显示 hello world--&gt;</span><br><span class="line">&lt;text&gt;&#123;&#123;msg&#125;&#125;&lt;<span class="regexp">/text&gt;</span></span><br><span class="line"><span class="regexp">&lt;!--属性绑定--&gt;</span></span><br><span class="line"><span class="regexp">&lt;view class="num-&#123;&#123;number&#125;&#125;"&gt;&lt;/</span>view&gt;</span><br><span class="line">&lt;view&gt;</span><br><span class="line">	姓名：&lt;text&gt;&#123;&#123;person.name&#125;&#125;&lt;<span class="regexp">/text&gt;</span></span><br><span class="line"><span class="regexp">	年龄：&lt;text&gt;&#123;&#123;person.age&#125;&#125;&lt;/</span>text&gt;</span><br><span class="line">&lt;<span class="regexp">/view&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>网络请求</strong>：</p>
<p><code>wx.request</code>可以实现异步请求，具体格式如下：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">wx.request(&#123;</span><br><span class="line">    url:<span class="string">'http://www.xxx.com'</span>,</span><br><span class="line">    data:&#123;<span class="comment">//请求提交的具体参数</span></span><br><span class="line">        </span><br><span class="line">    &#125;,</span><br><span class="line">    method:<span class="string">'post'</span>,</span><br><span class="line">    header: &#123;</span><br><span class="line">    	<span class="string">'content-type'</span>: <span class="string">'application/json'</span> <span class="comment">// 默认值</span></span><br><span class="line">  	&#125;,</span><br><span class="line">    success()&#123;<span class="comment">//请求成功的回调函数</span></span><br><span class="line">        </span><br><span class="line">    &#125;,</span><br><span class="line">    fail()&#123;<span class="comment">//请求失败的回调函数</span></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>小程序开发</category>
      </categories>
  </entry>
  <entry>
    <title>Spark学习笔记</title>
    <url>/2020/05/13/Spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="Spark学习笔记"><a href="#Spark学习笔记" class="headerlink" title="Spark学习笔记"></a>Spark学习笔记</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p>​        Spark是一个计算框架，可以整合Hadoop的HDFS和其他资源调度器s（YARN、Mesos、K8s）。</p>
<h3 id="2-Spark安装"><a href="#2-Spark安装" class="headerlink" title="2. Spark安装"></a>2. Spark安装</h3><h4 id="2-1-Local模式"><a href="#2-1-Local模式" class="headerlink" title="2.1 Local模式"></a>2.1 Local模式</h4><h4 id="2-2-Standalone模式"><a href="#2-2-Standalone模式" class="headerlink" title="2.2 Standalone模式"></a>2.2 Standalone模式</h4><h4 id="2-3-Yarn模式"><a href="#2-3-Yarn模式" class="headerlink" title="2.3 Yarn模式"></a>2.3 Yarn模式</h4><h3 id="3-Spark设计与运行流程"><a href="#3-Spark设计与运行流程" class="headerlink" title="3. Spark设计与运行流程"></a>3. Spark设计与运行流程</h3><h4 id="3-1-Spark基本概念"><a href="#3-1-Spark基本概念" class="headerlink" title="3.1 Spark基本概念"></a>3.1 Spark基本概念</h4><p>Driver、Master、Worker、Executor、RDD、DAG、Application、Job、Stage、Tasks、Partition</p>
<p><strong>一个Application可以划分为多个Job，当遇到一个Action操作后就会触发一个Job的计算；一个Job可划分为多个Stage，出现shuffle就划分一个阶段（ShuffleMapStage）；一个Stage包含多个Partition，一个Partition对应一个Task，因此一个Stage就是一个TaskSet</strong>。</p>
<p>划分Job依据：</p>
<p>注：有的Transformation算子也会被划分为一个Job。</p>
<p>application -&gt; jobs -&gt; stage -&gt; tasks。</p>
<h4 id="3-2-Spark架构"><a href="#3-2-Spark架构" class="headerlink" title="3.2 Spark架构"></a>3.2 Spark架构</h4><p><img src="C:\Users\ASUS\Pictures\spark.jpg" alt=""></p>
<h4 id="3-3-Spark运行流程"><a href="#3-3-Spark运行流程" class="headerlink" title="3.3 Spark运行流程"></a>3.3 Spark运行流程</h4><p>宽依赖：发生了Shuffle，就是宽依赖（不可以并行处理）一个父RDD对应多个儿子RDD分区，这些所有儿子RDD在没有得到父RDD分区数据时，不能干别的，只能等待。比如groupByKey操作。Shuffle会引发写磁盘操作(spill to disk)。</p>
<p>窄依赖： 一个父RDD对应一个儿子RDD，或多个父RDD对应一个儿子RDD。可以进行流水线优化（不发生磁盘写操作），对其中一个父RDD传过来的数据就可以处理，儿子RDD无需等待所有父RDD数据到达。filter、map操作。</p>
<p>Spark的DAGScheduler会根据程序生成的DAG确定宽依赖、窄依赖，进而划分作业到不同的阶段。</p>
<h4 id="3-4-Scheduler模块源码分析"><a href="#3-4-Scheduler模块源码分析" class="headerlink" title="3.4 Scheduler模块源码分析"></a>3.4 Scheduler模块源码分析</h4><p>​        Spark的任务调度是从DAG切割开始，主要是由DAGScheduler来完成。当遇到一个Action操作后就会触发一个Job的计算，并交给DAGScheduler来提交，下图是涉及到Job提交的相关方法调用流程图。</p>
<p>Stage的调度</p>
<p><img src="/images/spark-scheduler-dag-process.png" alt=""></p>
<p>Task级的调度</p>
<p><img src="/images/spark-scheduler-task-process.png" alt=""></p>
<p>​        DAGScheduler将Stage打包到TaskSet交给TaskScheduler，TaskScheduler会将其封装为TaskSetManager加入到调度队列中，TaskSetManager负责监控管理同一个Stage中的Tasks，TaskScheduler就是以TaskSetManager为单元来调度任务。</p>
<p>​        TaskScheduler支持两种调度策略，一种是FIFO，也是默认的调度策略，另一种是FAIR。</p>
<p>​        从调度队列中拿到TaskSetManager后，那么接下来的工作就是TaskSetManager按照一定的规则一个个取出Task给TaskScheduler，TaskScheduler再交给SchedulerBackend去发到Executor上执行。前面也提到，TaskSetManager封装了一个Stage的所有Task，并负责管理调度这些Task。(<a href="http://sharkdtu.com/posts/spark-scheduler.htm" target="_blank" rel="noopener">http://sharkdtu.com/posts/spark-scheduler.htm</a>)</p>
<h4 id="3-5-1-单机模式"><a href="#3-5-1-单机模式" class="headerlink" title="3.5.1 单机模式"></a>3.5.1 单机模式</h4><p>独立集群（stand alone）模式：该模式，Spark既负责计算，也负责资源的调度。有Master结点和Slaves结点，Master负责资源调度，Slaves负责执行计算（Executor）。</p>
<p>Yarn模式：Yarn负责资源调度，Spark负责计算。</p>
<p>yarn-client：driver位于提交job的机器上，实时展示job运行情况</p>
<p><img src="C:\Users\ASUS\Pictures\yarn-client.png" alt=""></p>
<p><strong>执行流程：</strong></p>
<p>1.客户端提交一个Application，在客户端启动一个Driver进程。</p>
<p>2.Driver进程会向RS(ResourceManager)发送请求，启动AM(ApplicationMaster)。</p>
<p>3.RS收到请求，随机选择一台NM(NodeManager)启动AM。这里的NM相当于Standalone中的Worker节点。</p>
<p>4.AM启动后，会向RS请求一批container资源，用于启动Executor。</p>
<p>5.RS会找到一批NM返回给AM,用于启动Executor。AM会向NM发送命令启动Executor。</p>
<p>6.Executor启动后，会反向注册给Driver，Driver发送task到Executor,执行情况和结果返回给Driver端。</p>
<p>yarn-cluster：driver位于集群中某个结点。</p>
<p><img src="/images/spark-submit-time.png" alt=""></p>
<p><strong>执行流程：</strong></p>
<p>1.客户机提交Application应用程序，发送请求到RS(ResourceManager),请求启动AM(ApplicationMaster)。</p>
<p>2.RS收到请求后随机在一台NM(NodeManager)上启动AM，</p>
<p>3.AM启动，AM拿到客户机提交的程序的代码，运行Driver进程；AM发送请求到RS，请求一批container用于启动Executor。</p>
<p>3.RS返回一批NM节点给AM，。</p>
<p>4.AM连接到NM,发送请求到NM在Container启动Executor。</p>
<p>5.Executor反向注册到AM所在的节点的Driver。Driver发送task到Executor。</p>
<p>NodeManager会向AM报告container资源情况，而Executor会向Driver报告计算情况。AM一个负责资源调度、一个负责计算，在cluster模式时。</p>
<h4 id="3-6-RDD编程"><a href="#3-6-RDD编程" class="headerlink" title="3.6 RDD编程"></a>3.6 RDD编程</h4><p>​        Resilient Distributed Dataset（弹性分布式数据集）是多个分区（Partition）的集合。一个RDD对象包含一个或多个Partition。</p>
<p>​        RDD操作（算子）类型：transformations和actions，前者不会进行计算（只生成新的RDD对象），后者会引起真正的计算（runJob进而划分阶段提交task，driver分发task给Executor去计算）。</p>
<p>RDD创建：</p>
<p>1.集合中创建：<code>parallelize</code>/<code>makeRDD</code></p>
<p>2.外部存储：<code>textFile</code></p>
<p>3.其他RDD转换</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">&gt;<span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">"a_b"</span>,<span class="string">"c_d"</span>,<span class="string">"e_f"</span>))</span><br><span class="line">&gt;rdd.flatMap(_.split(<span class="string">"_"</span>)).foreach(println)</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line">c</span><br><span class="line">d</span><br><span class="line">e</span><br><span class="line">f</span><br><span class="line">&gt;rdd.map(_.split(<span class="string">"_"</span>)).foreach(println)</span><br><span class="line">a,b</span><br><span class="line">c,d</span><br><span class="line">e,f</span><br></pre></td></tr></table></figure>
<p><code>mapPartition</code>、<code>foreachPartiton</code> 容易造成内存溢出（OOM）。</p>
<p><code>collect</code>会把所有数据拉取到Driver结点上。</p>
<p><strong>广播变量</strong></p>
<p>​        广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用，减少网络传输开销，优化性能。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，甚至是机器学习算法中的一个很大的特征向量，广播变量用起来都很顺手。 在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">broadcastVar: org.apache.spark.broadcast.<span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Broadcast</span>(<span class="number">35</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; broadcastVar.value</span><br><span class="line">res33: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>​        使用广播变量的过程如下：</p>
<p>​        (1) 通过对一个类型 T 的对象调用 SparkContext.broadcast 创建出一个 Broadcast[T] 对象。 任何可序列化的类型都可以这么实现。 </p>
<p>​        (2) 通过 value 属性访问该对象的值(在 Java 中为 value() 方法)。 </p>
<p>​        (3) 变量只会被发到各个节点一次，应作为只读值处理(修改这个值不会影响到别的节点)。</p>
<p><strong>多文件排序</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">HashPartitioner</span>, <span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03</span> </span>&#123;</span><br><span class="line">  <span class="comment">//实现多文件排序</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"demo"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> files = sc.textFile(<span class="string">"data/input2/*"</span>)</span><br><span class="line">    <span class="comment">//有多少个文件就会有多少个分区</span></span><br><span class="line">    println(<span class="string">"分区数量："</span>+files.partitions.size)</span><br><span class="line">    <span class="keyword">val</span> result = files.filter(_.trim.length &gt; <span class="number">0</span>)</span><br><span class="line">      .map(x =&gt; (x.toInt,<span class="number">1</span>))</span><br><span class="line">      .partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">1</span>))<span class="comment">//转化为一个分区,没有的话就会各个文件分开排序</span></span><br><span class="line">      .sortBy(x=&gt;x)</span><br><span class="line">      .map(_._1)</span><br><span class="line"></span><br><span class="line">    result.saveAsTextFile(<span class="string">"result"</span>)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>找出多个文件中的最大值和最小值</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark02</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"demo"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> files = sc.textFile(<span class="string">"data/input2/*"</span>)</span><br><span class="line"></span><br><span class="line">    files.map(x=&gt; (<span class="string">"key"</span>,x.toInt))</span><br><span class="line">      .groupByKey()<span class="comment">//转到一个分区上</span></span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        <span class="keyword">var</span> min = <span class="type">Integer</span>.<span class="type">MAX_VALUE</span></span><br><span class="line">        <span class="keyword">var</span> max = <span class="type">Integer</span>.<span class="type">MIN_VALUE</span></span><br><span class="line">        <span class="keyword">for</span>(num &lt;- x._2)&#123;</span><br><span class="line">          <span class="keyword">if</span> (num &gt; max)&#123;</span><br><span class="line">            max = num</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> (num &lt; min)&#123;</span><br><span class="line">            min = num</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        (min,max)</span><br><span class="line">      &#125;)</span><br><span class="line">      .foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>Transformation</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>map</strong>(<em>func</em>)</td>
<td>Return a new distributed dataset formed by passing each element of the source through a function <em>func</em>.</td>
</tr>
<tr>
<td><strong>filter</strong>(<em>func</em>)</td>
<td>Return a new dataset formed by selecting those elements of the source on which <em>func</em> returns true.</td>
</tr>
<tr>
<td><strong>flatMap</strong>(<em>func</em>)</td>
<td>Similar to map, but each input item can be mapped to 0 or more output items (so <em>func</em> should return a Seq rather than a single item).</td>
</tr>
<tr>
<td><strong>mapPartitions</strong>(<em>func</em>)</td>
<td>Similar to map, but runs separately on each partition (block) of the RDD, so <em>func</em> must be of type Iterator<T> =&gt; Iterator<U> when running on an RDD of type T.</td>
</tr>
<tr>
<td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>
<td>Similar to mapPartitions, but also provides <em>func</em> with an integer value representing the index of the partition, so <em>func</em> must be of type (Int, Iterator<T>) =&gt; Iterator<U> when running on an RDD of type T.</td>
</tr>
<tr>
<td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>
<td>Sample a fraction <em>fraction</em> of the data, with or without replacement, using a given random number generator seed.</td>
</tr>
<tr>
<td><strong>distinct</strong>([<em>numPartitions</em>]))</td>
<td>Return a new dataset that contains the distinct elements of the source dataset.</td>
</tr>
<tr>
<td><strong>groupByKey</strong>([<em>numPartitions</em>])</td>
<td>When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. <strong>Note:</strong> If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better performance. <strong>Note:</strong> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional <code>numPartitions</code> argument to set a different number of tasks.</td>
</tr>
<tr>
<td><strong>reduceByKey</strong>(<em>func</em>, [<em>numPartitions</em>])</td>
<td>When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <em>func</em>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument.</td>
</tr>
<tr>
<td><strong>sortByKey</strong>([<em>ascending</em>], [<em>numPartitions</em>])</td>
<td>When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td>
</tr>
<tr>
<td><strong>join</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</td>
<td>When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.</td>
</tr>
<tr>
<td><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td>Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>Action</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>Aggregate the elements of the dataset using a function <em>func</em> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>Return the first element of the dataset (similar to take(1)).</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>Return an array with the first <em>n</em> elements of the dataset.</td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.</td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>Run a function <em>func</em> on each element of the dataset. This is usually done for side effects such as updating an <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators" target="_blank" rel="noopener">Accumulator</a> or interacting with external storage systems. <strong>Note</strong>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-a-nameclosureslinka" target="_blank" rel="noopener">Understanding closures </a>for more details.</td>
</tr>
<tr>
<td><strong>count</strong>()</td>
<td>Return the number of elements in the dataset.</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>Spring Boot配置文件的加载位置</title>
    <url>/2020/04/29/Spring%20Boot%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E5%8A%A0%E8%BD%BD%E4%BD%8D%E7%BD%AE/</url>
    <content><![CDATA[<h2 id="Spring-Boot配置文件的加载位置"><a href="#Spring-Boot配置文件的加载位置" class="headerlink" title="Spring Boot配置文件的加载位置"></a>Spring Boot配置文件的加载位置</h2><p>整个项目路径如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">project</span><br><span class="line">|----config</span><br><span class="line">|--------application.properties(优先级<span class="number">1</span>)</span><br><span class="line">|----src</span><br><span class="line">|--------main</span><br><span class="line">|------------java</span><br><span class="line">|------------resources</span><br><span class="line">|----------------config</span><br><span class="line">|--------------------application.properties(优先级<span class="number">3</span>)</span><br><span class="line">|----------------application.properties(优先级<span class="number">4</span>)</span><br><span class="line">|--------test</span><br><span class="line">|----application.properties(优先级<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>从配置文件的位置来看，配置文件的优先级从高到低如下：</p>
<ol>
<li><code>file:/config</code></li>
<li><code>file:/</code></li>
<li><code>classpath:/config</code></li>
<li><code>classpath:/</code></li>
</ol>
<p><strong>注意</strong>：</p>
<ul>
<li><p>Spring Boot会从以上四个位置全部加载配置文件，且在高优先级配置文件中配置的内容会覆盖低优先级配置文件中的。</p>
</li>
<li><p>支持互补配置</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>Word Embedding</title>
    <url>/2020/04/17/Word%20Embedding/</url>
    <content><![CDATA[<h2 id="Word-Embedding之skip-gram"><a href="#Word-Embedding之skip-gram" class="headerlink" title="Word Embedding之skip-gram"></a>Word Embedding之skip-gram</h2><h3 id="1-Word-Embedding"><a href="#1-Word-Embedding" class="headerlink" title="1.Word Embedding"></a>1.Word Embedding</h3><p>​        词的分布式表示大致可以分为基于矩阵、基于聚类、基于神经网络等表示方法。1986年，Hinton就提出了分布式表示想法；2003年，Bengio也发表论文表达了相关想法。基于神经网络的分布式表示一般称为词向量、词嵌入（word embedding）、分布式表示（ distributed representation）。词嵌入就是指将<strong>词汇映射到实数向量</strong>空间的概念（模型）。神经网络词向量表示技术通过神经网络技术对上下文，以及上下文与目标词之间的关系进行建模。神经网络模型有skip-gram和Continous Bag of Words Model(CBOW)模型。</p>
<p>​        word2vec是谷歌2013年提出一种word embedding 的工具或者算法集合，在word2vec中给出了这skip-gram和cbow模型的训练和生成方式。</p>
<h3 id="2-skip-gram"><a href="#2-skip-gram" class="headerlink" title="2. skip gram"></a>2. skip gram</h3><p>​        由于词为字符串，而输入到网络中的是向量，因此传统常用one-hot表示，把每个词表示为一个很长的向量，但这个向量是非常稀疏的，且向量不包含其他含义，因此，word embedding就是找到n个属性来表示这个词，这种表示方法不仅可以大大降低维度，而且可以通过向量计算两个词的“距离”。比如少年、男人、少女这几个词，就可以选择年龄、性别这两个属性来描述。</p>
<p>​        skip-gram其实就是训练一个小型神经网络，将单词映射到向量空间只是其中的一步。<strong>该神经网络最后的输出，其实是词库中各个单词出现在给定输入单词附近的概率——给定中心词，预测附近单词。</strong></p>
<p>​        整体网络架构为三层（输入层、隐藏层、输出层）。假设有一个词汇量为10000的词库。</p>
<p>输入层为单词的one-hot表示向量（10000维），隐藏层没有激活函数，输出层采用softmax函数，输出一个概率分布（10000维向量），表示词汇表各个单词在输入词附近（nearby）的概率。隐藏层的神经元数量就是单词映射到实数空间属性个数。<strong>输入层和隐藏层之间的权重就是我们需要的，也就是作为词汇最后的映射空间。</strong></p>
<p><strong>网络模型</strong>：</p>
<p>输入：$word_{1\times v}、target$</p>
<p>参数：$W_{v \times N}$、$W_{N\times V}$</p>
<p>输出：$P_{1\times V}$</p>
<p><img src="/images/model.jpg" alt=""></p>
<p><strong>训练过程</strong>：</p>
<ol>
<li><p>训练数据的产生</p>
<p>训练数据采用滑动窗口，在一个句子中滑动，窗口大小是一个超参数。比如：</p>
</li>
</ol>
<p>   <img src="/images/word2vec.jpg" alt=""></p>
<p>   中心单词not，窗口大小为2。此次滑动产生的数据为：</p>
<p>   <img src="/images/2.jpg" alt=""></p>
<p>   以此类推。</p>
<p>   <img src="/images/3.jpg" alt=""></p>
<ol>
<li><p>训练</p>
<p>将产生的数据输入网络，计算输出层产生的概率向量和target word的one-hot向量的loss，然后反向传播更新参数矩阵。</p>
</li>
</ol>
<p>   <img src="/images/4.png" alt=""></p>
<h3 id="3-Negative-Sampling和Hierarchical-softmax"><a href="#3-Negative-Sampling和Hierarchical-softmax" class="headerlink" title="3. Negative Sampling和Hierarchical softmax"></a>3. Negative Sampling和Hierarchical softmax</h3><p>​        在2013年初<code>word2vec</code>论文发表后，10月谷歌又发了一篇文章，提出了negative sampling和Hierarchical softmax，两种方法都可以加速网络的收敛过程和训练速度。</p>
<h4 id="3-1-Negative-Sampling"><a href="#3-1-Negative-Sampling" class="headerlink" title="3.1 Negative Sampling"></a>3.1 Negative Sampling</h4><p>​        以上过程有助于了解网络是如何运作的，但是该网络采用softmax激活函数，每次训练都要计算隐藏层向量和参数矩阵2相乘并反向计算梯度，训练成本较高，因此word2vec中使用了基于负例采样（negative sampling）的skip gram以此来降低计算成本。具体就是将softmax函数转化为逻辑回归（二分类），将预测概率问题转为一个二分类问题，输出target是否为input的nearby。负采样就是在数据集中加入target不是input的nearby的样本。</p>
<p>​        采用negative sampling的skip-gram大致训练过程如下：</p>
<p>​        选择与input word相邻的单词作为output word，target为1，表示相邻，再随机选择5-20个不相邻单词作为output word，target为0，将input word输入到网络中经过隐藏层得到embedding后的vector，与output word对应的weight（参数矩阵2的某一列）点乘得到向量积，再经过sigmod函数，得到一个分数，与target的差即为loss，反向传播然后更新参数矩阵1（Embedding矩阵）中input word对应的weight，和参数矩阵2（Context矩阵）中output word对应的weight，这样每次训练只需要更新部分权重（负例对应的Context权重、input word对应的Embedding权重），而非整个权重，从而加快收敛过程。</p>
<p><img src="/images/word2vec-training-update.png" alt=""></p>
<h4 id="3-2-Hierarchical-softmax"><a href="#3-2-Hierarchical-softmax" class="headerlink" title="3.2 Hierarchical softmax"></a>3.2 Hierarchical softmax</h4><p>​        Hierarchical softmax利用词汇建立一棵哈夫曼树，树中每个结点除了包含指向左右子树结点的指针，还有对应结点的权重向量。树中每个节点都相当于一个二分类模型，计算左右结点的概率，最终到达叶子节点。</p>
<p><img src="/images/Hierarchical-Softmax.jpg" alt=""></p>
<h3 id="4-后续"><a href="#4-后续" class="headerlink" title="4. 后续"></a>4. 后续</h3><p>​        word2vec不仅可以用来处理nlp问题，而且基本已经成为深度学习中的一个基本模型。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>强烈推荐！！！</p>
<p>[1]. <a href="http://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener">http://jalammar.github.io/illustrated-word2vec/</a></p>
<p>[2]. <a href="https://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&amp;mid=2651669277&amp;idx=2&amp;sn=bc8f0590f9e340c1f1359982726c5a30&amp;chksm=bd4c648e8a3bed9817f30c5a512e79fe0cc6fbc58544f97c857c30b120e76508fef37cae49bc&amp;scene=0&amp;xtrack=1#rd（[1]中文翻译版）" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&amp;mid=2651669277&amp;idx=2&amp;sn=bc8f0590f9e340c1f1359982726c5a30&amp;chksm=bd4c648e8a3bed9817f30c5a512e79fe0cc6fbc58544f97c857c30b120e76508fef37cae49bc&amp;scene=0&amp;xtrack=1#rd（[1]中文翻译版）</a></p>
<p>[3]. <a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" target="_blank" rel="noopener">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>提升树和梯度提升树</title>
    <url>/2020/04/12/%E6%8F%90%E5%8D%87%E6%A0%91%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91/</url>
    <content><![CDATA[<h2 id="提升树和梯度提升树"><a href="#提升树和梯度提升树" class="headerlink" title="提升树和梯度提升树"></a>提升树和梯度提升树</h2><p>​        集成学习主要分为<strong>boosting</strong>、<strong>bagging</strong>、<strong>stacking</strong>，随机森林属于<strong>bagging</strong>，而本文介绍的提升算法属于<strong>boosting</strong>，同为该类的还有<strong>Adaboost</strong>。提升方法实际采用加法模型（基函数的线性组合）与前向分布算法。</p>
<p>​        注：阅读前请熟悉<strong>CART</strong>回归树的构建过程。</p>
<h3 id="1-提升树"><a href="#1-提升树" class="headerlink" title="1. 提升树"></a>1. 提升树</h3><p>​        根据《统计学习方法》中提升树的定义，提升树是以分类树或回归树为基本分类器的提升方法。以决策树为基函数的提升方法称为提升树。提升树可以表示为决策树的加法模型。即</p>
<script type="math/tex; mode=display">
f_M(x) = \sum_{m=1}^{M} T(x;\Theta_m)</script><p>，其中$M$为决策树的个数，$\Theta$为决策树的参数（若是回归树，参数就是在哪个维度哪个点进行分裂）。</p>
<p>首先确定初始的提升树$f_0(x) = 0$，第$m$步的模型表达式是：</p>
<script type="math/tex; mode=display">
f_m(x) = f_{m-1}(x) + T(x;\Theta_m)</script><p>意思就是前一次的模型的值与第$m$步训练出来的决策树的和。</p>
<p>在第$m$步，</p>
<script type="math/tex; mode=display">
\hat \Theta_m = arg\min_{\Theta_m} \sum_{i=1}^{N}L(y_i,f_{m-1}(x) + T(x;\Theta_m))</script><p>$L$为损失函数，在第$m$步时，$f_{m-1}(x) + T(x;\Theta_m)$即为$x$的预测值。$f_{m-1}(x)$的值已经确定（常数）。</p>
<p>在回归问题中，$L$为二次函数，</p>
<script type="math/tex; mode=display">
L(y_i,f_{m-1}(x) + T(x;\Theta_m)) = [y -f_{m-1}(x) - T(x;\Theta_m ]^2 
=[r -  T(x;\Theta_m)]^2</script><p>$r = y - f_{m-1}(x)$，为<strong>残差</strong>（真实值-预测值）。当$L$最小即$L = 0$，$r = T$，即这一棵树在拟合残差。在第$m$步，训练的这棵树是在拟合$f_{m-1}(x)$与$y$的差值即残差，以此不停的迭代。</p>
<p>​        个人想法：当在构建回归树时，采用了贪心的构建算法，因此，针对相同的$y$值，构建无数次都是同一棵树。因此，在提升树算法中，我们通过损失函数看到了，回归树是在拟合残差，即每一步输入到回归树中的y值是$y - f_{m-1}(x)$。</p>
<h3 id="2-梯度提升算法"><a href="#2-梯度提升算法" class="headerlink" title="2. 梯度提升算法"></a>2. 梯度提升算法</h3><p>​        针对提升树的优化问题，当损失函数是二次函数时，可以看到很好优化，但当损失函数为其他形式时，没有更好的优化方法，或者说优化比较困难。因此，$Fridemam$提出了梯度提升算法来优化提升树。回归问题和分类问题的区别是损失函数定义的不同。</p>
<p>针对回归问题的梯度提升算法，介绍如下：</p>
<p>（1）初始化</p>
<script type="math/tex; mode=display">
f_0(x) = arg\min_c \sum_{i=1}^N L(y_i,c)</script><p>（2）对$m = 1,2,…M$</p>
<p>​        1）对$i = 1,2,3,…,N$ 计算</p>
<script type="math/tex; mode=display">
r_{mi} = - [\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x) = f_{m-1}(x)}</script><p>​        2）针对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶子结点$R_{mj},j = 1,2,…,J$</p>
<p>​        3）对每个叶子结点$j = 1,2,…,J$，根据损失函数计算该叶子节点的权值（和1一样的问题，只有一个根节点的树）</p>
<script type="math/tex; mode=display">
c_{mj} =arg\min_c \sum_{x_i\in R_{mj}} L(y_i,c + f_{m-1}(x_i))</script><p>​        4）更新$f_m(x) = f_{m-1}(x) + \sum_{j=1}^J c_{mj}I(x\in R_{mj})$</p>
<p>​    （3）得到最终的梯度提升树。</p>
<h3 id="3-梯度提升树（GBDT）"><a href="#3-梯度提升树（GBDT）" class="headerlink" title="3. 梯度提升树（GBDT）"></a>3. 梯度提升树（GBDT）</h3><p>​        当梯度提升的损失函数为二次函数时，恰好，损失函数负梯度 == 伪残差。</p>
<script type="math/tex; mode=display">
r_{mi} = - [\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x) = f_{m-1}(x)}\\
\frac{\partial \frac{1}{2}(y_i-f(x_i)^2}{\partial f(x_i)} = -(y-f(x_i)) \\
\frac{\partial \frac{1}{2}(y_i-f(x_i)^2}{\partial^2 f(x_i)} = 1</script><p>还有就是，当损失函数为二次函数，第一步初始化的值是所有$y$值的均值。原因如下：</p>
<script type="math/tex; mode=display">
f_0(x) = arg\min_c \sum_{i=1}^N L(y_i,c) \\=  arg\min_c\sum_{i=1}^N (y_i-c)^2</script><p>令$p(c) = \sum_{i=1}^N (y_i-c)^2$，$\partial p/ \partial c = -2\sum_{i=1}^N(y_i-c) = 0$，$c = (y_1 + y_2 + … + y_N) / N$。即初始化时，</p>
<script type="math/tex; mode=display">
f_0(x) = \frac{y_1+y_2 + ... + y_N}{N}</script><p>与<strong>梯度下降</strong>不同的是，梯度提升是在<strong>函数空间</strong>求梯度，把函数作为参数来看待。而梯度下降是在<strong>参数空间</strong>求梯度。</p>
<p>当损失函数为二次函数，每个<strong>叶子结点最后的权值</strong>是划到该叶子结点所有y值的均值。(和初始化问题一样)。</p>
<h3 id="4-XGBoost"><a href="#4-XGBoost" class="headerlink" title="4. XGBoost"></a>4. XGBoost</h3><p>​        xgboost与梯度提升树不同的是目标函数中加入了正则项，以及使用了二阶导数（目标函数泰勒展开式中用到了），结果更为精确，同时支持分布式并行计算（特征维度），在大规模机器学习中速度很快。本章介绍xgboost的推导，以及xgboost框架中是如何构建一棵xgboost树的。二阶泰勒展开：</p>
<script type="math/tex; mode=display">
f(x+\Delta x) \approx f(x) + f^{\\'}(x)\Delta x + \frac{1}{2}\ddot{f(x)}\Delta x^2</script>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>寻找重复数</title>
    <url>/2020/03/20/%E5%AF%BB%E6%89%BE%E9%87%8D%E5%A4%8D%E6%95%B0/</url>
    <content><![CDATA[<h3 id="1-寻找重复数"><a href="#1-寻找重复数" class="headerlink" title="1. 寻找重复数"></a>1. 寻找重复数</h3><p>​        给定一个包含 <em>n</em> + 1 个整数的数组 <em>nums</em>，其数字都在 1 到 <em>n</em> 之间（包括 1 和 <em>n</em>），可知至少存在一个重复的整数。假设只有一个重复的整数，找出这个重复的数。这是<code>Leetcode 287</code>号问题。</p>
<ul>
<li><p>解法1：排序，然后看相邻元素是否是否相等</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">findDuplicate</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123; </span><br><span class="line">		Arrays.sort(nums);</span><br><span class="line">        <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i] == nums[i - <span class="number">1</span>]) &#123;</span><br><span class="line">                ans = nums[i];</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>解法2：hashmap统计次数或集合判重。</p>
</li>
<li><p>解法3：双指针</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">     <span class="comment">// 数组中的值代表了索引，因为是1-n之间的数，数组包含n+1个数字，不会越界</span></span><br><span class="line">     <span class="comment">// nums[fast]-&gt;具体数值 以nums[fast]作为索引得到-&gt;nums[nums[fast]]</span></span><br><span class="line">     <span class="comment">// nums = [2,5,9,6,9,3,8,9,7,1] 构造成链表</span></span><br><span class="line">     <span class="comment">// 2-&gt;[9]-&gt;1-&gt;5-&gt;3-&gt;6-&gt;8-&gt;7-&gt;[9]-&gt;1-&gt;5-&gt;3...如果存在重复数字，有环存在，找到环的入口。</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">findDuplicate</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123; </span><br><span class="line">		<span class="keyword">int</span> fast = <span class="number">0</span>, slow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            fast = nums[nums[fast]];</span><br><span class="line">            slow = nums[slow];</span><br><span class="line">            <span class="comment">// System.out.println("fast=" + fast + ",slow=" + slow);</span></span><br><span class="line">            <span class="keyword">if</span> (fast == slow)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> finder = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            finder = nums[finder];</span><br><span class="line">            slow = nums[slow];</span><br><span class="line">            System.out.println(<span class="string">"finder="</span> + finder + <span class="string">",slow="</span> + slow);</span><br><span class="line">            <span class="keyword">if</span> (finder == slow)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> slow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-环形链表II"><a href="#2-环形链表II" class="headerlink" title="2. 环形链表II"></a>2. 环形链表II</h3><p>解法3的链表形式。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ListNode</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> val;</span><br><span class="line">        ListNode next;</span><br><span class="line"></span><br><span class="line">        ListNode(<span class="keyword">int</span> x) &#123;</span><br><span class="line">            val = x;</span><br><span class="line">            next = <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title">detectCycle</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">        ListNode slow = head, fast = head;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (fast == <span class="keyword">null</span> || fast.next == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            slow = slow.next;</span><br><span class="line">            fast = fast.next.next;</span><br><span class="line">            <span class="keyword">if</span> (slow == fast)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (head == slow)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="comment">// System.out.println(head.val);</span></span><br><span class="line">            head = head.next;</span><br><span class="line">            slow = slow.next;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> slow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        ListNode head = <span class="keyword">new</span> ListNode(<span class="number">3</span>);</span><br><span class="line">        ListNode second = <span class="keyword">new</span> ListNode(<span class="number">2</span>);</span><br><span class="line">        ListNode third = <span class="keyword">new</span> ListNode(<span class="number">0</span>);</span><br><span class="line">        ListNode fourth = <span class="keyword">new</span> ListNode(-<span class="number">4</span>);</span><br><span class="line">        head.next = second;</span><br><span class="line">        second.next = third;</span><br><span class="line">        third.next = fourth;</span><br><span class="line">        <span class="comment">// fourth.next = second;</span></span><br><span class="line">        System.out.println(detectCycle(head).val);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-关于为何会在环的入口相遇的证明："><a href="#3-关于为何会在环的入口相遇的证明：" class="headerlink" title="3. 关于为何会在环的入口相遇的证明："></a>3. 关于为何会在环的入口相遇的证明：</h3><p>程序中，<code>fast</code>每次循环移动两次，<code>slow</code>每次移动一次，也就是说<code>fast</code>的速度是<code>slow</code>的2倍。</p>
<p>我们假设从索引为$0$元素开始，到环入口元素的距离为$k$，我们把入口元素作为环的起点。</p>
<p>当<code>slow</code>指针刚到达入口元素时，经过了 $k$ 次移动，即：从数组第一个元素到环入口元素有 $k$ 个距离，而此时，<code>fast</code>指针已经在环上了，且领先<code>slow</code>指针 $k$ 个距离，接下来就要分情况讨论了，假设环的周长为 $C$:</p>
<ol>
<li><p>当<script type="math/tex">k < \frac{C}{2}</script> 时,也就是说fast当前位置在环的上半部分，<code>fast</code>在<code>slow</code>的前面（顺时针移动）。</p>
<p>如果此时<code>slow</code>与<code>fast</code>相遇，<code>fast</code>一定会比<code>slow</code>多跑一圈，假设经过了 $t_1$ 时间，<code>fast</code>跑完一圈又回到了<code>slow</code>刚进入圆环时，<code>fast</code>的位置，由于<code>fast</code>的速度是<code>slow</code>的2倍，且<code>slow</code>从0点出发，<code>fast</code>跑完一圈，<code>slow</code>刚好在半圆位置。此时，二者距离为$\frac{C}{2} - k$，假设再经过 $t_2$ 时间，<code>fast</code>与<code>slow</code>相遇，考虑一下：</p>
<p><code>slow</code>经过的距离：$t_2$ (<code>slow</code>每次移动一格)</p>
<p><code>fast</code>经过的距离：$2t_2$</p>
<p>可知有如下关系：</p>
<script type="math/tex; mode=display">
t_2 + \frac{C}{2}-k = 2t_2</script><script type="math/tex; mode=display">
t_2 = \frac{C}{2} - k</script><p>此时，<code>slow</code>指针的位置为：</p>
<script type="math/tex; mode=display">
\frac{C}{2} + \frac{C}{2} - k</script><p>与起点距离为：</p>
<script type="math/tex; mode=display">
C - C + k = k</script><p>而数组第一个元素与圆环入口元素距离也为$k$。</p>
<p>因此，另一个指针从数组第一个元素开始，slow从与fast相遇位置开始一起每次移动一格，最终会在入口元素相遇。</p>
</li>
<li><p>当$k &gt; \frac{C}{2}$时，fast当前位置在环的下半部分。fast在slow后面，距离为$C - k$</p>
<p>这种情况，fast指针不必多跑一圈才能追上slow指针，假设经过$t_3$时间，fast追上了slow。</p>
<p>期间：</p>
<p>fast移动的距离：$2t_3$</p>
<p>slow移动的距离：$t_3$</p>
<p>有如下关系：</p>
<script type="math/tex; mode=display">
2t_3 = t_3 + C - k</script><script type="math/tex; mode=display">
t_3 = C - k</script><p>此时slow的位置在$C-k$初，距离达圆环入口处（顺时针移动），有</p>
<script type="math/tex; mode=display">
C - (C-k) = k</script><p>同样是k个距离。</p>
<p>得证。</p>
</li>
</ol>
<p><img src="/images/快慢指针示意图.jpg" alt=""></p>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
  </entry>
  <entry>
    <title>EM算法</title>
    <url>/2020/03/19/EM%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h2><p>唉，em算法是我看过最恶心的算法。是我概率论知识太差了？…</p>
<p>参考资料：$Andrew Ng$ $cs229-notes8$ 、统计学习方法（第二版）</p>
<h3 id="1-极大似然估计"><a href="#1-极大似然估计" class="headerlink" title="1.极大似然估计"></a>1.极大似然估计</h3><ul>
<li><p>似然（$likelihood$）与概率（$probability$）</p>
<p>​        概率，用于在已知一些参数的情况下，预测接下来在观测上所得到的结果；似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值，即<strong>估计参数的可能性</strong>。</p>
<p>参见：<a href="https://zh.wikipedia.org/wiki/似然函数" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0</a></p>
</li>
</ul>
<p>​        <strong>其实极大似然估计就是根据样本来估计统计模型的参数，选取一个参数使得当前观测的概率最大</strong>。<strong>似然函数取得最大值表示相应的参数能够使得统计模型最为合理。</strong></p>
<p>推荐宋浩老师的这节课。<a href="https://www.bilibili.com/video/av36206436?t=3565&amp;p=67" target="_blank" rel="noopener">https://www.bilibili.com/video/av36206436?t=3565&amp;p=67</a></p>
<p>主要有以下步骤：</p>
<ol>
<li>写出总体的概率函数或概率密度函数</li>
<li>写出似然函数（通常是概率连乘的形式）。在数理统计学中，似然函数是一种关于统计模型中的参数的函数。</li>
<li>两边取对数，得到<strong>对数似然函数</strong></li>
<li>求对数似然函数关于参数的导数或偏导，并求出使得导数或偏导为0的参数。该参数即为所求</li>
</ol>
<p><strong>注意：</strong></p>
<ul>
<li>Q：为什么是连乘的形式？A：所有样本之间的概率是相互独立的。</li>
<li>通常某个函数的极值点，导数为0或不存在。</li>
</ul>
<p><strong>例子：</strong></p>
<p>已知存在一批可观测样本$\{x_1,x_2,…,x_n\}$，随机变量 $X$ 满足正态分布 $N(\mu,\sigma^2)$，利用极大似然估计，求出正态分布的相关参数。</p>
<p><strong>解：</strong></p>
<ol>
<li><p>先写出正态分布的概率密度函数：</p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp^{-\frac{(x - \mu)^2}{2\sigma^2}}</script></li>
<li><p>写出似然函数</p>
<script type="math/tex; mode=display">
L(\mu,\sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i - \mu)^2}{2\sigma^2}} = (\frac{1}{\sqrt{2\pi}})^n(\sigma^2)^{-\frac{1}{2}n}e^{-\frac{\sum_{i}^{n}(x_i-\mu)^2 }{2\sigma^2}}</script></li>
<li><p>两边取对数，得到对数似然函数</p>
<script type="math/tex; mode=display">
\ln L(\mu,\sigma^2) = n\ln(\frac{1}{\sqrt{2\pi}})-\frac{1}{2}n\ln(\sigma^2)-\frac{\sum_{i=1}^n (x_i-\mu)^2}{2\sigma^2}</script></li>
<li><p>求对数似然函数关于$\mu$ ，$\sigma^2$ 的偏导</p>
<script type="math/tex; mode=display">
\frac{\partial \ln(L(\mu,\sigma^2))}{\partial \mu} = \frac{\sum_{i=1}^n(x_i-\mu)}{\sigma^2} = 0</script><script type="math/tex; mode=display">
\sum_{i=1}^n(x_i-\mu) = 0 \rightarrow x_1+x_2+...+x_n-n\mu = 0</script></li>
</ol>
<script type="math/tex; mode=display">
\mu = \frac{x_1+x_2+x_3+...+x_n}{n}</script><script type="math/tex; mode=display">
\frac{\partial \ln(L(\mu,\sigma^2))}{\partial \sigma^2} =-\frac{n}{2\sigma^2}+\frac{\sum_{i=1}^n (x_i-\mu)^2}{2\sigma^4} = 0</script><p>化简，可得：</p>
<script type="math/tex; mode=display">
\sigma^2 = \frac{(x_1-\mu)^2 + (x_2-\mu)^2 +...+(x_n-\mu)^2}{n}</script><p>观察上式可知:</p>
<ul>
<li>$\mu$ 即为样本<strong>均值</strong></li>
<li>$\sigma^2$ 为样本的<strong>方差</strong></li>
</ul>
<h3 id="2-Jensen不等式"><a href="#2-Jensen不等式" class="headerlink" title="2. Jensen不等式"></a>2. Jensen不等式</h3><h3 id="3-E-M算法的导出"><a href="#3-E-M算法的导出" class="headerlink" title="3. E-M算法的导出"></a>3. E-M算法的导出</h3><script type="math/tex; mode=display">
L(\theta) = \prod_i^n P(x_i|\theta)</script><script type="math/tex; mode=display">
L(\theta) =  \sum_i^n \log P(x_i|\theta) =\\ \sum_i^n\log\sum_Z  P(x_i,z|\theta) = \\
\sum_i^n\log\sum_ZQ(z)\frac{p(x_i,z|\theta)}{Q(z)}</script><p>注意：$z$为隐变量（<strong>latent variables</strong>）$Q(z)$为$z$的一个分布，是啥分布不确定。</p>
<p>求这个似然函数的导数比较麻烦和困难，因此提出了EM算法，通过迭代的方式逐步求解。</p>
<p>继续推导，有Jensen不等式有：</p>
<script type="math/tex; mode=display">
\sum_i^n\log\sum_ZQ(z)\frac{p(x_i,z|\theta)}{Q(z)} >= \sum_i^n\sum_ZQ(z)\log\frac{p(x_i,z|\theta)}{Q(z)}</script><p>当且仅当$\frac{p(x_i,z|\theta)}{Q(z)}$ 为常数时取等。即：</p>
<script type="math/tex; mode=display">
\frac{p(x_i,z|\theta)}{Q(z)} = c \\
\sum_Z Q(z) = 1 \\
Q(z) ∝ p(x_i,z;\theta)
Q(z) = p(z|x_i;\theta)</script><p>可以看出，Q是给定观测数据、参数的条件下，隐变量的一个后验分布(条件分布)。</p>
<p>带回到上个式子：</p>
<script type="math/tex; mode=display">
\theta^{j+1} = \arg\max_\theta \sum_i^n\sum_ZQ(z)\log\frac{p(x_i,z|\theta^j)}{Q(z)} = \\

\arg\max_\theta \sum_i^n\sum_Z p(z|x_i;\theta^j)\log p(x_i,z;\theta^j)</script><p>(与z无关的省略掉，给定参数$\theta^j$ 、观测数据，计算出$p(z|x_i;\theta^j)$ 再带进去。然后就只需要最大化$p(x,z;\theta)$ 得到$\theta^{j+1}$)。上述就是M步；我们来看看要极大化的那个式子</p>
<script type="math/tex; mode=display">
Q(z)\log p(x_i,z;\theta^j) = E_{Q}[\log p(x_i,z;\theta^j)] = \\
E_{p(z|x_i,\theta^j)}[\log p(x_i,z;\theta^j)] = \\
E_Z[\log p(x_i,z;\theta)|x_i;\theta^j]</script><p>我们将最后一个式子称为<strong>Q函数</strong>。注意，这个Q不同于上面那个Q分布。</p>
<p>​        <strong>Q函数是完全数据（观测数据和隐变量）的对数似然函数关于隐变量在给定观测数据和参数的情况下的条件分布的期望。E步的求期望，求的就是这个期望。</strong></p>
<p>​        念起来真的很抽象，结合例子做的话就好多了。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch 使用GPU加速</title>
    <url>/2020/03/07/PyTorch%20%E4%BD%BF%E7%94%A8GPU%E5%8A%A0%E9%80%9F/</url>
    <content><![CDATA[<h2 id="PyTorch-使用GPU加速"><a href="#PyTorch-使用GPU加速" class="headerlink" title="PyTorch 使用GPU加速"></a>PyTorch 使用GPU加速</h2><p><strong>注意</strong>：</p>
<ol>
<li><p>如果以前安装过CPU版本的PyTorch，务必先卸载，使用<code>pip</code>或<code>conda</code>命令进行卸载。</p>
</li>
<li><p>务必注意PyTorch和CUDA版本之间的对应，比如PyTorch 1.4 请安装CUDA 10.1。</p>
</li>
<li><p>获取CUDA和CuDNN和安装PyTorch GPU版本，以下操作大概率可行。</p>
<p><a href="https://blog.csdn.net/Mind_programmonkey/article/details/99688839#commentBox" target="_blank" rel="noopener">https://blog.csdn.net/Mind_programmonkey/article/details/99688839#commentBox</a></p>
</li>
<li><p>检查是否正确安装CUDA，在cmd输入：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>
<p>检查是否可用GPU加速，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">print(torch.cuda.is_avaliable())</span><br><span class="line"><span class="comment">#输出True 表示成功。</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>按照以上操作还不行怎么办？</p>
<ul>
<li>尝试去NVIDIA官网更新自己的显卡驱动</li>
<li>重启一下电脑？</li>
<li>实在不行，在下也没有办法了</li>
</ul>
</li>
</ol>
<p><strong>使用CUDA加速训练模型</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个卷积网络对FashionMNIST数据集进行分类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">6</span>, out_channels=<span class="number">12</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        </span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">12</span> * <span class="number">4</span> * <span class="number">4</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">60</span>)</span><br><span class="line">        self.out = nn.Linear(in_features=<span class="number">60</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, t)</span>:</span></span><br><span class="line">        <span class="comment"># (1) input layer</span></span><br><span class="line">        t = t</span><br><span class="line">        <span class="comment"># (2) hidden conv layer</span></span><br><span class="line">        t = self.conv1(t)</span><br><span class="line">        t = F.relu(t)</span><br><span class="line">        t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (3) hidden conv layer</span></span><br><span class="line">        t = self.conv2(t)</span><br><span class="line">        t = F.relu(t)</span><br><span class="line">        t = F.max_pool2d(t, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (4) hidden linear layer</span></span><br><span class="line">        t = t.reshape(<span class="number">-1</span>, <span class="number">12</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">        t = self.fc1(t)</span><br><span class="line">        t = F.relu(t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (5) hidden linear layer</span></span><br><span class="line">        t = self.fc2(t)</span><br><span class="line">        t = F.relu(t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (6) output layer</span></span><br><span class="line">        t = self.out(t)</span><br><span class="line">        <span class="comment">#t = F.softmax(t, dim=1)</span></span><br><span class="line">        <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练数据集的加载</span></span><br><span class="line">train_set = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">'./data'</span></span><br><span class="line">    ,train=<span class="literal">True</span></span><br><span class="line">    ,download=<span class="literal">True</span></span><br><span class="line">    ,transform=transforms.Compose([</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ])</span><br><span class="line">)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">100</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#指定gpu进行训练</span></span><br><span class="line">    device = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">    network = Network().to(device)</span><br><span class="line">    optimizer = optim.Adam(network.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    start = time.clock()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line"></span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        total_correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader: <span class="comment"># Get Batch</span></span><br><span class="line">            images, labels = batch </span><br><span class="line">             <span class="comment">#moving the inputs to gpu type</span></span><br><span class="line">            images,labels = images.to(device), labels.to(device)</span><br><span class="line">            network.eval()</span><br><span class="line">            preds = network(images) <span class="comment"># Pass Batch</span></span><br><span class="line">            loss = F.cross_entropy(preds, labels) <span class="comment"># Calculate Loss</span></span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward() <span class="comment"># Calculate Gradients</span></span><br><span class="line">            optimizer.step() <span class="comment"># Update Weights</span></span><br><span class="line"></span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">            total_correct += get_num_correct(preds, labels)</span><br><span class="line"></span><br><span class="line">        print(</span><br><span class="line">            <span class="string">"epoch"</span>, epoch, </span><br><span class="line">            <span class="string">"total_correct:"</span>, total_correct, </span><br><span class="line">            <span class="string">"loss:"</span>, total_loss</span><br><span class="line">        )</span><br><span class="line">elapsed = (time.clock() - start)</span><br><span class="line">print(elapsed)</span><br></pre></td></tr></table></figure>
<p>可以发现，使用CUDA比使用CPU训练快多了。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>实现梯度下降（线性回归为例）</title>
    <url>/2020/03/01/%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%BA%E4%BE%8B%EF%BC%89/</url>
    <content><![CDATA[<h2 id="梯度下降（线性回归为例）"><a href="#梯度下降（线性回归为例）" class="headerlink" title="梯度下降（线性回归为例）"></a>梯度下降（线性回归为例）</h2><h3 id="0-相关工作"><a href="#0-相关工作" class="headerlink" title="0. 相关工作"></a>0. 相关工作</h3><h4 id="0-1-线性回归模型"><a href="#0-1-线性回归模型" class="headerlink" title="0.1 线性回归模型"></a>0.1 线性回归模型</h4><script type="math/tex; mode=display">
\hat {y} = \theta_0 + \theta_1x_1 + \theta_2x_2 +···+\theta_nx_n</script><p>其中，$n$ 为 $x$ 的特征数量。</p>
<p>转为矩阵相乘形式：</p>
<script type="math/tex; mode=display">
y = X·\theta</script><p>其中，$X$ 为样本矩阵，个人喜欢行数代表样本数量 $m$，列数代表特征维度 $n$。$\theta$ 为参数矩阵，大小为 $n * 1$。</p>
<p>若行数代表特征维度 $n$，列数代表样本数量 $m$，可写为：</p>
<script type="math/tex; mode=display">
y = \theta^{T}·X</script><h4 id="0-2-定义损失函数"><a href="#0-2-定义损失函数" class="headerlink" title="0.2 定义损失函数"></a>0.2 定义损失函数</h4><p>采用均方误差损失函数$Mean Square Error(MSE) $ 。</p>
<p>某个样本的损失函数定义如下：</p>
<script type="math/tex; mode=display">
loss_j = (X_{j}·\theta - y)^2</script><p>整个训练集的损失函数为：</p>
<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (X_i·\theta - y_i)^2</script><p>我们要求的就是使得$J(\theta)$最小的$\theta$。</p>
<script type="math/tex; mode=display">
\hat{\theta} = arg\min_{\theta} J(\theta)</script><h4 id="0-3-定义梯度公式"><a href="#0-3-定义梯度公式" class="headerlink" title="0.3 定义梯度公式"></a>0.3 定义梯度公式</h4><p>梯度就是由多维变量偏导数的向量。</p>
<p>可知$J(\theta)$ 是一个复合函数，求导时采用链式法则，$J$ 对每个维度的参数的偏导，定义为：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m}(X^{i}·\theta - y^i)X_{j}^{i}</script><p>其中$X_j^i$是第$j$个属性之前的系数。表示第i个样本第j个维度的特征值。</p>
<p>矩阵相乘形式：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \theta_j} = \frac{1}{m} X_j^T(X·\theta - y)</script><p>$X_j^T$表示样本矩阵第j个维度的所有特征值。</p>
<p>由此进一步推导出：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \theta} = \frac{1}{m} X^T(X·\theta - y)</script><h3 id="1-批量梯度下降"><a href="#1-批量梯度下降" class="headerlink" title="1. 批量梯度下降"></a>1. 批量梯度下降</h3><script type="math/tex; mode=display">
\theta_{t+1} = \theta_t - \alpha\nabla\theta</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(W,x,y)</span>:</span></span><br><span class="line">    m = len(y)</span><br><span class="line">    predictions = np.dot(x,W)</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span>/<span class="number">2</span>*m)*np.sum(np.square(predictions - y))</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义批量梯度公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(W,x,y)</span>:</span></span><br><span class="line">    m = len(y)</span><br><span class="line">    predictions = np.dot(x,W)</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span>/m)*x.T.dot(predictions-y)</span><br><span class="line"><span class="comment">#迭代训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(W,x,y,alpha=<span class="number">0.001</span>,iterations=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    cost_history = np.zeros(iterations)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iterations):</span><br><span class="line">        cost_history[i] = loss(W,x,y)</span><br><span class="line">        grd = gradient(W,x,y)</span><br><span class="line">        print(grd.shape)<span class="comment">#(x.shape[1],1)</span></span><br><span class="line">        W = W - alpha*grd</span><br><span class="line">    <span class="keyword">return</span> W,cost_history</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment">#生成数据</span></span><br><span class="line">    X = <span class="number">2</span> * np.random.rand(<span class="number">100</span>,<span class="number">1</span>)</span><br><span class="line">    y = <span class="number">10</span> +<span class="number">3</span> * X + np.random.randn(<span class="number">100</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    W = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#偏置项</span></span><br><span class="line">    X_b = np.ones((X.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#加入偏置项，偏执项为(100,1)的全为1的向量</span></span><br><span class="line">    X_ = np.hstack((X_b,X))<span class="comment">#(100,2)</span></span><br><span class="line">    iteration = <span class="number">10000</span></span><br><span class="line">    W,loss = gradient_descent(W,X_,y,alpha=<span class="number">0.01</span>,iterations=iteration)</span><br><span class="line"></span><br><span class="line">    print(W)</span><br><span class="line">    <span class="comment">#由于将偏置项放在X的第一列，也就是第0维。因此，bias = 8.67 weight = 3.85</span></span><br><span class="line">    <span class="comment">#array([[8.67047187],</span></span><br><span class="line">    <span class="comment">#      [3.85509216]])</span></span><br><span class="line"></span><br><span class="line">    y_predict = np.dot(X_,W)</span><br><span class="line">    it = np.linspace(<span class="number">1</span>,iteration,iteration)</span><br><span class="line">    plt.plot(it,loss)</span><br><span class="line"></span><br><span class="line">    plt.scatter(X,y)</span><br><span class="line">    plt.plot(X,y_predict)</span><br></pre></td></tr></table></figure>
<p><img src="C:\Users\ASUS\Pictures\loss.png" alt=""></p>
<p><img src="C:\Users\ASUS\Pictures\linear_reg.png" alt=""></p>
<h3 id="2-随机梯度下降"><a href="#2-随机梯度下降" class="headerlink" title="2. 随机梯度下降"></a>2. 随机梯度下降</h3><p>随机梯度下降是指，在进行梯度更新时，随机选取某一个样本来更新梯度，而非对整个样本数据集求梯度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient_descent</span><span class="params">(W,x,y,alpha,iterations = <span class="number">1000</span>)</span>:</span></span><br><span class="line">    m = len(y)</span><br><span class="line">    cost_history = np.zeros(iterations)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iterations):</span><br><span class="line">        predictions = np.dot(x,W)</span><br><span class="line">        cost_history[i] = (<span class="number">1</span>/<span class="number">2</span>*m)*np.sum(np.square(predictions - y))</span><br><span class="line">        </span><br><span class="line">        rand_index = np.random.randint(<span class="number">0</span>,m)</span><br><span class="line">        <span class="comment">#从数据集中取出索引为rand_index的数据</span></span><br><span class="line">        x_i = x[rand_index,:].reshape(<span class="number">1</span>,x.shape[<span class="number">1</span>])</span><br><span class="line">        y_i = y[rand_index].reshape(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        grad = gradient(W,x_i,y_i)</span><br><span class="line">        </span><br><span class="line">        W = W - alpha*grad</span><br><span class="line">    <span class="keyword">return</span> W,cost_history</span><br></pre></td></tr></table></figure>
<h3 id="3-小批量梯度下降-mini-batch"><a href="#3-小批量梯度下降-mini-batch" class="headerlink" title="3. 小批量梯度下降(mini-batch)"></a>3. 小批量梯度下降(mini-batch)</h3><p>小批量梯度下降指：每一步的梯度计算，既不是基于整个训练集（如批量梯度下降）也不是基于单个实例（如随机梯度下降），而是基于一小部分随机的实例集也就是小批量，来进行梯度的更新。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mini_batch_gradient_descent</span><span class="params">(W,x,y,alpha=<span class="number">0.01</span>,itera=<span class="number">1000</span>,batch_size=<span class="number">10</span>)</span>:</span></span><br><span class="line">	m = len(y)</span><br><span class="line">    cost_history = []</span><br><span class="line">    batches = int(m / batch_size)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(itera):</span><br><span class="line">       	cost = <span class="number">0.0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,m,batch_size):</span><br><span class="line">            X_i = x[i:i+batch_size]<span class="comment">#size:[batch_size,m]</span></span><br><span class="line">            y_i = y[i:i+batch_size]<span class="comment">#size:[batch_size,1]</span></span><br><span class="line">        	</span><br><span class="line">            predictions = np.dot(X_i,W)</span><br><span class="line">            <span class="comment">#计算整个数据集上的loss</span></span><br><span class="line">            cost += (<span class="number">1</span>/<span class="number">2</span>*m)*np.sum(np.square(predictions - y_i))</span><br><span class="line">            grad = gradient(W,X_i,y_i)</span><br><span class="line">            </span><br><span class="line">            W = W - alpha*grad</span><br><span class="line">        cost_history.append(cost)</span><br><span class="line">    <span class="keyword">return</span> W,cost_history</span><br></pre></td></tr></table></figure>
<h3 id="4-动量梯度下降"><a href="#4-动量梯度下降" class="headerlink" title="4. 动量梯度下降"></a>4. 动量梯度下降</h3><script type="math/tex; mode=display">
v_t = \gamma v_{t-1} + \alpha \nabla J(\theta)</script><script type="math/tex; mode=display">
\theta = \theta - v_t</script><p>$\alpha$ 即学习率。$\gamma$ 是动量系数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">momentum_gradient_descent</span><span class="params">(W,x,y,alpha=<span class="number">0.01</span>,mini_batch=<span class="number">20</span>,itera=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    volocity = np.zeros(W.shape)</span><br><span class="line">    gamma = <span class="number">0.9</span></span><br><span class="line">    m = len(y)</span><br><span class="line">    cost_history = []</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(itera):</span><br><span class="line">        cost = <span class="number">0.0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,m,batch_size):</span><br><span class="line">            X_i = x[i:i+batch_size]<span class="comment">#size:[batch_size,m]</span></span><br><span class="line">            y_i = y[i:i+batch_size]<span class="comment">#size:[batch_size,1]</span></span><br><span class="line"></span><br><span class="line">            predictions = np.dot(X_i,W)</span><br><span class="line">            <span class="comment">#计算整个数据集上的loss</span></span><br><span class="line">            cost += (<span class="number">1</span>/<span class="number">2</span>*m)*np.sum(np.square(predictions - y_i))</span><br><span class="line">            grad = gradient(W,X_i,y_i)</span><br><span class="line">            <span class="comment">#volocity的维度和W相同，列向量 直接矩阵计算</span></span><br><span class="line">            <span class="comment">#相当于在每个维度上计算gamma*volocity[dim] + alpha*grad[dim]</span></span><br><span class="line">            volocity = gamma * volocity + alpha*grad </span><br><span class="line">            W = W - volocity</span><br><span class="line">        cost_history.append(cost)</span><br><span class="line">    <span class="keyword">return</span> W,cost_history</span><br></pre></td></tr></table></figure>
<h3 id="5-AdaGrad"><a href="#5-AdaGrad" class="headerlink" title="5. AdaGrad"></a>5. AdaGrad</h3><p>算法简介：</p>
<p><img src="/images/adagrad.png" alt=""></p>
<p><strong>在参数空间更为平缓的方向，该算法会取得更大的进步（因为平缓，所以历史梯度平方和较小，作为分母。对应学习下降的幅度较大），并且能够使得陡峭的方向变得平缓，从而加快训练速度。</strong>同时，每次迭代时，学习率也在不断改变，全局学习率逐参数的，除以历史梯度平方和的平方根，使得每个参数的学习率不同。</p>
<p><img src="/images/v2-1d979af221d94aea41972e62a8935a95_r.jpg" alt=""></p>
<h3 id="6-RMSprop"><a href="#6-RMSprop" class="headerlink" title="6. RMSprop"></a>6. RMSprop</h3><p>算法简介：</p>
<p><img src="/images/RMSprop.png" alt=""></p>
<p>相比于之前的<strong>AdaGrad</strong>，采用了指数加权平均来更新每个参数的历史梯度(近期的梯度比重较大，历史梯度指数减小)，增加了一个衰减系数来控制历史信息的获取多少。</p>
<p><img src="/images/momprop2-2.png" alt=""></p>
<h3 id="7-Adam"><a href="#7-Adam" class="headerlink" title="7. Adam"></a>7. Adam</h3><p><img src="/images/adam.png" alt=""></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>GitHub的骚操作</title>
    <url>/2020/02/27/GitHub%E7%9A%84%E9%AA%9A%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="GitHub的骚操作"><a href="#GitHub的骚操作" class="headerlink" title="GitHub的骚操作"></a>GitHub的骚操作</h2><h3 id="1-in-限制搜索"><a href="#1-in-限制搜索" class="headerlink" title="1. in 限制搜索"></a>1. in 限制搜索</h3><p><strong>用法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gnn <span class="keyword">in</span>:name</span><br><span class="line">spring <span class="keyword">in</span>:readme,name,description</span><br></pre></td></tr></table></figure>
<h3 id="2-starts-forks范围搜索"><a href="#2-starts-forks范围搜索" class="headerlink" title="2. starts/forks范围搜索"></a>2. starts/forks范围搜索</h3><p><strong>用法</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">name stars/forks:&gt;500</span><br><span class="line">name stars/forks:500..600</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">组合命令</span></span><br><span class="line">redis starts:&gt;5000 forks:6000..10000 in:name</span><br></pre></td></tr></table></figure>
<h3 id="3-awesome搜索"><a href="#3-awesome搜索" class="headerlink" title="3. awesome搜索"></a>3. awesome搜索</h3><p><strong>用法</strong>：展示站点上较好的项目</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awesome gan</span><br></pre></td></tr></table></figure>
<h3 id="4-高亮显示代码"><a href="#4-高亮显示代码" class="headerlink" title="4.高亮显示代码"></a>4.高亮显示代码</h3><p><strong>用法</strong>：在代码链接后加<code>#Lnum</code>，将高亮该行代码；<code>#L5-L50</code>高亮<code>5-50</code>行的代码。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">https://github.com/JeffLi1993/springboot-learning-example/blob/master/chapter-2-spring-boot-config/src/main/java/demo/springboot/web/HelloBookController.java#L1</span><br><span class="line"></span><br><span class="line">https://github.com/JeffLi1993/springboot-learning-example/blob/master/chapter-2-spring-boot-config/src/main/java/demo/springboot/web/HelloBookController.java#L1-L15</span><br></pre></td></tr></table></figure>
<h3 id="5-t搜索"><a href="#5-t搜索" class="headerlink" title="5. t搜索"></a>5. t搜索</h3><p>在某个项目首页下，<code>t</code>键可以查看该项目所有代码文件。</p>
<p>还有其他快捷键，可参考官方文档。</p>
<p><img src="/images/image-20200227130114291.png" alt="image-20200227130114291"></p>
<p><img src="/images/image-20200227130247176.png" alt="image-20200227130247176"></p>
<h3 id="6-关于git的一些知识"><a href="#6-关于git的一些知识" class="headerlink" title="6. 关于git的一些知识"></a>6. 关于git的一些知识</h3><ul>
<li><p><code>git</code>分为工作区、暂存区、版本库。将工作区的文件<code>add</code>到暂存区，<code>git</code>才可以对改文件进行版本管理，<code>add</code>之后就可以<code>commit</code>到版本库</p>
</li>
<li><p>已经<code>add</code>到暂存区的文件，又对其进行了修改，务必再次执行<code>add</code>操作，否则<code>commit</code>时，这个修改不会被提交到版本库。第一次修改 -&gt; <code>git add</code> -&gt; 第二次修改 -&gt; <code>git add</code> -&gt; <code>git commit</code>，两次修改合并为一次<code>commit</code></p>
</li>
<li><p>撤销修改：</p>
<ul>
<li><p>已经执行<code>add</code>的文件，现在在工作区区又进行了一次修改（这次修改还没<code>add</code>），<code>git checkout -- filename</code>可放弃工作区的这次修改</p>
</li>
<li><p>做了修改，并且已经<code>add</code>到暂存区，现在不想要这次修改了，先</p>
<p><code>git reset HEAD filename</code>，从暂存区中拉回到工作区，然后<code>git checkout -- filename</code>放弃这次修改。</p>
</li>
</ul>
</li>
<li><p>版本回退</p>
<ul>
<li><code>git reset --hard commitId</code>，可回退到之前的版本。可采用<code>git log</code>来查看每次<code>commit</code>的详细情况</li>
<li>要重新返回到最新版本，用<code>git reflog</code>查看命令历史，获取<code>commitId</code>，以便确定要回到未来的哪个版本。然后采用<code>reset</code>命令。</li>
</ul>
</li>
<li><p>分支管理</p>
<p>参考<a href="https://www.liaoxuefeng.com/wiki/896043488029600/900003767775424" target="_blank" rel="noopener">https://www.liaoxuefeng.com/wiki/896043488029600/900003767775424</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>Java 反射和动态代理</title>
    <url>/2020/02/23/Java%20%E5%8F%8D%E5%B0%84%E5%92%8C%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/</url>
    <content><![CDATA[<h2 id="Java-反射和动态代理"><a href="#Java-反射和动态代理" class="headerlink" title="Java 反射和动态代理"></a>Java 反射和动态代理</h2><h3 id="1-类加载机制"><a href="#1-类加载机制" class="headerlink" title="1. 类加载机制"></a>1. 类加载机制</h3><h4 id="1-1-类加载过程"><a href="#1-1-类加载过程" class="headerlink" title="1.1 类加载过程"></a>1.1 类加载过程</h4><p>大体来说，可以分为三个阶段：<strong>加载 $\rightarrow$ 链接 $\rightarrow$ 初始化</strong>。具体过程如图：1</p>
<p><img src="/images/loader.png" alt=""></p>
<p><strong>类加载时机</strong>:</p>
<p>1.创建类的实例，也就是new一个对象</p>
<p>2.访问某个类或接口的静态变量，或者对该静态变量赋值</p>
<p>3.调用类的静态方法</p>
<p>4.反射</p>
<p>5.初始化一个类的子类（会首先初始化子类的父类）</p>
<p>6.虚拟机启动时标明的启动类，即文件名和类名相同的那个类</p>
<h4 id="1-2-加载"><a href="#1-2-加载" class="headerlink" title="1.2 加载"></a>1.2 加载</h4><p>​        将<code>class</code>字节码文件内容加载到内存中，并将这些静态数据转换为方法区的运行时数据结构，然后在堆中生成一个代表这个类的<code>java.lang.Class</code>对象，作为方法区中<strong>类数据（Class Metadata）</strong>的返回入口。</p>
<p><strong><code>Class</code></strong>对象代表啥：</p>
<p>​        每当一个类加载到内存中后，这个类便成为运行时类，虚拟机会在<strong>堆区</strong>创建一个有关这个类的<code>Class</code>对象。</p>
<h4 id="1-1-类加载过程-1"><a href="#1-1-类加载过程-1" class="headerlink" title="1.1 类加载过程"></a>1.1 类加载过程</h4><h4 id="1-3-类加载器"><a href="#1-3-类加载器" class="headerlink" title="1.3 类加载器"></a>1.3 类加载器</h4><ul>
<li><code>Bootstrap  Class loader</code></li>
<li><code>Extension Class loader</code></li>
<li><code>Application Class loader</code></li>
</ul>
<p><strong>双亲委托模型</strong>：</p>
<p>​         如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行，如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将到达顶层的启动类加载器，如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。</p>
<p>​        即每个儿子都很懒，每次有活就丢给父亲去干，直到父亲说这件事我也干不了时，儿子自己才想办法去完成。 </p>
<p><strong>为什么采用这种模式</strong>：</p>
<ul>
<li>避免类的重复加载 </li>
<li>保证安全， Java中定义的核心类不会被随意替换 </li>
</ul>
<p><img src="/images/classloder.jpg" alt=""></p>
<h3 id="2-反射"><a href="#2-反射" class="headerlink" title="2. 反射"></a>2. 反射</h3><p>​        反射到底是干啥的？答：不用new也可以获取到一个对象的实例。可以在运行时构造任意一个类的对象，可以在运行时处理注解、获取泛型信息等。</p>
<p>​        反射相关的<code>api</code>在<code>java.lang.reflect</code>包下。以下是通过反射调用<code>show</code>方法的代码清单。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.nefu.reflect;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.InvocationTargetException;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">Main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InstantiationException, IllegalAccessException, ClassNotFoundException, NoSuchMethodException, SecurityException, IllegalArgumentException, InvocationTargetException </span>&#123;</span><br><span class="line">        <span class="comment">//1. 获取Class对象实例</span></span><br><span class="line">		Class&lt;?&gt; clazz = Class.forName(<span class="string">"com.nefu.reflect.Main"</span>);</span><br><span class="line">        <span class="comment">//2. 创建一个Main类的实例对象</span></span><br><span class="line">		Object obj = clazz.getDeclaredConstructor().newInstance();</span><br><span class="line">		Method show = clazz.getDeclaredMethod(<span class="string">"show"</span>);</span><br><span class="line">        <span class="comment">//3. 避免权限不够</span></span><br><span class="line">		show.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">		System.out.println(show.getReturnType());</span><br><span class="line">        <span class="comment">//4. 调用obj的show方法</span></span><br><span class="line">		show.invoke(obj);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">show</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		System.out.println(<span class="string">"hello world"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-动态代理"><a href="#3-动态代理" class="headerlink" title="3. 动态代理"></a>3. 动态代理</h3><p>​        代理类可以增强被代理类对象方法。</p>
<h4 id="3-1-静态代理"><a href="#3-1-静态代理" class="headerlink" title="3.1 静态代理"></a>3.1 静态代理</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test01</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		NikeClothFactory nikeClothFactory = <span class="keyword">new</span> NikeClothFactory();</span><br><span class="line">		NikeClothFactoryProxy proxy = <span class="keyword">new</span> NikeClothFactoryProxy(nikeClothFactory);</span><br><span class="line">		proxy.invoke();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">ClothFactory</span></span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">void</span> <span class="title">product</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">///被代理类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NikeClothFactory</span> <span class="keyword">implements</span> <span class="title">ClothFactory</span></span>&#123;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">product</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		System.out.println(<span class="string">"Nike 开始生产...."</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//代理类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NikeClothFactoryProxy</span></span>&#123;</span><br><span class="line">	ClothFactory clothFactory;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">NikeClothFactoryProxy</span><span class="params">(ClothFactory clothFactory)</span> </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">this</span>.clothFactory = clothFactory;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		System.out.println(<span class="string">"前置处理"</span>);</span><br><span class="line">		clothFactory.product();</span><br><span class="line">		System.out.println(<span class="string">"后置处理"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​        以上代码可以看出，当再有一个类实现<code>ClothFactory</code>接口，我们得继续编写一个对应的代理类进行增强处理。<strong>静态代理在编译期就确定了代理对象</strong>。</p>
<h4 id="3-2-动态代理"><a href="#3-2-动态代理" class="headerlink" title="3.2 动态代理"></a>3.2 动态代理</h4><p>​        在Java中，动态代理实现有<code>JDK</code>自带的动态代理，<code>CGLib</code>动态代理。 通过动态代理，可以无需声明代理类。是使用反射和字节码的技术，在运行期创建指定接口或类的子类（即动态代理类）以及其实例对象的技术。通过动态代理技术可以无侵入地对代码进行增强。 </p>
<p>​        两种动态代理的最大的区别是：<code>JDK</code>动态代理要求被代理对象必须基于接口来实现。动态代理类和被代理类必须实现同一个接口。动态代理只能对接口中声明的方法进行代理。对那些没有实现接口的bean。<code>JDK</code>动态代理无法代理。而<code>CGLib</code>通过继承被代理类的方式实现代理。</p>
<p>​        在<code>JDK</code>动态代理中，主要调用<code>java.lang.reflect.Proxy</code>类和<code>java.lang.reflect.InvocationHandler</code>接口。依然以静态代理中的<code>ClothFactory</code>为例，编写动态代理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.lang.reflect.InvocationHandler;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Proxy;</span><br><span class="line"></span><br><span class="line"><span class="comment">//被代理类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PumaClothFactory</span> <span class="keyword">implements</span> <span class="title">ClothFactory</span></span>&#123;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">product</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		System.out.println(<span class="string">"Puma 源自南美！"</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//测试类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test02</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		PumaClothFactory obj = <span class="keyword">new</span> PumaClothFactory();</span><br><span class="line">		ClothFactory proxy = (ClothFactory)Proxy</span><br><span class="line">				.newProxyInstance(obj.getClass().getClassLoader(), </span><br><span class="line">						obj.getClass().getInterfaces(),</span><br><span class="line">						<span class="keyword">new</span> PumaInvocationHandler(obj));</span><br><span class="line">		proxy.product();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//每一个动态代理实例都有一个关联的InvocationHandler</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PumaInvocationHandler</span> <span class="keyword">implements</span> <span class="title">InvocationHandler</span></span>&#123;</span><br><span class="line">	ClothFactory obj;</span><br><span class="line">	 <span class="function"><span class="keyword">public</span> <span class="title">PumaInvocationHandler</span><span class="params">(ClothFactory clothFactory)</span> </span>&#123;</span><br><span class="line">		<span class="comment">// TODO Auto-generated constructor stub</span></span><br><span class="line">		 <span class="keyword">this</span>.obj = clothFactory;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> Object <span class="title">invoke</span><span class="params">(Object proxy, Method method, Object[] args)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line">		<span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">		System.out.println(<span class="string">"前置处理"</span>);</span><br><span class="line">		<span class="comment">//真正调用被代理类的方法</span></span><br><span class="line">		method.invoke(obj, args);</span><br><span class="line">		System.out.println(<span class="string">"后置处理"</span>);</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​        可以看出，上述代码中并没有显示的编写代理类，而是调用了<code>Proxy.newProxyInstance</code>方法来动态创建代理类。</p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>链表</title>
    <url>/2020/02/15/%E9%93%BE%E8%A1%A8/</url>
    <content><![CDATA[<h2 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h2><p>Java</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ListNode</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> val;</span><br><span class="line">	ListNode next;</span><br><span class="line">	ListNode(<span class="keyword">int</span> x) &#123;</span><br><span class="line">		val = x;</span><br><span class="line">		next = <span class="keyword">null</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>C++</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> &#123;</span></span><br><span class="line">	<span class="keyword">int</span> val;</span><br><span class="line"> 	ListNode *next;</span><br><span class="line"> 	ListNode() : val(<span class="number">0</span>), next(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line"> 	ListNode(<span class="keyword">int</span> x) : val(x), next(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line"> 	ListNode(<span class="keyword">int</span> x, ListNode *next) : val(x), next(next) &#123;&#125;</span><br><span class="line"> &#125;;</span><br></pre></td></tr></table></figure>
<h3 id="1-反转链表"><a href="#1-反转链表" class="headerlink" title="1. 反转链表"></a>1. 反转链表</h3><p>​         反转一个单链表。 这是<code>Leetcode 206</code>号问题。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">reverseList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(head == <span class="literal">NULL</span> || head-&gt;next == <span class="literal">NULL</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> head;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode *dummy = <span class="keyword">new</span> ListNode(<span class="number">0</span>), *tmp;</span><br><span class="line">        <span class="keyword">while</span>(head != <span class="literal">NULL</span>)&#123;</span><br><span class="line">            tmp = head-&gt;next;</span><br><span class="line">            head-&gt;next = dummy-&gt;next;</span><br><span class="line">            dummy-&gt;next = head;</span><br><span class="line">            head = tmp;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dummy-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><img src="/images/reverse-node.png" alt=""></p>
<h3 id="2-环形链表"><a href="#2-环形链表" class="headerlink" title="2. 环形链表"></a>2. 环形链表</h3><p>​         给定一个链表，判断链表中是否有环。 这是<code>Leetcode 141</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasCycle</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">if</span> (head == <span class="keyword">null</span> || head.next == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        ListNode slow = head, fast = head.next;</span><br><span class="line">        <span class="keyword">while</span> (slow != fast) &#123;</span><br><span class="line">            <span class="keyword">if</span> (fast == <span class="keyword">null</span> || fast.next == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            slow = slow.next;</span><br><span class="line">            fast = fast.next.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-相交链表"><a href="#3-相交链表" class="headerlink" title="3. 相交链表"></a>3. 相交链表</h3><p>​         编写一个程序，找到两个单链表相交的起始节点。 这是<code>Leetcode 160</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ListNode <span class="title">getIntersectionNode</span><span class="params">(ListNode headA, ListNode headB)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">if</span> (headA == <span class="keyword">null</span> || headB == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">int</span> lena = <span class="number">0</span>, lenb = <span class="number">0</span>;</span><br><span class="line">        ListNode heada = headA, headb = headB;</span><br><span class="line">        <span class="keyword">while</span>(heada!=<span class="keyword">null</span>)&#123;</span><br><span class="line">            heada = heada.next;</span><br><span class="line">            lena++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(headb!=<span class="keyword">null</span>)&#123;</span><br><span class="line">            headb = headb.next;</span><br><span class="line">            lenb++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(lena &gt; lenb)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= lena - lenb; i++ )</span><br><span class="line">                headA = headA.next;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= lenb - lena; i++)</span><br><span class="line">                headB = headB.next;</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">while</span>(headA != <span class="keyword">null</span> &amp;&amp; headB != <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(headA == headB)&#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            headA = headA.next;</span><br><span class="line">            headB = headB.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> headA;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-链表的中间节点"><a href="#4-链表的中间节点" class="headerlink" title="4. 链表的中间节点"></a>4. 链表的中间节点</h3><p>快慢指针</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">middleNode</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(head == <span class="literal">NULL</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> head;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode *slow = head, *fast = head;</span><br><span class="line">        <span class="keyword">while</span>(slow != <span class="literal">NULL</span> &amp;&amp; fast != <span class="literal">NULL</span> &amp;&amp; fast-&gt;next != <span class="literal">NULL</span>)&#123;</span><br><span class="line">            slow = slow-&gt;next;</span><br><span class="line">            fast = fast-&gt;next-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> slow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><strong>值得注意的是：当链表长度为奇数时，fast最后指向最后一个节点:1-&gt;3-&gt;5…；当链表长度为偶数，fast最后为空</strong>.</p>
<h3 id="5-链表的倒数第k个节点"><a href="#5-链表的倒数第k个节点" class="headerlink" title="5. 链表的倒数第k个节点"></a>5. 链表的倒数第k个节点</h3><p>快慢指针，先让快指针走k步，之后快慢指针一起走，当快指针为空时，慢指针恰好走到倒数第k个节点。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ListNode <span class="title">getKthFromEnd</span><span class="params">(ListNode head, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        ListNode fast = head;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= k &amp;&amp; fast != <span class="keyword">null</span>; i++)&#123;</span><br><span class="line">            fast = fast.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(head != <span class="keyword">null</span> &amp;&amp; fast != <span class="keyword">null</span>)&#123;</span><br><span class="line">            head = head.next;</span><br><span class="line">            fast = fast.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="6-重排链表"><a href="#6-重排链表" class="headerlink" title="6. 重排链表"></a>6. 重排链表</h3><p>链表中间节点+反转后半个链表+合并两个链表（基本包含了常用的对链表的操作，具有代表性）</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">reorderList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(head == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 长度为1或者2</span></span><br><span class="line">    <span class="keyword">if</span>(head-&gt;next == <span class="literal">NULL</span> || head-&gt;next-&gt;next == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 1.快慢指针找到链表的中间节点</span></span><br><span class="line">    <span class="comment">// 2.反转中间节点以后的链表</span></span><br><span class="line">    <span class="comment">// 3.将反转后的链表插入前半个链表</span></span><br><span class="line">    ListNode *slow = head, *fast = head;</span><br><span class="line">    <span class="keyword">while</span>(slow != <span class="literal">NULL</span> &amp;&amp; fast != <span class="literal">NULL</span> &amp;&amp; fast-&gt;next != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        slow = slow-&gt;next;</span><br><span class="line">        fast = fast-&gt;next-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// slow 指针指向中间节点,反转</span></span><br><span class="line">    ListNode *dummy = <span class="keyword">new</span> ListNode(<span class="number">0</span>), *tmp;</span><br><span class="line">    <span class="comment">// 断开中间节点与后半部分链表的链接，可能会出现环</span></span><br><span class="line">    tmp = slow-&gt;next;</span><br><span class="line">    slow-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    slow = tmp;</span><br><span class="line">    <span class="keyword">while</span>(slow != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        tmp = slow-&gt;next;</span><br><span class="line">        slow-&gt;next = dummy-&gt;next;</span><br><span class="line">        dummy-&gt;next = slow;</span><br><span class="line">        slow = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    dummy = dummy-&gt;next;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 合并两端长度相差不超过1的链表</span></span><br><span class="line">    ListNode *tmp1 = <span class="literal">NULL</span>, *tmp2 = <span class="literal">NULL</span>, *cur = head;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(cur &amp;&amp; dummy)&#123;</span><br><span class="line">        tmp1 = cur-&gt;next;</span><br><span class="line">        tmp2 = dummy-&gt;next;</span><br><span class="line"></span><br><span class="line">        cur-&gt;next = dummy;</span><br><span class="line"></span><br><span class="line">        dummy-&gt;next = tmp1;</span><br><span class="line"></span><br><span class="line">        cur = tmp1;</span><br><span class="line">        dummy = tmp2;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="7-排序链表"><a href="#7-排序链表" class="headerlink" title="7. 排序链表"></a>7. 排序链表</h3><p>Leetcode 148号问题。</p>
<p>归并排序链表版，相比数组，空间复杂度只需要O(1)。思想其实比较简单，关键是分的过程中，注意找到中间节点后断掉后半部分链表以及边界的处理问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> ListNode <span class="title">sortList</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 链表的归并排序</span></span><br><span class="line">        <span class="keyword">if</span> (head == <span class="keyword">null</span> || head.next == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> head;</span><br><span class="line">        &#125;</span><br><span class="line">        ListNode mid = findMidNode(head);</span><br><span class="line">        ListNode rightNode = mid.next;</span><br><span class="line">        mid.next = <span class="keyword">null</span>;</span><br><span class="line">        ListNode left = sortList(head);</span><br><span class="line">        ListNode right = sortList(rightNode);</span><br><span class="line">        <span class="keyword">return</span> merge(left,right);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 找到链表中间节点</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ListNode <span class="title">findMidNode</span><span class="params">(ListNode head)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (head == <span class="keyword">null</span> || head.next == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> head;</span><br><span class="line">        ListNode slow = head, fast = head.next;</span><br><span class="line">        <span class="keyword">while</span> (slow != <span class="keyword">null</span> &amp;&amp; fast != <span class="keyword">null</span> &amp;&amp; fast.next != <span class="keyword">null</span>)&#123;</span><br><span class="line">            slow = slow.next;</span><br><span class="line">            fast = fast.next.next;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> slow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ListNode <span class="title">merge</span><span class="params">(ListNode p, ListNode q)</span></span>&#123;</span><br><span class="line">        <span class="comment">// 合并两个有序链表</span></span><br><span class="line">        ListNode head = <span class="keyword">new</span> ListNode(<span class="number">0</span>), dummy = head;</span><br><span class="line">        <span class="keyword">while</span> (p != <span class="keyword">null</span> &amp;&amp; q != <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span> (p.val &lt; q.val)&#123;</span><br><span class="line">                head.next = p;</span><br><span class="line">                p = p.next;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                head.next = q;</span><br><span class="line">                q = q.next;</span><br><span class="line">            &#125;</span><br><span class="line">            head = head.next;</span><br><span class="line">        &#125;</span><br><span class="line">        head.next = p == <span class="keyword">null</span> ? q:p;</span><br><span class="line">        <span class="keyword">return</span> dummy.next;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
  </entry>
  <entry>
    <title>Java虚拟机内存结构</title>
    <url>/2020/02/13/Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h2 id="Java虚拟机栈内存结构"><a href="#Java虚拟机栈内存结构" class="headerlink" title="Java虚拟机栈内存结构"></a>Java虚拟机栈内存结构</h2><p>​        主要介绍虚拟机栈的内存结构。</p>
<h3 id="1-构成"><a href="#1-构成" class="headerlink" title="1. 构成"></a>1. 构成</h3><p>​        Java虚拟机主要由<strong>堆区、方法区、虚拟机栈、本地方法栈、程序寄存器</strong>五部分组成。其中<strong>堆区、方法区是所有线程共享</strong>的，其余区域都是线程不共享的。</p>
<h3 id="2-虚拟机栈"><a href="#2-虚拟机栈" class="headerlink" title="2. 虚拟机栈"></a>2. 虚拟机栈</h3><p>​         虚拟机为每个新创建的线程都分配一个栈。栈以<strong>帧</strong>为单位保存线程的状态。虚拟机对栈只进行两种操作：以帧为单位的压栈和出栈操作。  </p>
<p>​        一个线程中的方法调用链可能会很长，很多方法都同时处理执行状态。对于执行引擎来讲，活动线程中，只有虚拟机栈顶的栈帧才是有效的，称为当前栈帧 (Current Stack Frame)，这个栈帧所关联的方法称为当前方法(Current Method)。执行引用所运行的所有字节码指令都只针对当前栈帧进行操作。</p>
<p>​        虚拟机栈主要存储<strong>栈帧</strong>（Stack Frame）这种数据结构，在栈帧内部，存储了<strong>局部变量表、操作数栈、动态链接、方法返回地址、其他附加信息</strong>等。  线程每调用一个方法就对应着 Stack 中 Stack Frame 的入栈，方法执行完毕或者异常终止对应着出栈（销毁）。 </p>
<ul>
<li><p>局部变量表：</p>
<p>​        定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量。局部变量表中的变量是<strong>重要的<code>GC Root</code>结点</strong>，只要被局部变量表中变量直接或间接引用的对象都不会被回收。 <strong>在编译程序代码的时候就可以确定栈帧中需要多大的局部变量表，具体大小可在编译后的 Class 文件中看到</strong>。局部变量表的容量以 Variable Slot（变量槽）为最小单位，每个变量槽都可以存储 32 位长度的内存空间。 </p>
<pre><code>     在方法执行时，虚拟机是使用局部变量表完成参数变量列表的传递过程，如果是实例方法，那么局部变量表中的0位索引的Slot默认是用于传递方法所属对象实例的引用，在方法中可以通过关键字“this”来访问这个隐含的参数，其余参数则按照参数列表的顺序来排列，占用从1开始的局部变量Slot，参数表分配 完毕后，再根据方法体内部定义的变量顺序和作用域来分配其余的Slot。
</code></pre><p>​        局部变量表中的Slot是可重用的，方法体中定义的变量，其作用域并不一定会覆盖整个方法，如果当前字节码PC计算器的值已经超出了某个变量的作用域，那么这个变量对应的Slot就可以交给其它变量使用。 </p>
</li>
<li><p>操作数栈</p>
</li>
<li><p>动态链接</p>
<p>​        每个栈帧都包含一个指向运行时常量池中该栈帧所属性方法的引用，持有这个引用是为了支持方法调用过程中的动态连接。在Class文件的常量池中存有大量的符号引用，字节码中的方法调用指令就以常量池中指向方法的符号引用为参数。这些符号引用一部分会在类加载阶段或第一次使用的时候转化为直接引用，这种转化 称为静态解析。另外一部分将在每一次的运行期期间转化为直接引用，这部分称为动态连接。 </p>
</li>
<li><p>方法返回地址</p>
<p>​        当一个方法被执行后，有两种方式退出这个方法。第一种方式是<strong>执行引擎</strong>遇到任意一个方法返回的字节码指令，这时候可能会有返回值传递给上层的方法调用者(调用当前方法的的方法称为调用者)，是否有返回值和返回值的类型将根据遇到何种方法返回指令来决定，这种退出方法方式称为正常完成出口(Normal Method Invocation Completion)。</p>
<p>​        另外一种退出方式是，在方法执行过程中遇到了异常，并且这个异常没有在方法体内得到处理，无论是Java虚拟机内部产生的异常，还是代码中使用 <code>throw</code>字节码指令产生的异常，只要在本方法的异常表中没有搜索到匹配的异常处理器，就会导致方法退出，这种退出方式称为异常完成出口(Abrupt Method Invocation Completion)。一个方法使用异常完成出口的方式退出，是不会给它的调用都产生任何返回值的。</p>
<p>​        无论采用何种方式退出，在方法退出之前，都需要返回到方法被调用的位置，程序才能继续执行，方法返回时可能需要在栈帧中保存一些信息，用来帮助恢复它的上层方法的执行状态。一般来说，方法正常退出时，调用者PC计数器的值就可以作为返回地址，栈帧中很可能会保存这个计数器值。而方法异常退出时，返回地址是要通过异常处理器来确定的，栈帧中一般不会保存这部分信息。</p>
<p>​         <strong>方法退出的过程实际上等同于把当前栈帧出栈</strong>，因此退出时可能执行的操作有：恢复上层方法的局部变量表和操作数栈，把返回值(如果有的话)压入调用都栈帧的操作数栈中，调用PC计数器的值以指向方法调用指令后面的一条指令等。 </p>
</li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>Pytorch 学习笔记之入门</title>
    <url>/2020/02/12/Pytorch%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<h2 id="Pytorch-学习笔记-CPU-only-之入门"><a href="#Pytorch-学习笔记-CPU-only-之入门" class="headerlink" title="Pytorch 学习笔记(CPU only)之入门"></a>Pytorch 学习笔记(CPU only)之入门</h2><p>如果想进一步了解神经网络的内部结构，建议不要使用这些深度学习框架，如果有能力和时间，建议采用<code>numpy</code>类库从底层实现一个神经网络各种操作，比如：前向传播、反向传播、梯度下降等。</p>
<h3 id="1-开始"><a href="#1-开始" class="headerlink" title="1. 开始"></a>1. 开始</h3><h4 id="1-1-构成"><a href="#1-1-构成" class="headerlink" title="1.1 构成"></a>1.1 构成</h4><p>Pytorch 是由 Facebook 于 2016 年推出的一款深度学习框架。Pytorch 主要由以下包构成：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>Packages</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>torch.nn</code></td>
<td>A subpackage that contains modules and extensible classes for building neural networks.<br>(包含用于构建神经网络的模块和可扩展类的子包。)</td>
</tr>
<tr>
<td><code>torch.autograd</code></td>
<td>A subpackage that supports all the differentiable Tensor operations in PyTorch.</td>
</tr>
<tr>
<td><code>torch.nn.functiona</code>l</td>
<td>A functional interface that contains typical operations used for building neural networks like loss functions, activation functions, and convolution operations.<br>(一个功能接口，其中包含用于构建神经网络的典型操作，例如损失函数，激活函数和卷积操作。)</td>
</tr>
<tr>
<td><code>torch.optim</code></td>
<td>A subpackage that contains standard optimization operations like SGD and Adam.<br>(包含基本优化例如:随机梯度下降、Adam 等操作的子包。)</td>
</tr>
<tr>
<td><code>torch.utils</code></td>
<td>A subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier.<br>(包含数据集和数据加载器等实用工具类的子包，使数据预处理更容易。)</td>
</tr>
<tr>
<td><code>torchvision</code></td>
<td>A package that provides access to popular datasets, model architectures, and image transformations for computer vision.<br>(包含可以访问流行的数据集，模型架构和计算机视觉图像转换的包)</td>
</tr>
</tbody>
</table>
</div>
<h4 id="1-2-Tensor"><a href="#1-2-Tensor" class="headerlink" title="1.2 Tensor"></a>1.2 Tensor</h4><p><code>tensor</code>（张量）是<code>Pytorch</code>中基本的数据结构。一维向量、二维矩阵、三维矩阵等都可以视为张量。</p>
<p><strong>tensor 就是一个高维矩阵</strong>。</p>
<p>在<code>Pytorch</code>中封装了<code>torch.Tensor</code>这个类，创建的<code>tensor</code>都是这个类的实例。在<code>torch</code>中，有四种方式创建一个<code>tensor</code>。</p>
<p>1.<code>torch.Tensor(data)</code></p>
<p>2.<code>torch.tensor(data)</code>(<strong>推荐</strong>)</p>
<p>3.<code>torch.as_tensor(data)</code></p>
<p>4.<code>torch.from_numpy(data)</code></p>
<p>传入的<code>data</code>可以是<code>python</code>中的<code>array</code>、<code>list</code>，或者<code>numpy</code>中的<code>ndarray</code>。 可以用 python 的索引和切片来获取和修改一个张量 tensor 中的内容。</p>
<p><strong>tensor 的属性</strong>：</p>
<p><code>dtype</code>：torch 中共有七种 CPU tensor 类型和八种 GPU tensor 类型。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Data type</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td>32-bit floating point</td>
<td><code>torch.FloatTensor</code></td>
<td><code>torch.cuda.FloatTensor</code></td>
</tr>
<tr>
<td>64-bit floating point</td>
<td><code>torch.DoubleTensor</code></td>
<td><code>torch.cuda.DoubleTensor</code></td>
</tr>
<tr>
<td>16-bit floating point</td>
<td>N/A</td>
<td><code>torch.cuda.HalfTensor</code></td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td><code>torch.ByteTensor</code></td>
<td><code>torch.cuda.ByteTensor</code></td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td><code>torch.CharTensor</code></td>
<td><code>torch.cuda.CharTensor</code></td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td><code>torch.ShortTensor</code></td>
<td><code>torch.cuda.ShortTensor</code></td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td><code>torch.IntTensor</code></td>
<td><code>torch.cuda.IntTensor</code></td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td><code>torch.LongTensor</code></td>
<td><code>torch.cuda.LongTensor</code></td>
</tr>
</tbody>
</table>
</div>
<p><code>shape</code>：返回<code>tensor</code>的维度，注：<code>size()</code>方法结果相同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">&gt; type(data)</span><br><span class="line">numpy.ndarray</span><br><span class="line"></span><br><span class="line">&gt; o1 = torch.Tensor(data)</span><br><span class="line">&gt; o2 = torch.tensor(data)</span><br><span class="line">&gt; o3 = torch.as_tensor(data)</span><br><span class="line">&gt; o4 = torch.from_numpy(data)</span><br><span class="line"></span><br><span class="line">&gt; print(o1)</span><br><span class="line">&gt; print(o2)</span><br><span class="line">&gt; print(o3)</span><br><span class="line">&gt; print(o4)</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.int32)</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.int32)</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.int32)</span><br><span class="line"></span><br><span class="line">&gt; data2 = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,(<span class="number">3</span>,<span class="number">3</span>))<span class="comment">#创建一个大小为3*3,初始值为1-10之间的矩阵</span></span><br><span class="line">&gt; o5 = torch.tensor(data2)</span><br><span class="line">&gt; print(o5)</span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">1</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">3</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>]], dtype=torch.int32)</span><br><span class="line">&gt; o5[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">tensor(<span class="number">5</span>, dtype=torch.int32)</span><br><span class="line">&gt; o5.shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># 转为numpy数组</span></span><br><span class="line">&gt; o5.numpy()</span><br><span class="line">array([[<span class="number">5</span>, <span class="number">1</span>, <span class="number">9</span>],</span><br><span class="line">       [<span class="number">7</span>, <span class="number">3</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
<p>创建 tensor 的其他方式：</p>
<ol>
<li><p><code>torch.eye(n)</code>：创建 n 阶的单位矩阵</p>
</li>
<li><p><code>torch.zeros(n)</code>： <code>param</code>可以是一个数字<code>n</code>，表示初始化为 1<em>n 的向量；也可以是一个元组或列表<code>(n,m)</code>，表示`n </em> m`的 0 矩阵。</p>
</li>
<li><p><code>torch.ones(param)</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(torch.ones([2,3]))</span><br><span class="line">#print(torch.ones(2,3)</span><br><span class="line">tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br><span class="line"></span><br><span class="line">print(torch.zeros(3))</span><br><span class="line">tensor([0., 0., 0., 0., 0.])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>troch.ones(param)</code>：同上</p>
</li>
<li><p><code>torch.rand(parma)</code>：初始化为<code>0-1</code>之间的数。</p>
</li>
<li><p><code>torch.randn(param)</code>： 标准正态分布 之中的数。</p>
</li>
</ol>
<h4 id="1-3-tensor-的操作"><a href="#1-3-tensor-的操作" class="headerlink" title="1.3 tensor 的操作"></a>1.3 tensor 的操作</h4><ul>
<li><p><code>numpy()</code>：转为<code>numpy.ndarray</code></p>
</li>
<li><p><code>eq()</code>：用来比较两个<code>tensor</code>各个元素是否相等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt; preds = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">&gt; labels = torch.tensor([<span class="number">1</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">&gt; preds.eq(labels)</span><br><span class="line">tensor([ <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>,  <span class="literal">True</span>])</span><br><span class="line">&gt; preds.eq(labels).sum().item()</span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>reshape()</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt; t = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line">])</span><br><span class="line">&gt; t.size()</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">&gt; t.shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">&gt; t.reshape([<span class="number">1</span>,<span class="number">12</span>])</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]]) <span class="comment">#二维的</span></span><br><span class="line"></span><br><span class="line">&gt; t.reshape(<span class="number">-1</span>)</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>])  <span class="comment">#一维的</span></span><br><span class="line"></span><br><span class="line">&gt; t.reshape(<span class="number">6</span>,<span class="number">2</span>)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#升维</span></span><br><span class="line">&gt; t.reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tensor(</span><br><span class="line">[</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">2.</span>]</span><br><span class="line">    ],</span><br><span class="line"></span><br><span class="line">    [</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]</span><br><span class="line">    ]</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>reshape 返回后的结果可能与原<code>tensor</code>共享数据，而维度不同，即：你改变了 reshape 后的数据，原数据也会被修改；又或者 reshape 后的不共享存储。（无语….）</p>
</li>
<li><p><code>squeeze()</code> 将所有为 1 的维度删掉，可以传入具体的参数，表示将第几个维度进行“挤压”，如果指定维度不为1，那么调用该方法tensor不发生改变。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  &gt; t1 &#x3D; torch.tensor([[1],[2],[3],[4]]) # [4,1]</span><br><span class="line">  &gt; t1_ &#x3D; t1.squeeze(-1)# t1.squeeze(1)</span><br><span class="line">  tensor([1, 2, 3, 4])#[4]</span><br><span class="line">  &gt; t1_.shape</span><br><span class="line">  torch.Size([4])</span><br><span class="line"></span><br><span class="line">  &gt; a &#x3D; torch.randn(1,1,3)</span><br><span class="line">  &gt; print(a)</span><br><span class="line">  tensor([[[ 1.8239, -1.0579,  0.3052]]])</span><br><span class="line"></span><br><span class="line">  &gt; b &#x3D; torch.squeeze(a) # 将a中所有为1的维度删掉。不为1的维度没有影响。</span><br><span class="line">  &gt; b</span><br><span class="line">  tensor([ 1.8239, -1.0579,  0.3052])</span><br><span class="line">#[3]</span><br><span class="line">  &gt; c&#x3D;torch.squeeze(a,1) # 第2维度是否为1，为1就删掉，成为2维矩阵；不为1则不受影响</span><br><span class="line">  &gt; c</span><br><span class="line">  tensor([[ 0.6552,  0.9220, -0.9763]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>unsqueeze()</code> 对给指定位置加上维数为一的维度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt; a = torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">&gt; a,shape</span><br><span class="line">torch.Size([<span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">&gt; b = a.unsqueeze(<span class="number">0</span>)</span><br><span class="line">&gt; b.shape</span><br><span class="line">torch.Size([<span class="number">1</span>,<span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">&gt; c = a.unsqueeze(<span class="number">1</span>)</span><br><span class="line">&gt; c.shape</span><br><span class="line">torch.Size([<span class="number">100</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>flatten</code>就是<code>reshape</code>和<code>squeezee</code>二者结合。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def flatten(t):</span><br><span class="line">    t &#x3D; t.reshape(1, -1)</span><br><span class="line">    t &#x3D; t.squeeze()</span><br><span class="line">    return t</span><br><span class="line">&gt; t &#x3D; torch.ones(4,3)</span><br><span class="line">&gt; flatten(t) # t.reshape(1,-1).squeezee()</span><br><span class="line">tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])</span><br><span class="line">&gt;  t.flatten()</span><br><span class="line">tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>cat()</code>两个矩阵的拼接，按照行/列进行拼接。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t1 = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]]) <span class="comment"># 2*4</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t2 = torch.tensor([[<span class="number">4</span>],[<span class="number">8</span>]]) <span class="comment"># 2*1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.concat((t1,t2),dim = <span class="number">1</span>) <span class="comment">#t2列数与t1不同，只能横着拼接，依旧是行方向</span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">8</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t3 = torch.tensor([[<span class="number">9</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]]) <span class="comment"># [9,2,3,4]会报错！维度不一样 [4] [[9,2,3,4]] =&gt; [1,4]</span></span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">        [<span class="number">9</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p><code>stack()</code> 要求输入的两个张量维度<strong>完全相同</strong>，结果比原张量维度<code>+1</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t1 = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t2 = torch.tensor([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.stack((t1,t2))</span><br><span class="line">tensor([[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">         [<span class="number">3</span>, <span class="number">4</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">         [<span class="number">7</span>, <span class="number">8</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res = torch.stack((t1,t2),<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res</span><br><span class="line">tensor([[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">         [<span class="number">5</span>, <span class="number">6</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">         [<span class="number">7</span>, <span class="number">8</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p><strong>tips</strong>：在图像数据集中，<code>data</code>的维度通常是四维的。每个维度分别代表：<code>[batch,channel,width,height]</code>*</p>
</li>
<li><p><code>view()</code>：用来改变<code>tensor</code>的维度。类似于<code>reshape()</code>。 如果是<code>torch.view(-1)</code>，则原张量会变成一维的结构（和<code>flatten</code>操作相同） 。</p>
<p><strong>注意：</strong><code>view()</code>操作会与原<code>tensor</code>共享数据！！！</p>
</li>
<li><p><code>permute</code>：交换维度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt; t = torch.randn(<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">&gt; t = t.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)</span><br><span class="line">&gt; t.shape</span><br><span class="line">[<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  </span><br><span class="line">* &#96;type_as&#96;：转换&#96;tensor&#96;类型，&#96;tensor&#96;默认小数为&#96;float&#96;型。</span><br><span class="line"></span><br><span class="line">  &#96;&#96;&#96;pyth</span><br><span class="line">  &gt; t1 &#x3D; np.linspace(-10,10,100)</span><br><span class="line">  &gt; t1_ &#x3D; torch.tensor(t1)</span><br><span class="line">  &gt; t1_.dtype</span><br><span class="line">  torch.float64</span><br><span class="line"></span><br><span class="line">  t1_ &#x3D; t1_.type_as(torch.FloatTensor())</span><br><span class="line">  &gt; t1_.dtype</span><br><span class="line">  torch.float32</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="1-4-tensor-的广播以及-reduction-操作"><a href="#1-4-tensor-的广播以及-reduction-操作" class="headerlink" title="1.4 tensor 的广播以及 reduction 操作"></a>1.4 tensor 的广播以及 reduction 操作</h4><p>和<code>numpy</code>类似，tensor 也可以进行每一列/行求均值(<code>mean</code>)、<code>max</code>、<code>argmax</code>、<code>sum</code>、<code>std</code>等操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt; t = torch.tensor([</span><br><span class="line">    [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">3</span>,<span class="number">0</span>]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line">&gt; t.sum()</span><br><span class="line">tensor(<span class="number">8.</span>)</span><br><span class="line">&gt; t.numel()</span><br><span class="line"><span class="number">9</span></span><br><span class="line"></span><br><span class="line">&gt; t.sum().numel()</span><br><span class="line"><span class="number">1</span></span><br><span class="line">&gt; t.prod()</span><br><span class="line">tensor(<span class="number">0.</span>)</span><br><span class="line">&gt; t.mean() <span class="comment">#求所有数值的均值</span></span><br><span class="line">tensor(<span class="number">.8889</span>)</span><br><span class="line"></span><br><span class="line">&gt; t.mean(axis=<span class="number">1</span>) <span class="comment">#求每一行的均值</span></span><br><span class="line">tensor([<span class="number">0.3333</span>, <span class="number">1.3333</span>, <span class="number">1.0000</span>])</span><br><span class="line"></span><br><span class="line">&gt; t.argmax(axis=<span class="number">0</span>) <span class="comment">#求每一列最大值的索引</span></span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">&gt; t.mean().item() <span class="comment">#将得到的tensor最终转为数值，只包含一个元素的tensor才可以有这个操作</span></span><br><span class="line"><span class="number">8.0</span></span><br><span class="line"></span><br><span class="line">&gt; t.argmax(dim = <span class="number">0</span>).tolist()</span><br><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h4 id="1-5-数据集的加载"><a href="#1-5-数据集的加载" class="headerlink" title="1.5 数据集的加载"></a>1.5 数据集的加载</h4><p>通常数据要经过以下处理：</p>
<ul>
<li>Extract data from a data source.</li>
<li>Transform data into a desirable format.</li>
<li>Load data into a suitable structure.</li>
</ul>
<p>加载自定义数据集通常使用<code>torch.utils.data</code>下的<code>Dataset,DataLoader</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FakeDataSet</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,data)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self,idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line">data = np.random.rand(<span class="number">100</span>,<span class="number">10</span>)</span><br><span class="line">dataset = FakeDataSet(data)</span><br><span class="line">dataloader = DataLoader(dataset,batch_size=<span class="number">10</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">    <span class="comment"># batch是一个元组，第一个元素表示索引，第二个元素表示真正的数据</span></span><br><span class="line">    print(batch[<span class="number">1</span>].shape)</span><br><span class="line"><span class="comment"># Outputs 一共10个batch</span></span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<h4 id="1-6-自动求梯度"><a href="#1-6-自动求梯度" class="headerlink" title="1.6 自动求梯度"></a>1.6 自动求梯度</h4><p>定义了一个损失函数后，通过<code>backward</code>方法就可以自动求变量的梯度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x<span class="number">-2</span>)**<span class="number">2</span> + <span class="number">2</span>*x </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fp</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*(x<span class="number">-2</span>) + <span class="number">2</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">2.0</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = f(x)</span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line">x.grad</span><br><span class="line"><span class="comment"># Outputs</span></span><br><span class="line">tensor([<span class="number">2.</span>])</span><br><span class="line"></span><br><span class="line">fp(x)</span><br><span class="line"><span class="comment"># Outputs</span></span><br><span class="line">tensor([<span class="number">2.</span>], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>
<h4 id="1-7-实现一个线性回归"><a href="#1-7-实现一个线性回归" class="headerlink" title="1.7 实现一个线性回归"></a>1.7 实现一个线性回归</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear = nn.Linear(in_features=<span class="number">1</span>,out_features=<span class="number">1</span>,bias=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x = x</span><br><span class="line">        out = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)</span><br><span class="line">y = <span class="number">3</span>*x + <span class="number">5</span> + torch.rand(x.size())</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br><span class="line"><span class="comment"># 选定loss函数</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="comment"># 定义优化函数 ，绑定模型参数</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    out = model(x)</span><br><span class="line">    ls = loss(out,y)</span><br><span class="line">    <span class="comment"># 清空梯度。否则会累计</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 求梯度</span></span><br><span class="line">    ls.backward()</span><br><span class="line">    <span class="comment"># 优化器反向传播更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"loss:"</span> + str(ls.item()))</span><br><span class="line"><span class="comment">#检验模型</span></span><br><span class="line">preds = model(x)</span><br><span class="line">plt.plot(x.numpy(), y.numpy(), <span class="string">'ro'</span>, label=<span class="string">'Original Data'</span>)</span><br><span class="line">plt.plot(x.numpy(), preds.data.numpy(), label=<span class="string">'Fitting Line'</span>)</span><br><span class="line"></span><br><span class="line">print(model.linear.weight,model.linear.bias)</span><br><span class="line">输出如下:</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[<span class="number">3.0281</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([<span class="number">5.4789</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/linearRegress.png" alt=""></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>朴素贝叶斯（Naive Bayes）</title>
    <url>/2020/02/09/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88Naive%20Bayes%EF%BC%89/</url>
    <content><![CDATA[<h2 id="朴素贝叶斯（Naive-Bayes）"><a href="#朴素贝叶斯（Naive-Bayes）" class="headerlink" title="朴素贝叶斯（Naive Bayes）"></a>朴素贝叶斯（Naive Bayes）</h2><h3 id="1-条件概率和全概率公式（由因到果）"><a href="#1-条件概率和全概率公式（由因到果）" class="headerlink" title="1. 条件概率和全概率公式（由因到果）"></a>1. 条件概率和全概率公式（由因到果）</h3><p>首先给出条件概率公式：</p>
<script type="math/tex; mode=display">
P(A|B) = \frac{P(AB)}{P(B)}</script><p>$P(AB)$是变量$A$和$B$的联合概率分布，$P(B)$是变量 B 的边缘概率分布。</p>
<p><strong>全概率公式</strong>：</p>
<p>若事件$Y_1，Y_2，…$构成一个<strong>完备事件组</strong>且都有正概率，则对任意一个事件$X$，有如下公式成立：</p>
<script type="math/tex; mode=display">
P(X) = \sum_{i}P(X|Y_i)P(Y_i)</script><p>解释：全概率公式的意义在于，当某一事件的概率难以求得时，可转化为在一系列条件下发生概率的和。</p>
<p><strong>乘法定理</strong>：</p>
<script type="math/tex; mode=display">
P(A_1,A_2,A_3,...,A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1,A_2)···P(A_n|A_1,A_2,...A_{n-1})</script><h3 id="2-贝叶斯公式（执果索因）"><a href="#2-贝叶斯公式（执果索因）" class="headerlink" title="2. 贝叶斯公式（执果索因）"></a>2. 贝叶斯公式（执果索因）</h3><p>贝叶斯公式就是当已知结果，问导致这个结果的第<code>i</code>原因的概率是多少。(转化到分类问题上，“结果”就是样本，“第i个原因”即属于哪个类别)。</p>
<p>由条件概率公式，可以写成：</p>
<script type="math/tex; mode=display">
P(Y_i|X) = \frac{P(XY_i)}{P(X)} = \frac{P(X|Y_i)P(Y_i)}{P(X)}</script><p>结合<strong>全概率公式</strong>，有:</p>
<script type="math/tex; mode=display">
P(Y_i|X) = \frac{P(X|Y_i)P(Y_i)}{\sum_{j}P(X|Y_j)P(Y_j)}</script><p>解释：我们把$P(Y_i)$叫做$Y$的<strong>先验概率</strong>，$P(Y_i|X)$称为<strong>后验概率</strong>。</p>
<p>下面介绍一个例子实际计算一下。</p>
<p><img src="/images/bayes.png" alt=""></p>
<h3 id="3-朴素贝叶斯"><a href="#3-朴素贝叶斯" class="headerlink" title="3. 朴素贝叶斯"></a>3. 朴素贝叶斯</h3><p><strong>朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法</strong>。朴素贝叶斯算法假设了给定样本情况下，数据集属性之间是相互<strong>条件独立</strong>这一<strong>朴素假设</strong> ，因此算法的逻辑性十分简单，并且算法较为稳定，当数据呈现不同的特点时，朴素贝叶斯的分类性能不会有太大的差异。换句话说就是朴素贝叶斯算法的健壮性比较好，对于不同类型的数据集不会呈现出太大的差异性。当数据集属性之间的关系相对比较独立时，朴素贝叶斯分类算法会有较好的效果。</p>
<p> 下面给出朴素贝叶斯的推导和数学定义。</p>
<p> 设有样本数据集$D=\{d_1,d_2,···,d_n\}$，每个样本特征属性集$X=\{x_1,x_2,···，x_d\}$，这$n$个样本共存在$m$个分类 $Y=\{y_1,y_2,···,y_m\}$，其中$x_1,x_2,···,x_d$x 相互独立且随机。则先验概率$P(Y)$，后验概率$P(Y|X)$.</p>
<p> 由于各个特征之间相互独立，在给定类别 $y$ 情况下，$P(X|Y)$进一步可以表示为：</p>
<script type="math/tex; mode=display">
P(X|Y =y_j) = \prod_{i = 1}^{d} P(x_i|Y = y_j)</script><p> 实际上是乘法定理和条件独立的应用。</p>
<p> 结合贝叶斯公式，有：</p>
<script type="math/tex; mode=display">
P(y_i|x_1,x_2,···,x_d) = \frac{P(y_i)\prod_{j = 1}^{d}P(x_j|y_i)}{\sum_k^{m}P(y_k)\prod_j^{d}P(x_j|y_k)}</script><p> 在给定样本的情况下，计算样本属于各个类别的概率时，分母都相等。</p>
<p> 从而，只需最大化分子。有：</p>
<script type="math/tex; mode=display">
P(y_i|x_1,x_2,···,x_d) ∝ P(y_i)\prod_{j = 1}^{d}P(x_j|y_i)</script><p>从而</p>
<script type="math/tex; mode=display">
\hat{y} = arg \max_{y_i}P(y_i)\prod_{j = 1}^{d}P(x_j|y_i),y_i \in Y</script><p>，这就是朴素贝叶斯定理。可能结果需要归一化，即各个类别概率加起来和为$1$。</p>
<p>给定样本$X$，类别为  $y_i$ 的概率（m为类别个数）即：</p>
<script type="math/tex; mode=display">
P(y_i|x_1,x_2,...,x_d) = \frac{P(y_i)\prod_{j = 1}^{d}P(x_j|y_i)}{\sum_{i = 1}^{m}[P(y_i)\prod_{j = 1}^{d}P(x_j|y_i)]}</script><p>最大的概率即为预测类别。这就是朴素贝叶斯。</p>
<p><strong>“朴素在哪里？”</strong></p>
<p> 事实上，朴素贝叶斯做了如下假设：</p>
<ul>
<li>一个特征出现的概率，与其他特征独立</li>
<li><p>每个特征同等重要</p>
<p>在真实的数据中，这个假设有可能并不会成立，如果一本书中出现了“机器学习”这个词，那么有很大概率会出现“数据挖掘”“特征工程”等词语，而出现“少林功夫”的概率是很低的。由此看来，词之间的概率并不独立，而且词对于分类的概率影响很大，每个词的重要性也是不同的。</p>
<p>因此，这样做出的假设是很“天真”，很”朴素“的。</p>
</li>
</ul>
<p><img src="/images/朴素贝叶斯.jpg" alt=""></p>
<h3 id="4-垃圾邮件识别"><a href="#4-垃圾邮件识别" class="headerlink" title="4. 垃圾邮件识别"></a>4. 垃圾邮件识别</h3><p> 现在有已经分好类别的垃圾邮件和非垃圾邮件，希望训练一个垃圾邮件过滤器。有一个想法是分别统计垃圾邮件和非垃圾邮件中所包含的单词，在测试时，只要分析给定邮件中的单词，来计算这封邮件是垃圾邮件还是正常的邮件（其他算法分析单词顺序更加有效，这里不做介绍）。下面给出更加形象化定义。</p>
<p> 垃圾邮件：$spam$，正常邮件：$ham$。在训练时，我们计算$P(W_1|spam)$、$P(W_2|spam)$、$P(W_3|ham)$…。现在有一封邮件，分析其单词组成，计算$P(spam|W_1,W_2,W_3…)$和$P(ham|W_1,W_2,W_3…)$概率分别是多少。</p>
<p><strong>“朴素”</strong>：因为一封邮件中可能既包含$W_1$、又包含$W_2$，朴素就是假设$W_1$、$W_2$…相互独立。</p>
<script type="math/tex; mode=display">
P(AB) = P(A|B)P(B) = P(B|A)P(A)</script><script type="math/tex; mode=display">
P(A|B) ∝ P(B|A)P(A)</script><script type="math/tex; mode=display">
P(spam|W_1,W_2,...) ∝ P(W_1,W_2,...|spam)P(spam)</script><script type="math/tex; mode=display">
P(spam|W_1,W_2,...) ∝ P(W_1|spam)P(W_2|spam)...P(spam)</script><p>同理可以计算</p>
<script type="math/tex; mode=display">
P(ham|W_1,W_2,...) ∝ P(W_1|ham)P(W_2|ham)...P(ham)</script><p>最后将二者概率进行归一化，就可得出是垃圾邮件还是正常邮件的概率。</p>
<script type="math/tex; mode=display">
P(ham|W_1,W_2,...) = \frac{P(W_1|ham)P(W_2|ham)...P(ham)}{P(W_1|ham)P(W_2|ham)...P(ham)+P(W_1|spam)P(W_2|spam)...P(spam)}</script><h3 id="5-生成模型和判别模型"><a href="#5-生成模型和判别模型" class="headerlink" title="5 生成模型和判别模型"></a>5 生成模型和判别模型</h3><ul>
<li><p><strong>判别模型(discriminative model)</strong></p>
<p>通过求解<strong>条件概率分布</strong> $P(y|x)$ 或者直接计算 $y$ 的值来预测 $y$。如：线性回归（Linear Regression）,逻辑回归（Logistic Regression）,支持向量机（<code>SVM</code>）, 传统神经网络（Traditional Neural Networks）,线性判别分析（Linear Discriminative Analysis），条件随机场（Conditional Random Field）。</p>
</li>
<li><p><strong>生成模型(generative model)</strong></p>
<p>通过对观测值和标注数据计算<strong>联合概率分布</strong> $P(x,y)$ ，再计算$P(y|x)$来达到判定估算 $y$ 的目的。朴素贝叶斯（Naive Bayes）, 隐马尔科夫模型（<code>HMM</code>）,贝叶斯网络（Bayesian Networks）和隐含狄利克雷分布（Latent Dirichlet Allocation）、混合高斯模型（<code>GMM</code>）。</p>
</li>
</ul>
<h3 id="6-贝叶斯网络"><a href="#6-贝叶斯网络" class="headerlink" title="6 贝叶斯网络"></a>6 贝叶斯网络</h3><p> 贝叶斯网络(Bayesian network)，又称信念网络(Belief Network)，或有向无环图模型(directed acyclic graphical model)，是一种概率图模型，于 1985 年由 Judea Pearl 首先提出。它是一种模拟人类推理过程中因果关系的不确定性处理模型，其网络拓朴结构是一个有向无环图(DAG)。</p>
<p> 贝叶斯网络的有向无环图中的节点表示随机变量<code>{X1,X2,...,Xn}{X1,X2,...,Xn}</code>。它们可以是可观察到的变量，或隐变量、未知参数等。认为有因果关系（或非条件独立）的变量或命题则用箭头来连接。若两个节点间以一个单箭头连接在一起，表示其中一个节点是“因(parents)”，另一个是“果(children)”，两节点就会产生一个条件概率值。一个简单的贝叶斯网络如下图：</p>
<p><img src="/images/bayesNetwork.jpg" alt=""></p>
<p>例如，假设节点 E 直接影响到节点 H，即$E→H$，则用从$E$指向$H$的箭头建立结点$E$到结点$H$的有向弧(E,H)，权值(即连接强度)用条件概率$P(H|E)$来表示，如下图所示：</p>
<p> <img src="/images/dag.png" alt=""></p>
<p> 贝叶斯网络中，全部随机变量的联合分布如下：</p>
<script type="math/tex; mode=display">
P(x_1,x_2,···,x_n) = \prod_{i = 1}^{n} P(x_i|parent(x_i))</script><p> 比如上图的贝叶斯网络中，$x_1,x_2,x_3,…,x_7$的联合概率分布是：</p>
<script type="math/tex; mode=display">
P(x_1,x_2,...,x_7) = p(x_1)p(x_2)p(x_3)p(x_4|x_1,x_2,x_3)p(x_5|x_1,x_3)p(x_6|x_4)p(x_7|x_4,x_5)</script><p>下面介绍贝叶斯网络的结构形式：</p>
<ul>
<li><p>head-to-head</p>
<p><img src="/images/head-to-head.png" style="zoom:50%;" /></p>
<p><strong>在 $c$ 未知的情况下，$a$ ，$b$ 被阻断（blocked），是独立的。</strong></p>
</li>
<li><p>tail-to-tail</p>
<p><img src="/images/tail-to-tail.png" style="zoom:50%;" /></p>
<p><strong>在 $c$ 给定的情况下，$a$ ，$b$ 是独立的。</strong></p>
</li>
<li><p>head-to-tail</p>
<p><img src="/images/head-to-tail.png" style="zoom:50%;" /></p>
<p><strong>在 $c$ 给定的情况下，$a$ ，$b$ 被阻断（blocked），是独立的。</strong>其实这就是马尔可夫链，当前状态只与前一个状态相关。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>JAVA 并发编程</title>
    <url>/2020/02/07/JAVA%20%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="JAVA-并发编程"><a href="#JAVA-并发编程" class="headerlink" title="JAVA 并发编程"></a>JAVA 并发编程</h2><h3 id="1-锁"><a href="#1-锁" class="headerlink" title="1. 锁"></a>1. 锁</h3><h4 id="1-1-锁的类别"><a href="#1-1-锁的类别" class="headerlink" title="1.1 锁的类别"></a>1.1 锁的类别</h4><ul>
<li><p><strong>公平锁与非公平锁</strong></p>
<p><strong>公平锁</strong>：线程严格申请锁的顺序争抢锁，这些线程会被加入一个等待队列中</p>
<p><strong>非公平锁</strong>：线程并非严格按照先到先得的规则来争抢锁，即，有可能晚到的线程先拿到锁。</p>
<p><code>ReentrantLock</code>默认是非公平锁。</p>
<p><code>synchronized</code>是非公平锁。</p>
</li>
<li><p><strong>可重入锁（递归锁）</strong></p>
<p>​        这里要提到一点：如果一个对象存在多个<code>synchronized</code>修饰的方法，当一个线程访问其中一个同步方法时，其他线程访问不了这个对象的其他同步方法。根据<code>synchronized</code>的原理很好理解。</p>
<p>​        可重入锁，某个对象存在多个同步方法，当线程已经获得锁的情况下，该线程可以继续访问其他同步方法，而不必等待，这样做是为了<strong>避免死锁</strong>。</p>
<p> <code>ReentrantLock</code>和 <code>synchronized</code>都是可重入锁。</p>
</li>
<li><p><strong>自旋锁（spin lock）</strong></p>
<p>​        如果锁被其他线程占有，这个请求锁的线程便会被加入等待队列，此时<code>CPU</code>会继续调度其他线程，由于频繁切换线程的开销比较大，而且争抢锁不是很频繁。自旋锁就是当没有获取到锁，<code>CPU</code>不会挂起该线程，而是一直轮询（仍然占有<code>cpu</code>），直到该线程争抢到锁。这样做减轻了频繁切换线程，但是轮询增加了<code>CPU</code>负担。</p>
</li>
<li><p><strong>读写锁</strong></p>
<p>​        相对于独占锁，每个锁只能被一个线程占有，读写锁对于读请求，读锁可以由多个线程共享；写操作，写锁，只允许一个线程独占，以此来提高并发性。即读读可以并存，读写、写写不可以共存。 能保证<strong>读写</strong>、<strong>写读</strong>和<strong>写写</strong>的过程是互斥，<strong>读读</strong>的时候是共享的。 <code>java.util.concurrent</code>包中<code>ReentrantReadWriteLock</code>就是读写锁。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//读的时候加锁，共享锁</span></span><br><span class="line">  lock.readLock().lock();</span><br><span class="line">...</span><br><span class="line">  lock.readLock().unlock();</span><br><span class="line">        </span><br><span class="line">  <span class="comment">//写的时候加锁，独占锁</span></span><br><span class="line">  lock.writeLock().lock();</span><br><span class="line">  ...</span><br><span class="line">  lock.writeLock().unlock();</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCache</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> Map&lt;String, Object&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ReentrantReadWriteLock lock = <span class="keyword">new</span> ReentrantReadWriteLock();</span><br><span class="line">    WriteLock writeLock = lock.writeLock();</span><br><span class="line">    ReadLock readLock = lock.readLock();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(String key, Object value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            writeLock.lock();</span><br><span class="line">            System.out.println(Thread.currentThread().getName() + <span class="string">" 正在写入..."</span>);</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            map.put(key, value);</span><br><span class="line">            System.out.println(Thread.currentThread().getName() + <span class="string">" 写入完成，写入结果是 "</span> + value);</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            writeLock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">get</span><span class="params">(String key)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            readLock.lock();</span><br><span class="line">            System.out.println(Thread.currentThread().getName() + <span class="string">" 正在读..."</span>);</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            Object res = map.get(key);</span><br><span class="line">            System.out.println(Thread.currentThread().getName() + <span class="string">" 读取完成，读取结果是 "</span> + res);</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            readLock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadWriteLockDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        MyCache cache = <span class="keyword">new</span> MyCache();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">final</span> <span class="keyword">int</span> temp = i;</span><br><span class="line">            <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                cache.put(temp + <span class="string">""</span>, temp + <span class="string">""</span>);</span><br><span class="line">            &#125;).start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">final</span> <span class="keyword">int</span> temp = i;</span><br><span class="line">            <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                cache.get(temp + <span class="string">""</span>);</span><br><span class="line">            &#125;).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p><strong>死锁的四个必要条件</strong></p>
<ul>
<li><p>互斥：资源在某一时刻只允许被一个线程所访问</p>
</li>
<li><p>不可抢占：进程所获得的资源在未使用完毕之前，不能被其他进程强行夺走，即只能由获得该资源的进程自己来释放（只能是主动释放)。 </p>
</li>
<li><p>循环等待：</p>
</li>
<li><p>占有且等待：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源已被其他进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。 </p>
<p><strong>本质原因</strong>：系统资源有限、进程推进顺不当。</p>
</li>
</ul>
</li>
</ul>
<h4 id="1-2-synchronized-的原理"><a href="#1-2-synchronized-的原理" class="headerlink" title="1.2 synchronized 的原理"></a>1.2 synchronized 的原理</h4><p>普通方法：锁是当前实例对象</p>
<p>静态方法：所示当前实例对象的Class对象</p>
<p>方法块：括号内的对象</p>
<p><strong>monitor机制</strong></p>
<p>​        每一个线程都有一个可用的<code>mointor record</code>列表，每一个被锁住的对象都会和一个<code>monitor record</code>关联（存放在对象头信息中）。当一个对象的<code>monitor</code>被持有后，该对象即处于锁定状态。</p>
<p>​        同步代码块开始之前，<code>monitor enter</code>指令插入，线程获取<code>monitor</code>所有权，方法块结束，<code>monitor exit</code>指令插入，释放锁。</p>
<p><strong>Java对象在内存中的结构</strong></p>
<p><img src="/images/Java对象结构.png" alt=""></p>
<h4 id="1-3-volatile-关键字"><a href="#1-3-volatile-关键字" class="headerlink" title="1.3 volatile 关键字"></a>1.3 volatile 关键字</h4><p><strong>Java Memory Model</strong></p>
<p>​        在Java内存模型中，每个线程执行时，都会拷贝主内存中数据到自己的栈内存（工作内存）中，当程序结束后，再把数据写回主内存。由此，当多个线程共同访问这个值，各个线程是相互独立的，其中一个修改，其他的并不知情，当多个线程协作修改数据，就会造成数据不一致。</p>
<p>​        <code>volatile</code>的作用就是，被其修饰的变量，一旦值发生改变，其他线程就会放弃自己栈内存中的值，重新向内存中取值，从而保证了数据的内存可见性。</p>
<p><img src="/images/JMM.png" style="zoom:80%;" /></p>
<p><strong>特性</strong>：禁止指令重排、不保证原子性、内存可见性</p>
<h4 id="1-4-synchronized-和-Lock-有什么区别？"><a href="#1-4-synchronized-和-Lock-有什么区别？" class="headerlink" title="1.4 synchronized 和 Lock 有什么区别？"></a>1.4 synchronized 和 Lock 有什么区别？</h4><ul>
<li>原始结构<ul>
<li>synchronized 是关键字属于 JVM 层面，反应在字节码上是 <code>monitorenter</code> 和 <code>monitorexit</code>，其底层是通过 <code>monito</code>r 对象来完成，其实 wait/notify 等方法也是依赖 monitor 对象只有在同步快或方法中才能调用 <code>wait/notify</code> 等方法。</li>
<li>Lock 是具体类（<code>java.util.concurrent.locks.Lock</code>）是 api 层面的锁。</li>
</ul>
</li>
<li>使用方法<ul>
<li><code>synchronized</code> 不需要用户手动去释放锁，当 <code>synchronized</code> 代码执行完后系统会自动让线程释放对锁的占用。</li>
<li><code>ReentrantLock</code> 则需要用户手动的释放锁，若没有主动释放锁，可能导致出现死锁的现象，lock() 和 unlock() 方法需要配合 try/finally 语句来完成。</li>
</ul>
</li>
<li>等待是否可中断<ul>
<li><code>synchronized</code> 不可中断，除非抛出异常或者正常运行完成。</li>
<li><code>ReentrantLock</code> 可中断，设置超时方法 <code>tryLock(long timeout, TimeUnit unit)，lockInterruptibly()</code> 放代码块中，调用 interrupt() 方法可中断。</li>
</ul>
</li>
<li>加锁是否公平<ul>
<li><code>synchronized</code> 非公平锁</li>
<li><code>ReentrantLock</code> 默认非公平锁，构造方法中可以传入 boolean 值，true 为公平锁，false 为非公平锁。</li>
</ul>
</li>
<li>锁可以绑定多个 Condition<ul>
<li><code>synchronized</code> 没有 <code>Condition</code>。</li>
<li><code>ReentrantLock</code>用来实现分组唤醒需要唤醒的线程们，可以精确唤醒，而不是像 <code>synchronized</code> 要么随机唤醒一个线程要么唤醒全部线程。</li>
</ul>
</li>
</ul>
<h3 id="2-线程池"><a href="#2-线程池" class="headerlink" title="2. 线程池"></a>2. 线程池</h3><p>​        为什么要用线程池？答：它预先创建好一部分线程，使用完后放回池中，避免了创建与销毁线程的昂贵开销，使得性能大大提升 。</p>
<p>主要特点为：</p>
<ul>
<li>线程复用</li>
<li>控制最大并发数量</li>
<li>管理线程</li>
</ul>
<h4 id="2-1-线程的状态"><a href="#2-1-线程的状态" class="headerlink" title="2.1 线程的状态"></a>2.1 线程的状态</h4><p>​        在Java中，线程共有以下几种状态</p>
<ul>
<li><p><strong>创建</strong>。</p>
<p>Java中有三种方法创建线程。</p>
<p>(1). 继承<code>Thread</code>类。重写<code>run</code>方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mythreas</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">        ...</span><br><span class="line">   	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(2). 实现<code>Runnable</code>接口中的<code>run</code>方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mythread2</span> <span class="keyword">implements</span> <span class="title">Runnable</span></span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">        ...</span><br><span class="line">   	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(3). 实现<code>Callable</code>接口中的<code>call</code>方法，注意，该方法是有返回值的，并且含有泛型。执行<code>Callable</code>方式，需要<code>FutureTask</code>实现类的支持，用于接受运算结果。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ThreadDemo</span> <span class="keyword">implements</span> <span class="title">Callable</span>&lt;<span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">100</span>; i++) &#123;</span><br><span class="line">            sum += <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadPoolDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Mythread t1 = <span class="keyword">new</span> Mythread();</span><br><span class="line">        <span class="comment">//1.执行Callable方式，需要FutureTask实现类的支持，用于接受运算结果。</span></span><br><span class="line">        FutureTask&lt;Integer&gt; result = <span class="keyword">new</span> FutureTask&lt;Integer&gt;(t1);</span><br><span class="line">        <span class="keyword">new</span> Thread(result).start();</span><br><span class="line">        </span><br><span class="line">        ThreadPoolExecutor poolExecutor = <span class="keyword">new</span> ThreadPoolExecutor(<span class="number">1</span>,<span class="number">1</span>,<span class="number">0L</span>,</span><br><span class="line">                TimeUnit.MILLISECONDS,</span><br><span class="line">                <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;(),</span><br><span class="line">                <span class="keyword">new</span> ThreadPoolExecutor.CallerRunsPolicy());</span><br><span class="line">        <span class="comment">// 通过线程池提交，Future类接受运算结果</span></span><br><span class="line">        Future&lt;Integer&gt; submit = poolExecutor.submit(t1);</span><br><span class="line">        Integer sum = <span class="keyword">null</span>;<span class="comment">//FutureTask也可用闭锁的操作</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            sum = result.get();</span><br><span class="line">            Integer res = submit.get();</span><br><span class="line">            System.out.println(res);</span><br><span class="line">            System.out.println(sum);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>就绪</strong>。线程已经<code>start</code>，只是没有得到<code>CPU</code>时间片。执行<code>yield</code>方法。</p>
</li>
<li><p><strong>运行</strong>。线程得到时间片执行。</p>
</li>
<li><p><strong>阻塞</strong>。线程执行<code>sleep/join</code>方法，或<code>wait</code>方法。注意执行两个方法线程状态的区别，<code>sleep</code>方法不会释放所持有的锁，时间结束后转到<code>就绪</code>态；<code>wait</code>方法会释放持有的锁，线程进入<strong>等待队列</strong>，直到有线程执行<code>notify/notifyAll</code>方法，该线程被唤醒，进入<strong>锁池</strong>中，争抢锁。拿到锁就可以转为<strong>就绪</strong>态。</p>
</li>
<li><p><strong>销毁</strong></p>
</li>
</ul>
<h4 id="2-2-线程池参数详解"><a href="#2-2-线程池参数详解" class="headerlink" title="2.2 线程池参数详解"></a>2.2 线程池参数详解</h4><p>首先给出<code>java</code>中线程池的构造函数（其中之一），有几个参数，然后解释他的工作流程。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ThreadPoolExecutor</span><span class="params">(<span class="keyword">int</span> corePoolSize,</span></span></span><br><span class="line"><span class="function"><span class="params">                          <span class="keyword">int</span> maximumPoolSize,</span></span></span><br><span class="line"><span class="function"><span class="params">                          <span class="keyword">long</span> keepAliveTime,</span></span></span><br><span class="line"><span class="function"><span class="params">                          TimeUnit unit,</span></span></span><br><span class="line"><span class="function"><span class="params">                          BlockingQueue&lt;Runnable&gt; workQueue,</span></span></span><br><span class="line"><span class="function"><span class="params">                          RejectedExecutionHandler handler)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,</span><br><span class="line">             Executors.defaultThreadFactory(), handler);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>corePoolSize</code> 核心线程数</p>
</li>
<li><p><code>maximumPoolSize 最大线程数</code></p>
</li>
<li><p><code>keepAliveTime</code>  保活时间</p>
</li>
<li><p><code>TimeUnit unit</code> 时间单位</p>
</li>
<li><p><code>BlockingQueue&lt;Runnable&gt; workQueue</code></p>
</li>
<li><p><code>RejectedExecutionHandler handler</code>  <strong>拒绝策略</strong></p>
<p><code>ThreadPoolExecutor.AbortPolicy</code>: 丢弃任务并抛出<code>RejectedExecutionException</code>异常。 (默认)<br><code>ThreadPoolExecutor.DiscardPolicy</code>：也是丢弃任务，但是不抛出异常。<br><code>ThreadPoolExecutor.DiscardOldestPolicy</code>：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）<br><code>ThreadPoolExecutor.CallerRunsPolicy</code>：由调用线程处理该任务</p>
<p><strong>工作流程</strong></p>
<ol>
<li>当有任务到达时，如果已经创建的线程数小于<code>corePoolSize</code>，那么创建一个线程来执行这个任务。</li>
<li>如果已经创建的线程数等于<code>corePoolSize</code>，并且存在空闲的线程，那么空闲的线程来执行这个任务。</li>
<li>如果没有空闲线程，那么那这个任务加入到阻塞队列中。</li>
<li>如果阻塞队列也满了，判断<code>corePoolSize &lt; maximumPoolSize</code>，比如<code>maximumPoolSize = 5,corePoolSize = 3</code>，那么创建一个线程来执行这个任务。否则，执行拒绝策略，或者存活线程数目达到最大线程数目，也会执行拒绝策略。（即：当最大线程数与队列均满了以后，才会执行拒绝策略。 ）</li>
<li>当大于<code>corePoolSize</code>并且空闲的线程（余下线程），在超过<code>keepAliveTime</code>后，会被回收。</li>
</ol>
</li>
</ul>
<p><strong>tips</strong>：阿里开发规范中强调：【强制】线程池不允许使用 Executors 去创建，而是通过 <code>ThreadPoolExecutor</code> 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 </p>
<p><img src="/images/ThreadPoolExecutor.png" alt=""></p>
<p><center>ThreadPoolExecutor类图</center></p>
<h4 id="2-3-阻塞队列"><a href="#2-3-阻塞队列" class="headerlink" title="2.3 阻塞队列"></a>2.3 阻塞队列</h4><p>当阻塞队列是空时，从队列中获取元素的操作将会被阻塞。</p>
<p>当阻塞队列是满时，往队列里添加元素的操作将会被阻塞。</p>
<p><code>ArrayBlockingQueue</code> ：一个由数组结构组成的有界阻塞队列。<br><code>LinkedBlockingQueue</code>：一个由链表结构组成的有界阻塞队列。<br><code>PriorityBlockingQueue</code> ：一个支持优先级排序的无界阻塞队列。<br><code>DelayQueue</code>： 一个使用优先级队列实现的无界阻塞队列。<br><code>SynchronousQueue</code>： 一个不存储元素的阻塞队列，每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态 。<br><code>LinkedTransferQueue</code>： 一个由链表结构组成的无界阻塞队列。<br><code>LinkedBlockingDeque</code>： 一个由链表结构组成的双向阻塞队列。 </p>
<ul>
<li>核心方法API</li>
</ul>
<p><img src="/images/image-20200208113359243.png" alt="image-20200208113359243"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">抛出异常</td>
<td>当阻塞队列满时,再往队列里面add插入元素会抛<code>IllegalStateException: Queue full</code><br/>当阻塞队列空时,再往队列Remove元素时候回抛出<code>NoSuchElementException</code></td>
</tr>
<tr>
<td style="text-align:left">特殊值</td>
<td>插入方法,成功返回<code>true</code> 失败返回<code>false</code><br/>移除方法,成功返回元素,队列里面没有就返回<code>null</code></td>
</tr>
<tr>
<td style="text-align:left">一直阻塞</td>
<td>当阻塞队列满时,生产者继续往队列里面<code>put</code>元素,队列会一直阻塞直到put数据or响应中断退出<br/>当阻塞队列空时,消费者试图从队列<code>take</code>元素,队列会一直阻塞消费者线程直到队列可用.</td>
</tr>
<tr>
<td style="text-align:left">超时退出</td>
<td>当阻塞队列满时,队列会阻塞生产者线程一定时间,超过后限时后生产者线程就会退出</td>
</tr>
</tbody>
</table>
</div>
<h4 id="2-4-线程池的种类"><a href="#2-4-线程池的种类" class="headerlink" title="2.4 线程池的种类"></a>2.4 线程池的种类</h4><p><code>Executors</code>类在创建线程池时，底层还是采用<code>ThreadPoolExecutor</code>的构造方法。</p>
<ul>
<li><p><code>Executors.newFixedThreadPool</code> 定长线程池</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newFixedThreadPool</span><span class="params">(<span class="keyword">int</span> nThreads)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ThreadPoolExecutor(nThreads, nThreads,</span><br><span class="line">                                      <span class="number">0L</span>, TimeUnit.MILLISECONDS,</span><br><span class="line">                                      <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>  <strong>作用</strong>：创建一个可重用固定线程数量的线程池，以共享的无界队列方式来运行这些线程。</p>
<p>  <strong>特征</strong>：<br>  （1）线程池中的线程处于一定的量，可以很好的控制线程的并发量<br>  （2）线程可以重复被使用，在显示关闭之前，都将一直存在<br>  （3）超出一定量的线程被提交时候需在队列中等待</p>
<ul>
<li><p><code>newCachedThreadPool</code> 缓存线程池</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Executors</span> </span>&#123;</span><br><span class="line">	....</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newCachedThreadPool</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ThreadPoolExecutor(<span class="number">0</span>, Integer.MAX_VALUE,</span><br><span class="line">                                      <span class="number">60L</span>, TimeUnit.SECONDS,</span><br><span class="line">                                      <span class="keyword">new</span> SynchronousQueue&lt;Runnable&gt;());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据需要创建线程，可以看到，<code>corePoolSize = 0 maximumPoolSize = Integer.MAX_VALUE</code>即没有核心线程，当60s内没有任务时，将会回收存活的线程，60s内有任务时，他可以重用已有的线程 。就是来一个任务创建一个线程，最多创建21亿个线程。</p>
</li>
<li><p><code>newScheduledThreadPool</code> 定时线程池</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ScheduledExecutorService <span class="title">newScheduledThreadPool</span><span class="params">(<span class="keyword">int</span> corePoolSize)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ScheduledThreadPoolExecutor(corePoolSize);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ScheduledThreadPoolExecutor</span><span class="params">(<span class="keyword">int</span> corePoolSize)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(corePoolSize, Integer.MAX_VALUE, <span class="number">0</span>, NANOSECONDS,</span><br><span class="line">              <span class="keyword">new</span> DelayedWorkQueue());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 该线程池适合执行延时任务。 </p>
</li>
</ul>
<ul>
<li><p><code>newSingleThreadExecutor</code> 只有一个线程的线程池 </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newSingleThreadExecutor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> FinalizableDelegatedExecutorService</span><br><span class="line">            (<span class="keyword">new</span> ThreadPoolExecutor(<span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">                                    <span class="number">0L</span>, TimeUnit.MILLISECONDS,</span><br><span class="line">                                    <span class="keyword">new</span> LinkedBlockingQueue&lt;Runnable&gt;()));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>总结</strong>：实际开发用哪个？哪个都不用，上面的阿里规范提到，用<code>ThreadPoolExecutor</code>来初始化一个线程池，参数自己指定。</p>
<h3 id="3-Compare-and-Swap"><a href="#3-Compare-and-Swap" class="headerlink" title="3. Compare and Swap"></a>3. Compare and Swap</h3><h4 id="3-1-原理"><a href="#3-1-原理" class="headerlink" title="3.1 原理"></a>3.1 原理</h4><p>​        <code>compare and swap</code> 简称<code>CAS</code>，本质上一条原子指令，即要么执行，要么不执行，利用<code>CPU</code>底层来实现并发过程中数据不一致问题。<code>java.util.concurrent.atmoic.AtomicInteger</code>等类底层实现就采用了<code>CAS</code>，来解决并发过程中<code>i++</code>导致的数据不一致问题。实际上<code>i++</code>在被虚拟机编译后，并非一条语句，而被划分为三条指令，因此在多线程环境下会造成数据不一致。</p>
<h4 id="3-2-带来的问题"><a href="#3-2-带来的问题" class="headerlink" title="3.2 带来的问题"></a>3.2 带来的问题</h4><p>典型的<code>ABA</code>问题。即<code>CAS</code>关注了结果正确，而忽视了过程是否正确。</p>
<p>解决方案：原子引用。类似于<code>MySQL</code>中的时间戳。</p>
<h3 id="4-线程不安全类"><a href="#4-线程不安全类" class="headerlink" title="4. 线程不安全类"></a>4. 线程不安全类</h3><h4 id="4-1-ArrayList"><a href="#4-1-ArrayList" class="headerlink" title="4.1  ArrayList"></a>4.1  ArrayList</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ContainerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        List&lt;Integer&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        Random random = <span class="keyword">new</span> Random();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                list.add(random.nextInt(<span class="number">10</span>));</span><br><span class="line">                System.out.println(list);</span><br><span class="line">            &#125;).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>会报： <code>java.util.ConcurrentModificationException</code></p>
<ul>
<li>解决方案<ul>
<li><code>new Vector();</code></li>
<li><code>Collections.synchronizedList(new ArrayList&lt;&gt;());</code></li>
<li><code>new CopyOnWriteArrayList&lt;&gt;();</code></li>
</ul>
</li>
<li>优化建议<ul>
<li>在读多写少的时候推荐使用 <code>CopeOnWriteArrayList</code> 这个类。底层采用了<code>ReentrantLock</code>，读不加锁，<code>set/add</code>加锁处理。</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ContainerDemo2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String []args)</span></span>&#123;</span><br><span class="line">        ArrayList&lt;Integer&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        CopyOnWriteArrayList&lt;Integer&gt; list2 = <span class="keyword">new</span> CopyOnWriteArrayList&lt;&gt;();</span><br><span class="line">        Random random = <span class="keyword">new</span> Random();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">new</span> Thread(() -&gt; &#123;</span><br><span class="line">                list2.add(random.nextInt(<span class="number">10</span>));</span><br><span class="line">                System.out.println(list2);</span><br><span class="line">            &#125;).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>HMM模型笔记</title>
    <url>/2020/02/07/HMM%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h2><p><strong>记住一下几个要点</strong></p>
<ul>
    <li>
        <strong>3个基本组成元素</strong>
    </li>
    <li>
        <strong>2个基本假设</strong>
    </li>
    <li>
        <strong>3个基本问题</strong>
    </li>
</ul>

<p><strong>tips</strong>：其实我在看好多算法时，涉及到许多数学公式，如果你刚开始接触这个模型算法，先不要直接就开始研究公式，先明白这个算法或模型时如何工作的，找个例子算一下，跑一下。数学公式是方便为了归纳总结而提出来的，如果你能很好的理解这个算法，那么他的公式也就不难理解了。这是我学习中的一点小经验。</p>
<h3 id="1-3个基本组成元素"><a href="#1-3个基本组成元素" class="headerlink" title="1 3个基本组成元素"></a>1 3个基本组成元素</h3><p><strong>隐马尔科夫模型可以用三元组表示</strong></p>
<p>同时，一般用 $I$ 表示<em>隐藏序列</em>，$O$ 表示<em>观测序列</em> ，用 $Q$ 表示可能的状态集合，用 $V$ 表示可能的观测集合</p>
<script type="math/tex; mode=display">
Q = \{q_1,q_2,...,q_M\}, V = \{v_1,v_2,...,v_N\}</script><p>，$M$ 为可能的状态数，$N$ 为可能的观测数。则可以确定状态转移概率矩阵大小为 $M$ 阶的方阵，观测概率矩阵大小为 $M * N$ 的矩阵。 </p>
<p><strong>总结</strong>：一条隐藏的马尔可夫链随机生成了一个不可观测的状态序列，然后每个状态又对应生成了一个观测结果（状态—&gt;观测, 观测概率矩阵B），这些观测结果按照时序排列形成了观测序列。</p>
<h4 id="1-1-初始状态概率序列（-pi-）"><a href="#1-1-初始状态概率序列（-pi-）" class="headerlink" title="1.1 初始状态概率序列（$\pi$）"></a>1.1 初始状态概率序列（$\pi$）</h4><script type="math/tex; mode=display">
\pi = (\pi_i)</script><p>$\pi_i$表示初始时刻处于状态$q_i$的概率。</p>
<h4 id="1-2-状态转移概率矩阵（-A-）"><a href="#1-2-状态转移概率矩阵（-A-）" class="headerlink" title="1.2 状态转移概率矩阵（$A$）"></a>1.2 状态转移概率矩阵（$A$）</h4><script type="math/tex; mode=display">
A = [a_{ij}]_{M \times M}</script><p>$a_{ij}$表示时刻 $t$ 处于状态$q_i$，在时刻$t+1$转移到状态 $q_j$的概率。这也意味着$A$中每行元素和为$1$。</p>
<h4 id="1-3-观测概率矩阵（-B-）"><a href="#1-3-观测概率矩阵（-B-）" class="headerlink" title="1.3 观测概率矩阵（$B$）"></a>1.3 观测概率矩阵（$B$）</h4><p>不同状态生成不同观测结果的概率。</p>
<script type="math/tex; mode=display">
B = [b_{ik}]_{M \times N}</script><p>$b_{ik}$表示时刻$t$处于状态$q_i$生成观测结果$v_k$的概率（即每一行表示一个状态，每一列表示一个观测结果）。</p>
<h3 id="2-2个基本假设"><a href="#2-2个基本假设" class="headerlink" title="2 2个基本假设"></a>2 2个基本假设</h3><h4 id="2-1-齐次性，当前状态只和前一个状态相关"><a href="#2-1-齐次性，当前状态只和前一个状态相关" class="headerlink" title="2.1  齐次性，当前状态只和前一个状态相关"></a>2.1  齐次性，当前状态只和前一个状态相关</h4><p>​        任意时刻 $t$ 的状态只依赖前一时刻的状态。与其他时刻的观测与状态无关，也与 $t$ 时刻无关。 </p>
<h4 id="2-2-观测独立性假设"><a href="#2-2-观测独立性假设" class="headerlink" title="2.2 观测独立性假设"></a>2.2 观测独立性假设</h4><p>​        假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测及状态无关。</p>
<h3 id="3-3个基本问题"><a href="#3-3个基本问题" class="headerlink" title="3 3个基本问题"></a>3 3个基本问题</h3><h4 id="3-1-概率计算"><a href="#3-1-概率计算" class="headerlink" title="3.1 概率计算"></a>3.1 概率计算</h4><p><strong>问题描述</strong>： </p>
<p>​        在给定模型参数 $\lambda$ 的情况下，观测序列 $O$ 出现的概率是多少？即 $P(O|\lambda)$ 概率值为多少？</p>
<p><strong>如何解决</strong></p>
<p>​        1.暴力计算，算法实现的时间复杂度太高，理论可行，实际不采用！</p>
<p>​        2.前向算法，利用动态规划，将时间复杂度降到O($n^2$)</p>
<p>​        3.后向算法，基本同前向算法。</p>
<h4 id="3-2-预测问题（解码问题）"><a href="#3-2-预测问题（解码问题）" class="headerlink" title="3.2 预测问题（解码问题）"></a>3.2 预测问题（解码问题）</h4><p><strong>问题描述</strong></p>
<p>​        给定信息：</p>
<p>​        <strong>模型</strong>：$\lambda = \{A,B,\pi\}$</p>
<p>​        <strong>观测序列</strong>：$O = \{o_1,o_2,…,o_T\}$</p>
<p>​        <strong>求解</strong>：求使得$P(O|S)$最大的状态序列$S$.</p>
<p><strong>如何解决</strong></p>
<p>​        <strong>维特比</strong>算法（动态规划—最大概率路径）。其实简单来说，要找到一条从起点到终点的最优路径，最大概率路径因为满足<strong>最优子结构</strong>和<strong>重叠子问题</strong>，因此可以从采用动态规划来解。</p>
<p>$\sigma_t(i)$ 表示 <code>t</code>时刻状态为 <code>i</code>, 计算如下：</p>
<script type="math/tex; mode=display">
\sigma_{t+1}(i) = max\{\sigma_{t}a_{ji}\} * b_i(o_{t+1}), j = 1,2,...N. N为状态数</script><p>$a_{ji}$表示由第<code>j</code>个状态向第<code>i</code>个状态转移的概率，$b_i(o_{t+1})$表示由第<code>i</code>个状态生成<code>t+1</code>时序时的观测的概率。递推计算，<code>from t =1 to t = T</code>。然后输出状态序列。</p>
<h4 id="3-3-学习问题（参数估计）"><a href="#3-3-学习问题（参数估计）" class="headerlink" title="3.3 学习问题（参数估计）"></a>3.3 学习问题（参数估计）</h4><p><strong>问题描述</strong></p>
<p>​        给定信息：</p>
<p>​        <strong>观测序列</strong>: $O = \{o_1,o_2,…,o_T\}$</p>
<p>​        <strong>求解</strong>：模型$\lambda$ 参数，</p>
<p><strong>如何求解</strong></p>
<p>​        <strong>EM</strong>算法</p>
<h3 id="4-有啥用"><a href="#4-有啥用" class="headerlink" title="4 有啥用?"></a>4 有啥用?</h3><h4 id="4-1-中文分词"><a href="#4-1-中文分词" class="headerlink" title="4.1 中文分词"></a>4.1 中文分词</h4><p>我们假设每个词都有Begin、Medium、End、S分别代表开头、中间、结尾、独立的词等隐状态，所给的文本为观测态，我们要做的就是训练（语料库）一个HMM模型，在给定文本的情况下，输出隐含态，也就是分词结果。</p>
<h4 id="4-2-语音识别"><a href="#4-2-语音识别" class="headerlink" title="4.2 语音识别"></a>4.2 语音识别</h4><h4 id="参考书目"><a href="#参考书目" class="headerlink" title="参考书目"></a>参考书目</h4><p>统计学习方法（第2版），李航。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>二叉树</title>
    <url>/2020/02/06/%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
    <content><![CDATA[<h2 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h2><p>首先给出二叉树的定义    </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> val;</span><br><span class="line">    TreeNode left;</span><br><span class="line">    TreeNode right;</span><br><span class="line">    TreeNode(<span class="keyword">int</span> x) &#123; val = x; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-你能遍历二叉树吗"><a href="#1-你能遍历二叉树吗" class="headerlink" title="1. 你能遍历二叉树吗"></a>1. 你能遍历二叉树吗</h3><p>​        树是一种天然的递归结构，因此涉及到树的问题，大多数可以由递归解决。遍历二叉树最形象的就是递归算法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//前序遍历</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">preorder</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(root == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    System.out.println(root.val);</span><br><span class="line">    preorder(root.left);</span><br><span class="line">    preorder(root.right);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​        其他两种遍历方法只需改变函数的调用位置即可。以下设计的主要是非递归算法。这就用到了栈这个数据结构。</p>
<h4 id="1-1-前序遍历"><a href="#1-1-前序遍历" class="headerlink" title="1.1 前序遍历"></a>1.1 前序遍历</h4><p>​        该问题是<code>Leetcode 144</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">preorderTraversal</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">       LinkedList&lt;TreeNode&gt; stack = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">       ArrayList&lt;Integer&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">       <span class="keyword">if</span> (root == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> res;</span><br><span class="line">       stack.push(root);</span><br><span class="line">       <span class="keyword">while</span> (!stack.isEmpty()) &#123;</span><br><span class="line">           TreeNode top = stack.pop();</span><br><span class="line">           res.add(top.val);</span><br><span class="line">           <span class="keyword">if</span> (top.right != <span class="keyword">null</span>)</span><br><span class="line">               stack.push(top.right);</span><br><span class="line">           <span class="keyword">if</span> (top.left != <span class="keyword">null</span>)</span><br><span class="line">               stack.push(top.left);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-2-中序遍历"><a href="#1-2-中序遍历" class="headerlink" title="1.2 中序遍历"></a>1.2 中序遍历</h4><p>​        该问题是<code>Leetcode 94</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">inorderTraversal</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">       List&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">       <span class="keyword">if</span>(root == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> ans;</span><br><span class="line">       LinkedList&lt;TreeNode&gt; s = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">       TreeNode cur = root;</span><br><span class="line">       <span class="keyword">while</span>(!s.isEmpty()||cur != <span class="keyword">null</span>)&#123;</span><br><span class="line">           <span class="keyword">while</span>(cur!=<span class="keyword">null</span>)&#123;</span><br><span class="line">               s.push(cur);</span><br><span class="line">               cur = cur.left;</span><br><span class="line">           &#125;</span><br><span class="line">           TreeNode top = s.pop();</span><br><span class="line">           ans.add(top.val);</span><br><span class="line">           cur = top.right;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> ans;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-3-后序遍历"><a href="#1-3-后序遍历" class="headerlink" title="1.3 后序遍历"></a>1.3 后序遍历</h4><p>​        这是<code>Leetcode 145</code>号问题，下面介绍的方法其实是利用栈，模拟了函数递归执行的过程。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Command</span> </span>&#123;</span><br><span class="line">       String com;</span><br><span class="line">       TreeNode t;</span><br><span class="line"></span><br><span class="line">       Command(String s, TreeNode p) &#123;</span><br><span class="line">           com = s;</span><br><span class="line">           t = p;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">postorderTraversal</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">       ArrayList&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">       <span class="keyword">if</span>(root == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> ans;</span><br><span class="line">       LinkedList&lt;Command&gt; s = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">       s.push(<span class="keyword">new</span> Command(<span class="string">"go"</span>, root));</span><br><span class="line">       <span class="keyword">while</span> (!s.isEmpty()) &#123;</span><br><span class="line">           Command top = s.pop();</span><br><span class="line">           <span class="keyword">if</span> (top.com.equals(<span class="string">"print"</span>)) &#123;</span><br><span class="line">               ans.add(top.t.val);</span><br><span class="line">           &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">               s.push(<span class="keyword">new</span> Command(<span class="string">"print"</span>, top.t));</span><br><span class="line">               <span class="keyword">if</span> (top.t.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">                   s.push(<span class="keyword">new</span> Command(<span class="string">"go"</span>, top.t.right));</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="keyword">if</span> (top.t.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">                   s.push(<span class="keyword">new</span> Command(<span class="string">"go"</span>, top.t.left));</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> ans;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-4-层次遍历"><a href="#1-4-层次遍历" class="headerlink" title="1.4 层次遍历"></a>1.4 层次遍历</h4><p>​        层次遍历用到了<strong>队列</strong>这种数据结构。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">levelOrder</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">       List&lt;Integer&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">       <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">           <span class="keyword">return</span> ans;</span><br><span class="line">       &#125;</span><br><span class="line">       LinkedList&lt;TreeNode&gt; q = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">       q.offer(root);</span><br><span class="line">       <span class="keyword">while</span>(!q.isEmpty())&#123;</span><br><span class="line">           TreeNode top = q.poll();</span><br><span class="line">           ans.add(top.val);</span><br><span class="line">           <span class="keyword">if</span>(top.left != <span class="keyword">null</span>)&#123;</span><br><span class="line">               q.offer(top.left);</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">if</span>(top.right != <span class="keyword">null</span>)&#123;</span><br><span class="line">               q.offer(top.right);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> ans;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>​        如果要将每一层的结果保存在一个列表中，该如何？这是<code>Leetcode 102</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//和层次遍历一样，只不过每次都要保存队列的长度</span></span><br><span class="line"><span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;</span><br><span class="line">       List&lt;List&lt;Integer&gt;&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">       <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">           <span class="keyword">return</span> ans;</span><br><span class="line">       &#125;</span><br><span class="line">       LinkedList&lt;TreeNode&gt; q = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">       q.offer(root);</span><br><span class="line">       <span class="keyword">while</span>(!q.isEmpty())&#123;</span><br><span class="line">           <span class="keyword">int</span> len = q.size();</span><br><span class="line">           ArrayList&lt;Integer&gt; tmp = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">               TreeNode top = q.poll();</span><br><span class="line">               tmp.add(top.val);</span><br><span class="line">               <span class="keyword">if</span>(top.left != <span class="keyword">null</span>)&#123;</span><br><span class="line">               	q.offer(top.left);</span><br><span class="line">           	&#125;</span><br><span class="line">           	<span class="keyword">if</span>(top.right != <span class="keyword">null</span>)&#123;</span><br><span class="line">               	q.offer(top.right);</span><br><span class="line">           	&#125;</span><br><span class="line">           &#125;</span><br><span class="line">           ans.add(tmp);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> ans;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>​        </p>
<h4 id="1-5-拓展以及变体"><a href="#1-5-拓展以及变体" class="headerlink" title="1.5 拓展以及变体"></a>1.5 拓展以及变体</h4><h5 id="1-5-1-路径总和"><a href="#1-5-1-路径总和" class="headerlink" title="1.5.1 路径总和"></a>1.5.1 路径总和</h5><p>​        这是<code>Leetcode 113</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">给定一个二叉树和一个目标和，找到所有从根节点到叶子节点路径总和等于给定目标和的路径。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">说明: 叶子节点是指没有子节点的节点。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, <span class="keyword">int</span> sum) &#123;</span><br><span class="line">       List&lt;List&lt;Integer&gt;&gt; ans = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">       <span class="keyword">if</span>(root == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> ans;</span><br><span class="line">       helper(root, <span class="keyword">new</span> LinkedList&lt;Integer&gt;(), ans, sum);</span><br><span class="line">       <span class="keyword">return</span> ans;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">helper</span><span class="params">(TreeNode root , LinkedList&lt;Integer&gt; tmp, List&lt;List&lt;Integer&gt;&gt; ans, <span class="keyword">int</span> sum)</span></span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(root.left == <span class="keyword">null</span> &amp;&amp; root.right == <span class="keyword">null</span>)&#123;<span class="comment">///叶子节点</span></span><br><span class="line">           <span class="keyword">if</span>(root.val - sum == <span class="number">0</span>)&#123;</span><br><span class="line">               tmp.add(root.val);</span><br><span class="line">               LinkedList&lt;Integer&gt; p = (LinkedList&lt;Integer&gt;) tmp.clone();<span class="comment">//做一个深拷贝</span></span><br><span class="line">               ans.add(p);</span><br><span class="line">               tmp.removeLast();<span class="comment">//否则，修改tmp会影响到ans里的结果，因为只有一个引用</span></span><br><span class="line">               <span class="keyword">return</span> ;</span><br><span class="line">           &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">               <span class="keyword">return</span> ;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       tmp.add(root.val);</span><br><span class="line">       <span class="keyword">if</span>(root.left!=<span class="keyword">null</span>)&#123;</span><br><span class="line">           helper(root.left, tmp, ans, sum - root.val);</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">if</span>(root.right!=<span class="keyword">null</span>)&#123;</span><br><span class="line">           helper(root.right, tmp, ans, sum - root.val);</span><br><span class="line">       &#125;</span><br><span class="line">       tmp.removeLast();<span class="comment">//回溯</span></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h5 id="1-5-2-二叉搜索树的最近公共祖先"><a href="#1-5-2-二叉搜索树的最近公共祖先" class="headerlink" title="1.5.2  二叉搜索树的最近公共祖先"></a>1.5.2  二叉搜索树的最近公共祖先</h5><p>​        这是<a href="https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-search-tree/" target="_blank" rel="noopener">235号问题</a>。 主要利用了二叉树的性质。如果查找的两个结点值一个大于根节点值，一个小于根节点值或等于。那么这两个节点分布在根节点两个子树中，最近的结点就是根节点。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> TreeNode <span class="title">lowestCommonAncestor</span><span class="params">(TreeNode root, TreeNode p, TreeNode q)</span> </span>&#123;</span><br><span class="line">     <span class="keyword">int</span> pval = p.val;</span><br><span class="line">     <span class="keyword">int</span> qval = q.val;</span><br><span class="line">     <span class="keyword">int</span> rval = root.val;</span><br><span class="line">     <span class="keyword">if</span>(rval &gt;= pval &amp;&amp; rval &lt;= qval || rval &gt;= qval &amp;&amp; rval &lt;= pval)&#123;</span><br><span class="line">         <span class="keyword">return</span> root;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">if</span>(rval &lt; pval &amp;&amp; rval &lt; qval)&#123;</span><br><span class="line">         <span class="keyword">return</span> lowestCommonAncestor(root.right,p,q);  </span><br><span class="line">     &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         <span class="keyword">return</span> lowestCommonAncestor(root.left,p,q);</span><br><span class="line">         </span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h5 id="1-5-3-二叉树的所有路径"><a href="#1-5-3-二叉树的所有路径" class="headerlink" title="1.5.3 二叉树的所有路径"></a>1.5.3 二叉树的所有路径</h5><p>​        这是<code>Leetcode 257</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">给定一个二叉树，返回所有从根节点到叶子节点的路径。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">说明: 叶子节点是指没有子节点的节点。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">binaryTreePaths</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;String&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">       <span class="keyword">if</span> (root == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> res;</span><br><span class="line">       <span class="comment">// 非递归</span></span><br><span class="line">       LinkedList&lt;TreeNode&gt; stack = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">       stack.push(root);</span><br><span class="line">       LinkedList&lt;String&gt; path = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">       path.push(String.valueOf(root.val));</span><br><span class="line">       TreeNode top;</span><br><span class="line">       String p;</span><br><span class="line">       <span class="keyword">while</span> (!stack.isEmpty()) &#123;</span><br><span class="line">           top = stack.pop();</span><br><span class="line">           p = path.pop();</span><br><span class="line">           <span class="keyword">if</span> (top.left == <span class="keyword">null</span> &amp;&amp; top.right == <span class="keyword">null</span>) &#123;</span><br><span class="line">               res.add(p);</span><br><span class="line">               <span class="keyword">continue</span>;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">if</span> (top.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">               stack.push(top.right);</span><br><span class="line">               path.push(p + <span class="string">"-&gt;"</span> + String.valueOf(top.right.val));</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">if</span> (top.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">               stack.push(top.left);</span><br><span class="line">               path.push(p + <span class="string">"-&gt;"</span> + String.valueOf(top.left.val));</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h5 id="1-5-4-将有序数组转为二叉搜索树"><a href="#1-5-4-将有序数组转为二叉搜索树" class="headerlink" title="1.5.4 将有序数组转为二叉搜索树"></a>1.5.4 将有序数组转为二叉搜索树</h5><p>​        这是<code>Leetcode 108</code>号问题。这道题关键在于要求转为<strong>平衡二叉树</strong>。而二分的思想恰好保证了左右子树高度差不超过1 。因为每次除以2，左右两个数组长度最多相差1。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">将一个按照升序排列的有序数组，转换为一棵高度平衡二叉搜索树。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">本题中，一个高度平衡二叉树是指一个二叉树每个节点的左右两个子树的高度差的绝对值不超过1。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> TreeNode <span class="title">sortedArrayToBST</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(nums == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">return</span> generate(nums, <span class="number">0</span>, nums.length - <span class="number">1</span>);<span class="comment">//注意边界条件</span></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">private</span> TreeNode <span class="title">generate</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span></span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(start &gt; end)</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">int</span> mid = (start + end) / <span class="number">2</span>;</span><br><span class="line">       TreeNode root = <span class="keyword">new</span> TreeNode(arr[mid]);</span><br><span class="line">       root.left = generate(arr, start, mid-<span class="number">1</span>);</span><br><span class="line">       root.right = generate(arr, mid + <span class="number">1</span>, end);</span><br><span class="line">       <span class="keyword">return</span> root;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h5 id="1-5-5-验证二叉搜索树"><a href="#1-5-5-验证二叉搜索树" class="headerlink" title="1.5.5 验证二叉搜索树"></a>1.5.5 验证二叉搜索树</h5><p>​        给定一个二叉树，判断其是否是一个有效的二叉搜索树。 这是<code>Leetcode 98</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//中序遍历的解法，二叉树的中序遍历结果是递增的。</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValidBST</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       LinkedList&lt;TreeNode&gt; stack = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">       TreeNode cur;</span><br><span class="line">       <span class="keyword">double</span> preVal = -Double.MAX_VALUE;<span class="comment">//存储前一个结点值，如果当前访问的val小于preVal（递减了），返回alse</span></span><br><span class="line">       <span class="keyword">while</span> (root != <span class="keyword">null</span> || !stack.isEmpty()) &#123;</span><br><span class="line">           <span class="keyword">while</span> (root != <span class="keyword">null</span>) &#123;</span><br><span class="line">               stack.push(root);</span><br><span class="line">               root = root.left;</span><br><span class="line">           &#125;</span><br><span class="line">           cur = stack.pop();</span><br><span class="line">           <span class="keyword">if</span> (cur.val &lt;= preVal)</span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">           preVal = cur.val;</span><br><span class="line">           root = cur.right;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//递归解法 在深搜过程中不断替换最大值和最小值，通过比较根节点值和最大值、最小值从而判定</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValidBST</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> valid(root,Long.MAX_VAK)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">valid</span><span class="params">(TreeNode root,<span class="keyword">long</span> max, <span class="keyword">long</span> min)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(root.val &gt; max || root.val &lt; min)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//左子树：最大值为根节点值, 右子树:最小值为根节点值</span></span><br><span class="line">    <span class="keyword">return</span> valid(root.left, root.val, min) &amp;&amp; valid(root.right, max, root.val);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="1-5-6-二叉树的最近公共祖先"><a href="#1-5-6-二叉树的最近公共祖先" class="headerlink" title="1.5.6 二叉树的最近公共祖先"></a>1.5.6 二叉树的最近公共祖先</h5><p>​        这是<code>Leetcode 236</code>号问题。</p>
<p>​        注意：题目给定的是<strong>二叉树</strong>，不是二叉搜索树。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> TreeNode <span class="title">lowestCommonAncestor</span><span class="params">(TreeNode root, TreeNode p, TreeNode q)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(root == <span class="keyword">null</span> || p == root || q == root)&#123;</span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        &#125;</span><br><span class="line">        TreeNode left = lowestCommonAncestor(root.left,p,q);</span><br><span class="line">        TreeNode right = lowestCommonAncestor(root.right,p,q);</span><br><span class="line">        <span class="keyword">if</span>(left == <span class="keyword">null</span> &amp;&amp; right == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="comment">//左右子树中并不包含p、q</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(left != <span class="keyword">null</span> &amp;&amp; right != <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="comment">// 左右子树中包含p、q，则root就是其公共祖先</span></span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        &#125;<span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> left == <span class="keyword">null</span> ? right : left;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-完全二叉树"><a href="#2-完全二叉树" class="headerlink" title="2. 完全二叉树"></a>2. 完全二叉树</h3><h4 id="2-1-是否是一个完全二叉树"><a href="#2-1-是否是一个完全二叉树" class="headerlink" title="2.1 是否是一个完全二叉树"></a>2.1 是否是一个完全二叉树</h4><p>​        完全二叉树的简单描述就是结点严格按照从上到下、从左到右以此排列，中间不允许出现空结点。因此，算法可以按照定义来编写，利用层次遍历的结果判定。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isCBT</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (root == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">       Queue&lt;TreeNode&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">       <span class="keyword">boolean</span> leaf = <span class="keyword">false</span>;</span><br><span class="line">       TreeNode L, R, tmp;</span><br><span class="line">       queue.offer(root);</span><br><span class="line">       <span class="keyword">while</span> (!queue.isEmpty()) &#123;</span><br><span class="line">           tmp = queue.poll();</span><br><span class="line">           L = tmp.left;</span><br><span class="line">           R = tmp.right;</span><br><span class="line">           <span class="keyword">if</span> (L == <span class="keyword">null</span> &amp;&amp; R != <span class="keyword">null</span>) &#123;</span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">if</span> (leaf &amp;&amp; (L != <span class="keyword">null</span> || R != <span class="keyword">null</span>)) &#123;</span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="comment">// 只要左右子树不都存在，leaf标志位就设为true</span></span><br><span class="line">           <span class="keyword">if</span> (L != <span class="keyword">null</span>)</span><br><span class="line">               queue.offer(L);</span><br><span class="line">           <span class="keyword">if</span> (R != <span class="keyword">null</span>)</span><br><span class="line">               queue.offer(R);</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">               leaf = <span class="keyword">true</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-2-二叉树的结点个数有多少"><a href="#2-2-二叉树的结点个数有多少" class="headerlink" title="2.2 二叉树的结点个数有多少"></a>2.2 二叉树的结点个数有多少</h4><p>​        这是<code>Leetcode 222</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">countLevel</span><span class="params">(TreeNode p)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (p == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> depth = <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">while</span> (p.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">           depth++;</span><br><span class="line">           p = p.left;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> depth;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">countNodes</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(root == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> ans = <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">int</span> lheight = countLevel(root.left);</span><br><span class="line">       <span class="keyword">int</span> rheight = countLevel(root.right);</span><br><span class="line">       <span class="keyword">if</span>(lheight == rheight)&#123;</span><br><span class="line">           ans += (<span class="number">1</span> &lt;&lt; lheight) - <span class="number">1</span>;<span class="comment">//算出左子树</span></span><br><span class="line">           <span class="comment">//递归遍历右子树</span></span><br><span class="line">           <span class="keyword">return</span> ans + countNodes(root.right);</span><br><span class="line">       &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">           ans += (<span class="number">1</span> &lt;&lt; rheight) - <span class="number">1</span>;</span><br><span class="line">           <span class="keyword">return</span> ans + countNodes(root.left);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-二叉树的性质"><a href="#3-二叉树的性质" class="headerlink" title="3. 二叉树的性质"></a>3. 二叉树的性质</h3><h4 id="3-1-二叉树的深度（高度）"><a href="#3-1-二叉树的深度（高度）" class="headerlink" title="3.1 二叉树的深度（高度）"></a>3.1 二叉树的深度（高度）</h4><p>​        编写一个函数，求二叉树的深度（最大深度）。这是<code>Leetcode 104</code> 号问题。利用递归的思想可以很简单的写出来。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxDepth</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> Math.max(maxDepth(root.left),maxDepth(root.right)) + <span class="number">1</span>;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-2-是否是一个平衡二叉树"><a href="#3-2-是否是一个平衡二叉树" class="headerlink" title="3.2 是否是一个平衡二叉树"></a>3.2 是否是一个平衡二叉树</h4><p>​        平衡二叉树的定义是任意左右子树高度差不超过<code>1</code> 。因此，会用到求<code>二叉树深度</code>这个函数。</p>
<p>​        首先看根节点这棵树左右子树高度差是否超过1，如果是，那么返回<code>false</code>，否则，递归查看左子树、右子树。</p>
<p>​        这是<code>Leetcode 110</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isBalanced</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">if</span>(Math.abs(maxDepth(root.left) - maxDepth(root.right)) &lt;= <span class="number">1</span>)&#123;</span><br><span class="line">           <span class="keyword">return</span> isBalanced(root.left) &amp;&amp; isBalanced(root.right);</span><br><span class="line">       &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-3-你能交换二叉树的两个节点吗"><a href="#3-3-你能交换二叉树的两个节点吗" class="headerlink" title="3.3 你能交换二叉树的两个节点吗"></a>3.3 你能交换二叉树的两个节点吗</h4><p>​        这是<code>Leetcode 226</code> 号问题，反转二叉树。还有一个非常有名的互联网段子。</p>
<p>​        其实，如果已经掌握了二叉树这种递归结构，可以很快的写出代码。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> TreeNode <span class="title">invertTree</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(root == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">       swap(root);</span><br><span class="line">       <span class="keyword">return</span> root;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(TreeNode root)</span></span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(root == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> ;</span><br><span class="line">       TreeNode tmp = root.left;</span><br><span class="line">       root.left = root.right;</span><br><span class="line">       root.right = tmp;</span><br><span class="line">       swap(root.left);</span><br><span class="line">       swap(root.right);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-4-对称二叉树"><a href="#3-4-对称二叉树" class="headerlink" title="3.4 对称二叉树"></a>3.4 对称二叉树</h4><p>​        这是<code>Leetcode 101</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isSymmetric</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(root == <span class="keyword">null</span>)&#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> judge(root.left, root.right);</span><br><span class="line">       </span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">judge</span><span class="params">(TreeNode left, TreeNode right)</span></span>&#123;</span><br><span class="line">       <span class="keyword">if</span>(left == <span class="keyword">null</span> &amp;&amp; right == <span class="keyword">null</span>)&#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">if</span>(left == <span class="keyword">null</span> || right == <span class="keyword">null</span>)</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">       <span class="keyword">if</span>(left.val == right.val)&#123;</span><br><span class="line">           <span class="keyword">return</span> judge(left.left, right.right) &amp;&amp; judge(left.right, right.left);</span><br><span class="line">       &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-堆"><a href="#4-堆" class="headerlink" title="4. 堆"></a>4. 堆</h3><h4 id="4-1-堆的定义"><a href="#4-1-堆的定义" class="headerlink" title="4.1 堆的定义"></a>4.1 堆的定义</h4><p>​        堆有大根堆和小根堆，本质上是一个二叉树，树的根元素始终是最大值或最小值。理解堆，涉及到两个重要操作：<strong>上浮(swift up)</strong>和<strong>下沉(swift down)</strong>。</p>
<h4 id="4-2-构造堆"><a href="#4-2-构造堆" class="headerlink" title="4.2 构造堆"></a>4.2 构造堆</h4><p>​        给定一个数组，要求建立一个大根堆，而且实现维护堆的算法。</p>
<h4 id="4-3-堆排序"><a href="#4-3-堆排序" class="headerlink" title="4.3 堆排序"></a>4.3 堆排序</h4><p>​        堆排序的最坏时间复杂度和平均复杂度都是 $O(nlogn)$，并且不占用额外的空间。</p>
<h4 id="4-4-优先队列"><a href="#4-4-优先队列" class="headerlink" title="4.4 优先队列"></a>4.4 优先队列</h4><p>​        优先队列底层采用了<strong>堆</strong>实现。优先队列可以有效解决<code>topK</code>类问题。</p>
<p>​        采用优先队列的例子，这是<code>Leetcode 451</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    给定一个字符串，请将字符串里的字符按照出现的频率降序排列。</span></span><br><span class="line"><span class="comment">    </span></span><br><span class="line"><span class="comment">    "tree" -&gt; "eert"</span></span><br><span class="line"><span class="comment">    </span></span><br><span class="line"><span class="comment">    "cccaaa" -&gt; "cccaaa"</span></span><br><span class="line"><span class="comment">    </span></span><br><span class="line"><span class="comment">    首先利用哈希表统计词频，然后构造一个优先队列，队首元素是词频最高的。然后以此将队列中元素出队，就得到一个按照词频由高到低排序的字符串。</span></span><br><span class="line"><span class="comment">    其实是对map按照value进行排序。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">frequencySort</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        TreeMap&lt;Character,Integer&gt; map = <span class="keyword">new</span> TreeMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">char</span> ch[] = s.toCharArray();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">char</span> c:ch)&#123;</span><br><span class="line">           map.put(c,map.getOrDefault(c,<span class="number">0</span>) + <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        PriorityQueue&lt;Map.Entry&lt;Character,Integer&gt;&gt; q = <span class="keyword">new</span> PriorityQueue&lt;&gt;((o1,o2)-&gt;&#123;</span><br><span class="line">           <span class="keyword">if</span> (o1.getValue() == o2.getValue())&#123;</span><br><span class="line">               <span class="keyword">return</span> o1.getKey().compareTo(o2.getKey());</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">return</span> o2.getValue() - o1.getValue();</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;Character,Integer&gt; set:map.entrySet())&#123;</span><br><span class="line">            q.offer(set);</span><br><span class="line">        &#125;</span><br><span class="line">        StringBuilder res = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        <span class="keyword">while</span> (!q.isEmpty())&#123;</span><br><span class="line">            Map.Entry&lt;Character,Integer&gt; entry = q.poll();</span><br><span class="line">            <span class="keyword">int</span> times = entry.getValue();</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; times; i++)</span><br><span class="line">                res.append(entry.getKey());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
  </entry>
  <entry>
    <title>二分查找</title>
    <url>/2020/02/05/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/</url>
    <content><![CDATA[<h2 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h2><h3 id="1-写一个正确的二分查找程序"><a href="#1-写一个正确的二分查找程序" class="headerlink" title="1. 写一个正确的二分查找程序"></a>1. 写一个正确的二分查找程序</h3><p>需要注意的是，二分查找有许多注意的地方，比如循环中止条件是什么，<code>left</code> 和 <code>right</code> 初始值是什么。下面以<code>Leetcode 704</code>号问题，写一个基本的二分查找算法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">来源：力扣（LeetCode）</span></span><br><span class="line"><span class="comment">链接：https://leetcode-cn.com/problems/binary-search</span></span><br><span class="line"><span class="comment">著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">search</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = nums.length - <span class="number">1</span>;<span class="comment">// 前闭后闭的取值范围 对应着循环结束时，要取等号。</span></span><br><span class="line">        <span class="keyword">while</span>(left &lt;= right)&#123; <span class="comment">// 此时取值范围里只有一个数字。</span></span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) /<span class="number">2</span>; <span class="comment">// 防止 left + right 越界</span></span><br><span class="line">            <span class="keyword">if</span>(nums[mid] == target)&#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &gt; target)&#123;</span><br><span class="line">                right = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-left的意义是什么"><a href="#2-left的意义是什么" class="headerlink" title="2. left的意义是什么"></a>2. left的意义是什么</h3><p>我们可以看最后要跳出循环时，也就是<code>left = right</code>时，此时<code>mid = left = right</code>,如果数组中存在这个元素，自然返回<code>mid</code>索引，如果不存在呢，回到以下代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(nums[mid] &gt; target)&#123;</span><br><span class="line">    right = mid - <span class="number">1</span>;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    left = mid + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong><code>left</code>的位置恰好就是该元素插入到该数组的位置</strong>。当<code>nums[mid] &gt; target</code>时，如果要将<code>target</code>插入到数组中，要将<code>nums[mid]</code>元素向后移一个位置，空出来的位置就是应该插入的位置，也就是<code>left</code>指向的位置。如果小于呢，<code>left = mid + 1</code>,此时<code>left</code>会向后移一位，恰好又是应该插入的位置。</p>
<p>由此解决<code>Leetcode 35</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">给定一个排序数组和一个目标值，在数组中找到目标值，并返回其索引。如果目标值不存在于数组中，返回它将会被按顺序插入的位置。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">你可以假设数组中无重复元素。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">searchInsert</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//解决target不存在数组中时，返回要插入的位置</span></span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = nums.length - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(left &lt;= right)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] == target)&#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(target &gt; nums[mid])&#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                right = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-其他相关问题"><a href="#3-其他相关问题" class="headerlink" title="3. 其他相关问题"></a>3. 其他相关问题</h3><h4 id="3-1-在排序数组中查找元素的第一个和最后一个位置"><a href="#3-1-在排序数组中查找元素的第一个和最后一个位置" class="headerlink" title="3.1 在排序数组中查找元素的第一个和最后一个位置"></a>3.1 在排序数组中查找元素的第一个和最后一个位置</h4><p>这是<code>Leetcode 34</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">给定一个按照升序排列的整数数组 nums，和一个目标值 target。找出给定目标值在数组中的开始位置和结束位置。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">你的算法时间复杂度必须是 O(log n) 级别。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">如果数组中不存在目标值，返回 [-1, -1]。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">来源：力扣（LeetCode）</span></span><br><span class="line"><span class="comment">链接：https://leetcode-cn.com/problems/find-first-and-last-position-of-element-in-		sorted-array</span></span><br><span class="line"><span class="comment">著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span>  <span class="keyword">int</span>[] searchRange(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target) &#123;</span><br><span class="line">       <span class="keyword">int</span> first = -<span class="number">1</span>, second = -<span class="number">1</span>;</span><br><span class="line">       first = lower_boud(nums,target);</span><br><span class="line">       second = upper_boud(nums,target);</span><br><span class="line">       <span class="keyword">int</span> ans[] = <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;first,second&#125;;</span><br><span class="line">       <span class="keyword">return</span> ans;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">// 以下是辅助函数</span></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">lower_boud</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> target)</span></span>&#123;</span><br><span class="line">       <span class="comment">//利用二分搜索查找target在数组中最左边位置</span></span><br><span class="line">       <span class="keyword">int</span> left = <span class="number">0</span>, right = arr.length - <span class="number">1</span>,res = -<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">while</span> (left &lt;= right)&#123;</span><br><span class="line">           <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">           <span class="keyword">if</span> (target == arr[mid])&#123;</span><br><span class="line">               res = mid;</span><br><span class="line">               <span class="comment">//逼近左边界,继续向左</span></span><br><span class="line">               right = mid - <span class="number">1</span>;</span><br><span class="line">           &#125;<span class="keyword">else</span> <span class="keyword">if</span> (target &lt; arr[mid])&#123;</span><br><span class="line">               right = mid - <span class="number">1</span>;</span><br><span class="line">           &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">               left = mid + <span class="number">1</span>;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">upper_boud</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> target)</span></span>&#123;</span><br><span class="line">       <span class="keyword">int</span> left = <span class="number">0</span>, right = arr.length - <span class="number">1</span>,res = -<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">while</span> (left &lt;= right)&#123;</span><br><span class="line">           <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">           <span class="keyword">if</span> (target == arr[mid])&#123;</span><br><span class="line">               res = mid;</span><br><span class="line">               <span class="comment">//逼近右边界,继续向右</span></span><br><span class="line">               left = mid + <span class="number">1</span>;</span><br><span class="line">           &#125;<span class="keyword">else</span> <span class="keyword">if</span> (target &lt; arr[mid])&#123;</span><br><span class="line">               right = mid - <span class="number">1</span>;</span><br><span class="line">           &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">               left = mid + <span class="number">1</span>;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> res;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-2-第一个错误的版本"><a href="#3-2-第一个错误的版本" class="headerlink" title="3.2 第一个错误的版本"></a>3.2 第一个错误的版本</h4><p>这是<code>Leetcode 278</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">你是产品经理，目前正在带领一个团队开发新的产品。不幸的是，你的产品的最新版本没有通过质量检测。由于每个版本	都是基于之前的版本开发的，所以错误的版本之后的所有版本都是错的。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">假设你有 n 个版本 [1, 2, ..., n]，你想找出导致之后所有版本出错的第一个错误的版本。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">你可以通过调用 bool isBadVersion(version) 接口来判断版本号 version 是否在单元测试中出错。实现一个		函数来查找第一个错误的版本。你应该尽量减少对调用 API 的次数。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">示例:</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">给定 n = 5，并且 version = 4 是第一个错误的版本。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">调用 isBadVersion(3) -&gt; false</span></span><br><span class="line"><span class="comment">调用 isBadVersion(5) -&gt; true</span></span><br><span class="line"><span class="comment">调用 isBadVersion(4) -&gt; true</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">所以，4 是第一个错误的版本。 </span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">来源：力扣（LeetCode）</span></span><br><span class="line"><span class="comment">链接：https://leetcode-cn.com/problems/first-bad-version</span></span><br><span class="line"><span class="comment">著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">firstBadVersion</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> left = <span class="number">1</span>, right = n;</span><br><span class="line">       <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">           <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">           <span class="keyword">if</span> (isBadVersion(mid) == <span class="keyword">true</span>) &#123;<span class="comment">// 已经是错误版本，看看之前的版本有没有错</span></span><br><span class="line">               right = mid - <span class="number">1</span>;</span><br><span class="line">           &#125; <span class="keyword">else</span> &#123;<span class="comment">// 正确版本，看往后的版本有没有错</span></span><br><span class="line">               left = mid + <span class="number">1</span>;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> left;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>还是看一下边界条件，当<code>left = right = mid</code>, 如果<code>isBadVersion(mid) == true</code>成立，表明这就是第一个错误版本，返回<code>left</code>；否则，表明<code>mid</code>是一个正确版本，那么<code>left = mid + 1;</code> <code>left</code> 就是第一个错误的版本。</p>
<h4 id="3-3-寻找峰值"><a href="#3-3-寻找峰值" class="headerlink" title="3.3 寻找峰值"></a>3.3 寻找峰值</h4><p>这是<code>Leetcode 162</code>号问题</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">峰值元素是指其值大于左右相邻值的元素。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">给定一个输入数组 nums，其中 nums[i] ≠ nums[i+1]，找到峰值元素并返回其索引。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">数组可能包含多个峰值，在这种情况下，返回任何一个峰值所在位置即可。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">你可以假设 nums[-1] = nums[n] = -∞。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">来源：力扣（LeetCode）</span></span><br><span class="line"><span class="comment">链接：https://leetcode-cn.com/problems/find-peak-element</span></span><br><span class="line"><span class="comment">著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findPeakElement</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.length == <span class="number">1</span>) &#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">int</span> left = <span class="number">0</span>, right = nums.length -<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">           <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">           <span class="keyword">if</span> (nums[mid] &gt; nums[mid + <span class="number">1</span>]) &#123;</span><br><span class="line">               right = mid;</span><br><span class="line">           &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">               left = mid + <span class="number">1</span>;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> left;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>类似的还有<code>Leetcode 852</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">我们把符合下列属性的数组 A 称作山脉：</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">A.length &gt;= 3</span></span><br><span class="line"><span class="comment">存在 0 &lt; i &lt; A.length - 1 使得A[0] &lt; A[1] &lt; ... A[i-1] &lt; A[i] &gt; A[i+1] &gt; ... &gt; A[A.length - 1]</span></span><br><span class="line"><span class="comment">给定一个确定为山脉的数组，返回任何满足 A[0] &lt; A[1] &lt; ... A[i-1] &lt; A[i] &gt; A[i+1] &gt; ... &gt; A[A.length - 1] 的 i 的值。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">来源：力扣（LeetCode）</span></span><br><span class="line"><span class="comment">链接：https://leetcode-cn.com/problems/peak-index-in-a-mountain-array</span></span><br><span class="line"><span class="comment">著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">peakIndexInMountainArray</span><span class="params">(<span class="keyword">int</span>[] A)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = A.length - <span class="number">1</span>; </span><br><span class="line">		 <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">	            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">	            <span class="keyword">int</span> v = A[mid];</span><br><span class="line">	            <span class="comment">// 按照定义，peak元素后至少有一个比他小的元素，因此不必担心越界</span></span><br><span class="line">             	<span class="keyword">if</span> (v &lt; A[mid + <span class="number">1</span>]) &#123;</span><br><span class="line">	                left = mid + <span class="number">1</span>;</span><br><span class="line">	            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">	                right = mid - <span class="number">1</span>;</span><br><span class="line">	            &#125;</span><br><span class="line">	        &#125;</span><br><span class="line">	    <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：以上两道题在边界处理方面有所不同，应当注意取分。</p>
<p>考虑<code>852</code>号问题边界情况，最后一次循环已经到了“峰顶”元素，且此时<code>left = right = mid</code>，由于<code>A[mid]</code>为“峰顶”元素，且其后面至少有一个元素，那么<code>A[mid] &gt; A[mid+1]</code>（<code>mid+1</code>没有越界），因此执行<code>right = mid - 1</code>，又<code>right &lt; left</code>，跳出循环，返回<code>left</code>。</p>
<p><code>Leetcode 1095</code>号问题。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * // This is MountainArray's API interface.</span></span><br><span class="line"><span class="comment"> * // You should not implement it, or speculate about its implementation</span></span><br><span class="line"><span class="comment"> * interface MountainArray &#123;</span></span><br><span class="line"><span class="comment"> *     public int get(int index) &#123;&#125;</span></span><br><span class="line"><span class="comment"> *     public int length() &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findInMountainArray</span><span class="params">(<span class="keyword">int</span> target, MountainArray mountainArr)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 最小的index</span></span><br><span class="line">        <span class="comment">// [1,2,3,4,5,3,1]</span></span><br><span class="line">        <span class="keyword">int</span> len = mountainArr.length();</span><br><span class="line">        <span class="keyword">int</span> peak = findPeak(<span class="number">0</span>, len - <span class="number">1</span>, mountainArr);</span><br><span class="line">        <span class="keyword">int</span> l = searchLeft(<span class="number">0</span>, peak, target, mountainArr);</span><br><span class="line">        <span class="keyword">return</span> l == -<span class="number">1</span> ? searchRight(peak, len - <span class="number">1</span>, target, mountainArr) : l;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 找到山峰值</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">findPeak</span><span class="params">(<span class="keyword">int</span> left, <span class="keyword">int</span> right, MountainArray mountainArray)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">int</span> v = mountainArray.get(mid);</span><br><span class="line">            <span class="comment">// 由于题中定义peak后面会有一个元素，因此不会越界</span></span><br><span class="line">            <span class="keyword">if</span> (v &lt; mountainArray.get(mid + <span class="number">1</span>)) &#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                right = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 左边找 [0, peak]严格升序数组</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">searchLeft</span><span class="params">(<span class="keyword">int</span> left, <span class="keyword">int</span> right, <span class="keyword">int</span> target, MountainArray mountainArray)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">int</span> v = mountainArray.get(mid);</span><br><span class="line">            <span class="keyword">if</span> (v == target) &#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (v &gt; target) &#123;</span><br><span class="line">                right = mid - <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// [peak,len-1]右边是严格降序数组</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">searchRight</span><span class="params">(<span class="keyword">int</span> left, <span class="keyword">int</span> right, <span class="keyword">int</span> target, MountainArray mountainArray)</span>	</span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">int</span> v = mountainArray.get(mid);</span><br><span class="line">            <span class="keyword">if</span> (v == target) &#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (v &gt; target) &#123;</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                right = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-4-Pie"><a href="#3-4-Pie" class="headerlink" title="3.4 Pie"></a>3.4 Pie</h4><p>杭电oj  <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1969" target="_blank">1969</a></p>
<p>题目大意为：</p>
<p>给定$n$个不同半径的派（高度均为1，口味互不相同），有$f + 1$个人，求一个最大体积$V$，使得这$n$个派可以分为$f+1$份（不可以多个不同口味的派拼起来凑成$V$）。可以发现最大体积一定不会大于最大派的体积$Max$，因此在区间$[0,Max]$中进行二分搜索就可以。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> pi = <span class="built_in">acos</span>(<span class="number">-1.0</span>);</span><br><span class="line"><span class="keyword">int</span> t, f, n, r[<span class="number">10005</span>], MAX;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">check</span><span class="params">(<span class="keyword">double</span> v)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">        <span class="keyword">double</span> tmp = r[i] * r[i] * pi;</span><br><span class="line">        cnt += <span class="built_in">floor</span>(tmp / v);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(cnt &gt;= (f + <span class="number">1</span>))&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; t;</span><br><span class="line">    <span class="keyword">while</span>(t--)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; n &gt;&gt; f;</span><br><span class="line">        MAX = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; r[i];</span><br><span class="line">            MAX = <span class="built_in">max</span>(MAX, r[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">double</span> left = <span class="number">0.0</span>, right = MAX * MAX * pi, mid;</span><br><span class="line">        <span class="comment">// 在0 - 最大面积中选一个值使得所有pie能够分成 f+1份</span></span><br><span class="line">        <span class="keyword">while</span>(right - left &gt;= <span class="number">1e-7</span>)&#123;</span><br><span class="line">            mid = (left + right) / <span class="number">2.0</span>;</span><br><span class="line">            <span class="comment">// check mid，是否可以分成 f+1</span></span><br><span class="line">            <span class="keyword">if</span>(check(mid) == <span class="number">1</span>)&#123;<span class="comment">// 还可以继续增大搜索</span></span><br><span class="line">                left = mid + <span class="number">1e-6</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123; <span class="comment">// 不够分了，要减小</span></span><br><span class="line">                right = mid - <span class="number">1e-6</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="comment">// 最后跳出循环时left = right</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%.4lf\n"</span>, left);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-利用二分法求解方程"><a href="#4-利用二分法求解方程" class="headerlink" title="4. 利用二分法求解方程"></a>4. 利用二分法求解方程</h3><p>杭电oj <a href="http://acm.hdu.edu.cn/showproblem.php?pid=2199" target="_blank">2199</a></p>
<p>说的是给定函数$8x^4 + 7x^3 + 2x^2 + 3x + 6 == Y$，针对输入的不同的$Y$，求出$x$在区间$[0,100]$上的解，要求最终$x$精确到小数点后4位。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> t;</span><br><span class="line"><span class="keyword">double</span> y;</span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">f</span><span class="params">(<span class="keyword">double</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">8.0</span> * <span class="built_in">pow</span>(x,<span class="number">4.0</span>) + <span class="number">7.0</span> * <span class="built_in">pow</span>(x, <span class="number">3.0</span>) + <span class="number">2.0</span> * <span class="built_in">pow</span>(x, <span class="number">2.0</span>) + <span class="number">3.0</span> * x + <span class="number">6.0</span>;;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; t;</span><br><span class="line">    <span class="keyword">while</span>(t--)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; y;</span><br><span class="line">        <span class="comment">// 函数在[0,100]上是单调递增的</span></span><br><span class="line">        <span class="keyword">if</span>(y &lt; f(<span class="number">0</span>) || y &gt; f(<span class="number">100</span>))&#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"No solution!\n"</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">// 二分查找</span></span><br><span class="line">            <span class="keyword">double</span> left = <span class="number">0.0</span>, right = <span class="number">100.0</span>, mid;</span><br><span class="line">            <span class="keyword">while</span>(right - left &gt;= <span class="number">1e-6</span>)&#123;</span><br><span class="line">                mid = (left + right) / <span class="number">2.0</span>;</span><br><span class="line">                <span class="keyword">double</span> tmp = f(mid);</span><br><span class="line">                <span class="keyword">if</span>(tmp &gt; y)&#123;</span><br><span class="line">                    right = mid - <span class="number">1e-7</span>;</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    left = mid + <span class="number">1e-7</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 最终left right趋于一个值</span></span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%.4lf\n"</span>, (left + right) / <span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>杭电oj <a href="http://acm.hdu.edu.cn/showproblem.php?pid=2899" target="_blank">2899</a></p>
<p>给定函数$f(x) = 6x^7 + 8x^6 + 7x^3 + 5x^2 -yx,x\in[0,100]$，对于给定的不同的$y$值，求出$f(x)$在$[0,100]$上的最小值（精确到小数点后4位）。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> t;</span><br><span class="line"><span class="keyword">double</span> y;</span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">f</span><span class="params">(<span class="keyword">double</span> x, <span class="keyword">double</span> y)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">6.0</span> * <span class="built_in">pow</span>(x, <span class="number">7</span>) + <span class="number">8.0</span> * <span class="built_in">pow</span>(x, <span class="number">6</span>) + <span class="number">7.0</span> * <span class="built_in">pow</span>(x, <span class="number">3</span>)</span><br><span class="line">            + <span class="number">5.0</span> * <span class="built_in">pow</span>(x, <span class="number">2</span>) - y * x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">df</span><span class="params">(<span class="keyword">double</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">42.0</span> * <span class="built_in">pow</span>(x, <span class="number">6</span>) + <span class="number">48.0</span> * <span class="built_in">pow</span>(x, <span class="number">5</span>) + <span class="number">21.0</span> * <span class="built_in">pow</span>(x, <span class="number">2</span>) + <span class="number">10.0</span> * x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; t;</span><br><span class="line">    <span class="keyword">while</span>(t--)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; y;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">        针对给定的y，求出df中使得df(x) = y 的x值，判定x是否在[0,100]中，若在</span></span><br><span class="line"><span class="comment">        则x就是极小值点，带入f中求得极小值</span></span><br><span class="line"><span class="comment">        若不在, 在f(0)与f(100)中取最小值</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">double</span> left = <span class="number">0.0</span>, right = <span class="number">100.0</span>, mid, ans, x_star;</span><br><span class="line">        <span class="keyword">if</span>(y &lt; df(<span class="number">0</span>) || y &gt; df(<span class="number">100</span>))&#123;<span class="comment">//x_star 不在[0,100]之间</span></span><br><span class="line">            ans = <span class="built_in">min</span>(f(<span class="number">0.0</span>,y),f(<span class="number">100.0</span>,y));</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">while</span>(right - left &gt; <span class="number">1e-7</span>)&#123;</span><br><span class="line">                mid = (left + right) / <span class="number">2.0</span>;</span><br><span class="line">                <span class="keyword">if</span>(df(mid) &gt; y)&#123;</span><br><span class="line">                    right = mid - <span class="number">1e-6</span>;</span><br><span class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span>(df(mid) &lt; y)&#123;</span><br><span class="line">                    left = mid + <span class="number">1e-6</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            x_star = (left + right) / <span class="number">2.0</span>;</span><br><span class="line">            ans = f(x_star, y);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%.4lf\n"</span>,ans);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5-三分法"><a href="#5-三分法" class="headerlink" title="5. 三分法"></a>5. 三分法</h3><p>二分法能够有效解决有序序列的问题，然而当一个序列先增大后减小（或者先减小后增大）即存在一个峰值时，就要用到三分的思想了。三分和二分有点像，三分是在left和right指针区间内，设置$mid1$、$mid2$两个指针，即$mid1$与$mid2$将$[left，right]$区间三等分。之后根据具体问题更新left和right指针位置，即可能：$left = mid1$或者$right = mid2$。</p>
<ul>
<li>例子：求解函数$f(x) = x^2 -4x + 3,x \in [0,5]$上的极小值。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">f</span><span class="params">(<span class="keyword">double</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x * x - <span class="number">4</span> * x + <span class="number">3</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">double</span> left = <span class="number">0.0</span>, right = <span class="number">5.0</span>, mid1, mid2;</span><br><span class="line">    <span class="keyword">while</span>(right - left &gt; <span class="number">1e-8</span>)&#123;</span><br><span class="line">        mid1 = left + (right - left) / <span class="number">3.0</span>;</span><br><span class="line">        mid2 = right - (right - left) / <span class="number">3.0</span>;</span><br><span class="line">        <span class="comment">// 求[0,5]区间f的极小值</span></span><br><span class="line">        <span class="keyword">if</span>(f(mid1) &gt; f(mid2))&#123; <span class="comment">// mid2对应的点更小</span></span><br><span class="line">            left = mid1;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            right = mid2;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">double</span> min_value = f(left);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%.6lf\n"</span>,min_value);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%.6lf\n"</span>,left);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
  </entry>
  <entry>
    <title>JVM 垃圾回收器分类总结</title>
    <url>/2020/02/04/JVM%20%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%E5%88%86%E7%B1%BB%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="JVM-垃圾回收器分类总结"><a href="#JVM-垃圾回收器分类总结" class="headerlink" title="JVM 垃圾回收器分类总结"></a>JVM 垃圾回收器分类总结</h2><p>概览：</p>
<p>由于垃圾回收存在多种算法：标记-复制、标记-清除、标记-整理。组合多种垃圾回收器，形成分代收集算法。</p>
<p><strong>新生代收集器</strong>：Serial、ParNew、Parallel Scavenge</p>
<p><strong>老年代收集器</strong>：CMS、Serial Old、Parallel Old</p>
<p><strong>整堆收集器</strong>： G1</p>
<h3 id="1-Serial-垃圾回收器"><a href="#1-Serial-垃圾回收器" class="headerlink" title="1. Serial 垃圾回收器"></a>1. Serial 垃圾回收器</h3><p> 串行垃圾回收器，单线程，执行垃圾回收时其他线程停止等待。</p>
<h3 id="2-Parallel-New-垃圾回收器"><a href="#2-Parallel-New-垃圾回收器" class="headerlink" title="2. Parallel New 垃圾回收器"></a>2. Parallel New 垃圾回收器</h3><p> 并行垃圾回收器。相较于 Serial，多线程并行进行垃圾回收。其他线程也必须停止等待。</p>
<h3 id="3-Parallel-Scavenge-收集器"><a href="#3-Parallel-Scavenge-收集器" class="headerlink" title="3. Parallel Scavenge 收集器"></a>3. Parallel Scavenge 收集器</h3><p> 与<strong>吞吐量</strong>关系密切，故也称为吞吐量优先收集器。</p>
<p> <strong>特点</strong>：属于新生代收集器也是采用复制算法的收集器，又是并行的多线程收集器（与 ParNew 收集器类似）。</p>
<h3 id="4-Parallel-Old-收集器"><a href="#4-Parallel-Old-收集器" class="headerlink" title="4. Parallel Old 收集器"></a>4. Parallel Old 收集器</h3><p> 是 Parallel Scavenge 收集器的老年代版本。</p>
<p> <strong>特点</strong>：多线程，采用标记-整理算法。</p>
<p> <strong>应用场景</strong>：注重高吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge+Parallel Old 收集器。</p>
<h3 id="5-Serial-Old-收集器"><a href="#5-Serial-Old-收集器" class="headerlink" title="5. Serial Old 收集器"></a>5. Serial Old 收集器</h3><p> Serial Old 是 Serial 收集器的老年代版本。</p>
<p> <strong>特点</strong>：同样是单线程收集器，采用标记-整理算法。</p>
<h3 id="6-CMS-垃圾回收器"><a href="#6-CMS-垃圾回收器" class="headerlink" title="6. CMS 垃圾回收器"></a>6. CMS 垃圾回收器</h3><p> 并发垃圾回收器，在进行垃圾回收时，其余线程不必停止等待。</p>
<p> <strong>CMS 收集器的运行过程分为下列 4 步：</strong></p>
<p> <strong>初始标记</strong>：标记 GC Roots 能直接到的对象。速度很快但是仍存在 Stop The World 问题。</p>
<p> <strong>并发标记</strong>：进行 GC Roots Tracing 的过程，找出存活对象且用户线程可并发执行。</p>
<p> <strong>重新标记</strong>：为了修正并发标记期间因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录。仍然存在 Stop The World 问题。</p>
<p> <strong>并发清除</strong>：对标记的对象进行清除回收。</p>
<p> CMS 收集器的内存回收过程是与用户线程一起并发执行的。</p>
<h3 id="7-G1-垃圾回收器"><a href="#7-G1-垃圾回收器" class="headerlink" title="7. G1 垃圾回收器"></a>7. G1 垃圾回收器</h3><p>一款面向服务端应用的垃圾收集器。它把内存划分为多个不同的子区域，G1只存在逻辑上的分区，子区域可能随G1的运行在不同代期间前后切换，每个子区域大小为1-32<code>M</code>不等，最多设置<code>2048</code>个区域。</p>
<p><strong>特点如下：</strong></p>
<p>并行与并发：G1 能充分利用多 CPU、多核环境下的硬件优势，使用多个 CPU 来缩短 Stop-The-World 停顿时间。部分收集器原本需要停顿 Java 线程来执行 GC 动作，G1 收集器仍然可以通过并发的方式让 Java 程序继续运行。</p>
<p>分代收集：G1 能够独自管理整个 Java 堆，并且采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次 GC 的旧对象以获取更好的收集效果。</p>
<p>空间整合：G1 运作期间不会产生空间碎片，收集后能提供规整的可用内存。</p>
<p>可预测的停顿：G1 除了追求低停顿外，还能建立可预测的停顿时间模型。能让使用者明确指定在一个长度为 M 毫秒的时间段内，消耗在垃圾收集上的时间不得超过 N 毫秒。</p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>造成Java Out Of Memory原因总结</title>
    <url>/2020/02/04/%E9%80%A0%E6%88%90Java%20Out%20Of%20Memory%E5%8E%9F%E5%9B%A0%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="造成-Java-Out-Of-Memory-原因总结"><a href="#造成-Java-Out-Of-Memory-原因总结" class="headerlink" title="造成 Java Out Of Memory 原因总结"></a>造成 Java Out Of Memory 原因总结</h2><p>总结起来有 6 点原因，以下分点阐述。</p>
<h3 id="1-heap-space"><a href="#1-heap-space" class="headerlink" title="1 heap space"></a>1 heap space</h3><p> 当申请的空间大小超出了堆中剩余空间就会产生这个原因。比如：通过 <strong>_-Xmx10m_</strong> 设置虚拟机堆最大空间为 10 M，运行代码：<code>byte data[] = new data[1024 * 1024 * 20];</code> 就会产生错误。</p>
<h3 id="2-GC-overhead-limit"><a href="#2-GC-overhead-limit" class="headerlink" title="2 GC overhead limit"></a>2 GC overhead limit</h3><p> 当 Java 虚拟机连续多次垃圾回收却只回收了不到$2\%$的内存，也就是说你的程序有$98\%$的时间都在进行垃圾回收，却回收了不到$2\%$的内存，就会产生这个错误。</p>
<h3 id="3-Direct-buffer-memory"><a href="#3-Direct-buffer-memory" class="headerlink" title="3 Direct buffer memory"></a>3 Direct buffer memory</h3><p> 由于在 Java 8 以后，hotspot 虚拟机废止了永久代，转而采用<i>MetaSpace</i>实现方法区。Metaspace 并不依赖于 JVM 内存，而直接占用本地内存。因此，当在 NIO 过程中，经常使用 ByteBuffer 来读取和写入数据，这是一种基于 Channel、Buffer 的 I/O 方式。它可以使用 native 函数库直接分配 OS 本地内存，然后通过一个存储在堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。因为避免了 Java 堆和 Native 堆中来回复制数据（不需要内存拷贝），可以提高性能。Buffer 空间直接在本地分配内存，不属于 GC 管辖范围，JVM 不要执行 GC。当本地内存(Direct Memory)用完，而堆内存反而很少使用，就会产生这个错误。</p>
<p>以下代码设置元空间占用本地最大内存为 5 m，却要直接申请 50 m 的内存，会报错。</p>
<p><code>-XX:+PrintGCDetails -XX:MaxDirectMemorySize=5m</code></p>
<p><code>ByteBuffer buffer = ByteBuffer.allocateDirect(50 * 1024 * 1024);</code></p>
<h3 id="4-Metaspace"><a href="#4-Metaspace" class="headerlink" title="4 Metaspace"></a>4 Metaspace</h3><p> Java 8 后，采用元空间实现方法区。方法区主要存放加载的类信息、静态变量/方法、运行时常量池等信息。当加载的类过多以致超出了元空间的大小，就会产生这个错误。</p>
<p><strong>注意</strong>：</p>
<p>方法区是Java虚拟机的一个规范，在Java 8之前，持久代作为方法区的实现，Java 8及以后，采用元空间(Meta Space)作为方法区的实现，并且元空间使用的是本地内存。</p>
<p><code>-XX:MetaspaceSize=8m -XX:MaxMetaspaceSize=8m</code> 设置初始和最大元空间大小。</p>
<h3 id="5-unable-to-create-new-native-thread"><a href="#5-unable-to-create-new-native-thread" class="headerlink" title="5 unable to create new native thread"></a>5 unable to create new native thread</h3><p>高并发请求服务器时，经常出现：</p>
<p><code>java.lang.OutOfMemoryError: unable to create new native thread</code></p>
<p>不同操作系统允许进程创建的线程数量有限，当 Java 应用创建太多线程时，超过了系统承载极限。linux 默认徐云一个进程创建 1024 个线程。</p>
<p><strong>tips</strong>:当一个线程 start 后，线程状态会发生改变，不允许再次 start。</p>
<h3 id="6-Stack-Overflow"><a href="#6-Stack-Overflow" class="headerlink" title="6 Stack Overflow"></a>6 Stack Overflow</h3><p> 最经典的一个错误，无限递归导致虚拟机栈空间不足，就会报这个错误。</p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>Faster R-CNN中anchor的理解</title>
    <url>/2020/02/04/Faster%20R-CNN%E4%B8%ADanchor%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="Faster-R-CNN-中-anchor-的理解"><a href="#Faster-R-CNN-中-anchor-的理解" class="headerlink" title="Faster R-CNN 中 anchor 的理解"></a>Faster R-CNN 中 anchor 的理解</h2><h3 id="1-什么是-anchor"><a href="#1-什么是-anchor" class="headerlink" title="1 什么是 anchor"></a>1 什么是 anchor</h3><p>​ 当图像经过 CNN 网络（如 VGG16）提取出特征即 <strong>feature map</strong> 之后，针对 <strong>feature map</strong> 进行 Region Proposal 操作。首先要明白的是 anchor 是相对于 feature map 在原图上的概念。提取出来的特征相对于原图像存在视野域这个概念，如果视野比例为 8，即 feature map 上一个点，对应在原图是一个 8 * 8 的正方形。那么，该正方形的中心就被称为 anchor。</p>
<h3 id="2-anchor-boxes"><a href="#2-anchor-boxes" class="headerlink" title="2 anchor boxes"></a>2 anchor boxes</h3><p>​ 在确定 anchor 后，针对改 anchor 要选定 9 个框，首先有 anchor ratio 这个概念。即 <strong>1:1, 1:2, 2:1</strong> 。也就是说 9 个 anchor boxes 的边长比例满足 ratio，确切的说，anchor boxes 存在 3 组，每组 3 个，每个 boxes 的边长都是 1:1,1:2,2:1 。 还有 scales 这个概念， 即 <strong>8, 16, 32</strong> 。anchor boxes 中，1:1 这组的边长<strong>等于</strong>视野域比例*scale。</p>
<p>如果视野域比例为<strong>16</strong>，那么边长分别为<strong>128,256,512</strong>。其余比例为 2 的边，按照 128/256/512 的 1.5 倍进行扩大。</p>
<p>如 512*1.5 = 768，768 /2 = 384，256*1.5 = 384，384/2 = 192，128*1.5 = 192, 192 /2= 96</p>
<p><img src="/images/faster-rcnn.jpg" alt="anchor示意图"></p>
<h3 id="3-后续操作"><a href="#3-后续操作" class="headerlink" title="3 后续操作"></a>3 后续操作</h3><p>​ 假设 feature map 大小为 M*N，那么 anchor boxes 数量为 <strong>9MN*</strong> 。对于这些 boxes 在进行前景背景分类以及 ground truth 回归等操作，最终选定目标物体。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>HashMap LoadFactor</title>
    <url>/2019/11/09/Hashap-LoadFactor/</url>
    <content><![CDATA[<p>The Load factor is a measure that decides when to increase the HashMap capacity to maintain the get() and put() operation complexity of O(1). The default load factor of HashMap is <strong>0.75f</strong> (75% of the map size).</p>
<p>The initial capacity of the HashMap is the number of buckets in the hash table. It creates when we create the object of HashMap class. The initial capacity of the HashMap is 24, i.e.,16. The capacity of the HashMap is doubled each time it reaches the threshold. The capacity is increased to 25=32, 26=64, and so on.</p>
<p>One way to calculate size:<br>When the load factor ratio (m/n) reaches 0.75 at that time, hashmap increases its capacity.<br>Where,<br>m is the number of entries in a hashmap.(size of HashMap)<br>n is the total size of hashmap.</p>
]]></content>
  </entry>
  <entry>
    <title>Canny 边缘检测</title>
    <url>/2019/11/07/canny/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_kernel</span><span class="params">(size, sigma=<span class="number">1</span>)</span>:</span></span><br><span class="line">    size = int(size) // <span class="number">2</span></span><br><span class="line">    x, y = np.mgrid[-size:size+<span class="number">1</span>, -size:size+<span class="number">1</span>]</span><br><span class="line">    normal = <span class="number">1</span> / (<span class="number">2.0</span> * np.pi * sigma**<span class="number">2</span>)</span><br><span class="line">    g =  np.exp(-((x**<span class="number">2</span> + y**<span class="number">2</span>) / (<span class="number">2.0</span>*sigma**<span class="number">2</span>))) * normal</span><br><span class="line">    <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sobel_filters</span><span class="params">(img)</span>:</span></span><br><span class="line">    Kx = np.array([[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">-2</span>, <span class="number">0</span>, <span class="number">2</span>], [<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>]], np.float32)</span><br><span class="line">    Ky = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">-1</span>, <span class="number">-2</span>, <span class="number">-1</span>]], np.float32)</span><br><span class="line">    </span><br><span class="line">    Ix = ndimage.filters.convolve(img, Kx)</span><br><span class="line">    Iy = ndimage.filters.convolve(img, Ky)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#print(Ix[0:100,0:100])</span></span><br><span class="line">    </span><br><span class="line">    G = np.hypot(Ix, Iy)</span><br><span class="line">    G = G / G.max() * <span class="number">255</span></span><br><span class="line">    theta = np.arctan2(Iy, Ix)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (G, theta)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">non_max_suppression</span><span class="params">(img, D)</span>:</span></span><br><span class="line">    M, N = img.shape</span><br><span class="line">    Z = np.zeros((M,N), dtype=np.int32)</span><br><span class="line">    angle = D * <span class="number">180.</span> / np.pi</span><br><span class="line">    angle[angle &lt; <span class="number">0</span>] += <span class="number">180</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,M<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,N<span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                q = <span class="number">255</span></span><br><span class="line">                r = <span class="number">255</span></span><br><span class="line">                </span><br><span class="line">               <span class="comment">#angle 0</span></span><br><span class="line">                <span class="keyword">if</span> (<span class="number">0</span> &lt;= angle[i,j] &lt; <span class="number">22.5</span>) <span class="keyword">or</span> (<span class="number">157.5</span> &lt;= angle[i,j] &lt;= <span class="number">180</span>):</span><br><span class="line">                    q = img[i, j+<span class="number">1</span>]</span><br><span class="line">                    r = img[i, j<span class="number">-1</span>]</span><br><span class="line">                <span class="comment">#angle 45</span></span><br><span class="line">                <span class="keyword">elif</span> (<span class="number">22.5</span> &lt;= angle[i,j] &lt; <span class="number">67.5</span>):</span><br><span class="line">                    q = img[i+<span class="number">1</span>, j<span class="number">-1</span>]</span><br><span class="line">                    r = img[i<span class="number">-1</span>, j+<span class="number">1</span>]</span><br><span class="line">                <span class="comment">#angle 90</span></span><br><span class="line">                <span class="keyword">elif</span> (<span class="number">67.5</span> &lt;= angle[i,j] &lt; <span class="number">112.5</span>):</span><br><span class="line">                    q = img[i+<span class="number">1</span>, j]</span><br><span class="line">                    r = img[i<span class="number">-1</span>, j]</span><br><span class="line">                <span class="comment">#angle 135</span></span><br><span class="line">                <span class="keyword">elif</span> (<span class="number">112.5</span> &lt;= angle[i,j] &lt; <span class="number">157.5</span>):</span><br><span class="line">                    q = img[i<span class="number">-1</span>, j<span class="number">-1</span>]</span><br><span class="line">                    r = img[i+<span class="number">1</span>, j+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (img[i,j] &gt;= q) <span class="keyword">and</span> (img[i,j] &gt;= r):</span><br><span class="line">                    Z[i,j] = img[i,j]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    Z[i,j] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">except</span> IndexError <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">threshold</span><span class="params">(img, lowThresholdRatio=<span class="number">0.05</span>, highThresholdRatio=<span class="number">0.09</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    highThreshold = img.max() * highThresholdRatio;</span><br><span class="line">    lowThreshold = highThreshold * lowThresholdRatio;</span><br><span class="line">    </span><br><span class="line">    M, N = img.shape</span><br><span class="line">    res = np.zeros((M,N), dtype=np.int32)</span><br><span class="line">    </span><br><span class="line">    weak = np.int32(<span class="number">25</span>)</span><br><span class="line">    strong = np.int32(<span class="number">255</span>)</span><br><span class="line">    </span><br><span class="line">    strong_i, strong_j = np.where(img &gt;= highThreshold)</span><br><span class="line">    zeros_i, zeros_j = np.where(img &lt; lowThreshold)</span><br><span class="line">    </span><br><span class="line">    weak_i, weak_j = np.where((img &lt;= highThreshold) &amp; (img &gt;= lowThreshold))</span><br><span class="line">    </span><br><span class="line">    res[strong_i, strong_j] = strong</span><br><span class="line">    res[weak_i, weak_j] = weak</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (res, weak, strong)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hysteresis</span><span class="params">(img, weak, strong=<span class="number">255</span>)</span>:</span></span><br><span class="line">    M, N = img.shape  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, M<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, N<span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> (img[i,j] == weak):</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="keyword">if</span> ((img[i+<span class="number">1</span>, j<span class="number">-1</span>] == strong) <span class="keyword">or</span> (img[i+<span class="number">1</span>, j] == strong) <span class="keyword">or</span> (img[i+<span class="number">1</span>, j+<span class="number">1</span>] == strong)</span><br><span class="line">                        <span class="keyword">or</span> (img[i, j<span class="number">-1</span>] == strong) <span class="keyword">or</span> (img[i, j+<span class="number">1</span>] == strong)</span><br><span class="line">                        <span class="keyword">or</span> (img[i<span class="number">-1</span>, j<span class="number">-1</span>] == strong) <span class="keyword">or</span> (img[i<span class="number">-1</span>, j] == strong) <span class="keyword">or</span> (img[i<span class="number">-1</span>, j+<span class="number">1</span>] == strong)):</span><br><span class="line">                        img[i, j] = strong</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        img[i, j] = <span class="number">0</span></span><br><span class="line">                <span class="keyword">except</span> IndexError <span class="keyword">as</span> e:</span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器视觉基础</category>
      </categories>
  </entry>
</search>
