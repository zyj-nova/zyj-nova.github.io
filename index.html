<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="description" content="天空如此辽阔，大地不过是必经之路" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     张永剑的博客
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<link rel="alternate" href="/atom.xml" title="张永剑的博客" type="application/atom+xml">
</head>

</html>

<body>
  <div id="app">
    <main class="content">
      
<div id="main">
  <section class="outer">
  <article class="articles">
    
    
    
    
    <article id="post-多元高斯及其极大似然估计" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/06/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%8F%8A%E5%85%B6%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"
    >多元高斯及其极大似然估计</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/10/06/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%8F%8A%E5%85%B6%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/" class="article-date">
  <time datetime="2020-10-06T07:54:18.217Z" itemprop="datePublished">2020-10-06</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="多元高斯及其极大似然估计"><a href="#多元高斯及其极大似然估计" class="headerlink" title="多元高斯及其极大似然估计"></a>多元高斯及其极大似然估计</h2><p>参考：<a href="https://blog.csdn.net/Joyliness/article/details/80097491" target="_blank" rel="noopener">https://blog.csdn.net/Joyliness/article/details/80097491</a></p>
<h3 id="1-独立多元高斯的概率密度函数"><a href="#1-独立多元高斯的概率密度函数" class="headerlink" title="1. 独立多元高斯的概率密度函数"></a>1. 独立多元高斯的概率密度函数</h3><p>$n$个独立的随机变量$(x_1,x_2,…x_n)$的联合密度函数为</p>
<script type="math/tex; mode=display">
f_{\mu,\Sigma}(\pmb x)=\frac{1}{\sqrt{(2\pi)^n\det\Sigma}}e^{-\frac{1}{2}(\pmb x - \pmb \mu)^T\Sigma^{-1}(\pmb x - \pmb \mu)}</script><p>就是多元高斯分布（Multivariate Normal Distribution，MVN）。其中，$\pmb \mu$是各个随机变量的期望值所组成的向量，$\Sigma$是随机变量间的协方差矩阵，是$n \times n$的正定矩阵。</p>
<h3 id="2-多元高斯的极大似然估计"><a href="#2-多元高斯的极大似然估计" class="headerlink" title="2. 多元高斯的极大似然估计"></a>2. 多元高斯的极大似然估计</h3><p>对于$N$个满足多元高斯分布的独立样本集：$\{\pmb x_1,\pmb x_2,..,\pmb x_N\},\pmb x \in \R^n$，似然函数为：</p>
<script type="math/tex; mode=display">
\prod_{i=1}^Nf_{\mu,\Sigma}(\pmb x_i) = (2\pi)^{-\frac{Nn}{2}}|\Sigma|^{-\frac{N}{2}}e^{-\frac{1}{2}\sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)}</script><p>取对数：</p>
<script type="math/tex; mode=display">
\ln L(\pmb \mu,\Sigma) =\ln\prod_{i=1}^Nf_{\mu,\Sigma}(\pmb x_i) =-\frac{Nn}{2}\ln2\pi -\frac{N}{2}\ln |\Sigma| - \sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)\\
=C - \frac{N}{2}\ln |\Sigma| - \frac{1}{2}\sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)</script><p>其中$C = -\frac{Nn}{2}\ln2\pi$是一个与参数$\pmb \mu,\Sigma$ 无关的常数，因此：</p>
<script type="math/tex; mode=display">
arg\max_{\pmb \mu,\Sigma}\ln\prod_{i=1}^Nf_{\mu,\Sigma}(\pmb x_i)</script><ul>
<li>$\ln L(\pmb \mu,\Sigma)$对$\pmb \mu$求偏导（标量对向量求偏导）</li>
</ul>
<p>先将$\sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)$展开，得到：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N\pmb x_i^T\Sigma^{-1}\pmb x_i - 2\sum_{i=1}^N\pmb x^T\Sigma^{-1}\pmb \mu + N\pmb \mu^T\Sigma^{-1}\pmb \mu</script><p>后两项对$\pmb \mu$求偏导得到：</p>
<script type="math/tex; mode=display">
-2\sum_{i=1}^N\Sigma^{-1}\pmb x_i + 2N\Sigma^{-1}\pmb \mu</script><p>因此：</p>
<script type="math/tex; mode=display">
\frac{\partial \ln L(\pmb \mu,\Sigma)}{\partial \pmb \mu} = 2\sum_{i=1}^N\Sigma^{-1}\pmb x_i - 2N\Sigma^{-1}\pmb \mu = 0</script><p>求得：</p>
<script type="math/tex; mode=display">
\bar{\pmb \mu} = \frac{1}{N}\sum_{i=1}^N \pmb x_i</script><ul>
<li>$\ln L(\pmb \mu, \Sigma)$对$\Sigma$求偏导（标量对矩阵求偏导）</li>
</ul>
<p>tips：$\frac{\partial \det(X)}{\partial X } = \det (X)tr(X^{-1})$</p>
<script type="math/tex; mode=display">
\frac{\partial -\frac{N}{2}\ln \det(\Sigma)}{\partial \Sigma} = -\frac{N}{2}tr(X^{-1})</script><script type="math/tex; mode=display">
\frac{\partial \ln L(\pmb \mu, \Sigma)}{\partial \Sigma} = 0</script><p>解得：</p>
<script type="math/tex; mode=display">
\bar \Sigma = \frac{1}{N} \sum_{i=1}^N(\pmb x_i - \bar{\pmb \mu})(\pmb x_i - \bar{\pmb \mu})^T</script>
      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Transformer模型介绍" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/03/Transformer%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/"
    >Transformer模型介绍</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/10/03/Transformer%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/" class="article-date">
  <time datetime="2020-10-03T06:58:05.171Z" itemprop="datePublished">2020-10-03</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="Transformer模型介绍"><a href="#Transformer模型介绍" class="headerlink" title="Transformer模型介绍"></a>Transformer模型介绍</h2><p>文献：Vaswani A,  Shazeer N,  Parmar N,  et al.  Attention is all you need[C]. Advances in neural information processing systems. 2017: 5998-6008.</p>
<p>详解：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a>  The Annotated Transformer.</p>
<p>视频：<a href="https://www.youtube.com/watch?v=ugWDIIOHtPA" target="_blank" rel="noopener">https://www.youtube.com/watch?v=ugWDIIOHtPA</a>  李宏毅：Transformer</p>
<h3 id="1-encoder-decoder"><a href="#1-encoder-decoder" class="headerlink" title="1. encoder-decoder"></a>1. encoder-decoder</h3><h3 id="2-self-attention"><a href="#2-self-attention" class="headerlink" title="2. self attention"></a>2. self attention</h3><h3 id="3-multi-head-attention"><a href="#3-multi-head-attention" class="headerlink" title="3. multi head attention"></a>3. multi head attention</h3>
      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-贝叶斯垃圾邮件分类器" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/09/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/"
    >贝叶斯垃圾邮件分类器</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/09/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/" class="article-date">
  <time datetime="2020-09-29T03:06:11.061Z" itemprop="datePublished">2020-09-29</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="贝叶斯垃圾邮件分类器"><a href="#贝叶斯垃圾邮件分类器" class="headerlink" title="贝叶斯垃圾邮件分类器"></a>贝叶斯垃圾邮件分类器</h2><p>利用朴素贝叶斯算法，在给定训练数据上，训练一个垃圾分类器。有关朴素贝叶斯的基本思想，可以参见另一篇博客：<a href="https://zyj-nova.github.io/2020/02/09/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88Naive%20Bayes%EF%BC%89/" target="_blank" rel="noopener">https://zyj-nova.github.io/2020/02/09/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88Naive%20Bayes%EF%BC%89/</a></p>
<h3 id="0-数据集获取"><a href="#0-数据集获取" class="headerlink" title="0 数据集获取"></a>0 数据集获取</h3><p>链接：<a href="https://pan.baidu.com/s/1P3Jg67nerg63GVbCkc1YJg" target="_blank" rel="noopener">https://pan.baidu.com/s/1P3Jg67nerg63GVbCkc1YJg</a> ，提取码：576u 。</p>
<p>数据集为英文语料，分为训练集和测试集。训练集包含351封垃圾邮件（文件名标有“spmsg”）、351封正常邮件；测试集包含130封正常邮件和130封垃圾邮件。</p>
<h3 id="1-特征提取"><a href="#1-特征提取" class="headerlink" title="1 特征提取"></a>1 特征提取</h3><p>由于每一封邮件都不是数值型的特征，因此我们需要把他们编码。首先需要构建一个字典，统计所有邮件中单词出现的频率，同时删除字典中的非字母字符、长度为1的字符，之后取前3000个出现频率最高的作为最终的字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">n_dim = <span class="number">3000</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_Dictionary</span><span class="params">(train_dir)</span>:</span></span><br><span class="line">    emails = [os.path.join(train_dir, f) <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(train_dir)]</span><br><span class="line">    all_words = []</span><br><span class="line">    <span class="keyword">for</span> mail <span class="keyword">in</span> emails:</span><br><span class="line">        <span class="keyword">with</span> open(mail) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment"># 遍历每一行</span></span><br><span class="line">            <span class="keyword">for</span> i,line <span class="keyword">in</span> enumerate(f):</span><br><span class="line">                <span class="keyword">if</span>(i == <span class="number">2</span>):</span><br><span class="line">                    <span class="comment">#正文开始</span></span><br><span class="line">                    words = line.split()<span class="comment">#默认按照空格分词</span></span><br><span class="line">                    all_words += words</span><br><span class="line">    dictionary = Counter(all_words)</span><br><span class="line">    <span class="comment"># 词典构建好，删除多余的词汇</span></span><br><span class="line">    list_to_remove = list(dictionary.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> list_to_remove:</span><br><span class="line">        <span class="keyword">if</span> item.isalpha() == <span class="literal">False</span>:</span><br><span class="line">            <span class="keyword">del</span> dictionary[item]</span><br><span class="line">        <span class="keyword">elif</span> len(item) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">del</span> dictionary[item]</span><br><span class="line">    dictionary = dictionary.most_common(n_dim)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dictionary</span><br></pre></td></tr></table></figure>
<p>构建完字典以后，就可以对每一封邮件进行编码了。特征向量的每一个分量即邮件中出现的对应词频。例如邮件“do over，do again”可以被编码为$[0,0,…,2,…,1,…,1]$。每一个分量都代表对应的词频。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将每个邮件转化为词频向量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_features</span><span class="params">(mail_dir)</span>:</span></span><br><span class="line">    </span><br><span class="line">    files = [os.path.join(mail_dir, f) <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(mail_dir)]</span><br><span class="line">    labels = np.ones(len(files))</span><br><span class="line">    feature_matrix = np.zeros((len(files), n_dim))</span><br><span class="line">    docID = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        <span class="keyword">with</span> open(file) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">if</span> f.name[<span class="number">30</span>:].find(<span class="string">"sp"</span>) != <span class="number">-1</span>:</span><br><span class="line">                <span class="comment">#垃圾邮件</span></span><br><span class="line">                labels[docID] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(f):</span><br><span class="line">                <span class="keyword">if</span> i == <span class="number">2</span>:</span><br><span class="line">                    words = line.split()</span><br><span class="line">                    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">                        wordID = <span class="number">0</span></span><br><span class="line">                        <span class="keyword">for</span> i,d <span class="keyword">in</span> enumerate(dictionary):</span><br><span class="line">                            <span class="keyword">if</span>(d[<span class="number">0</span>] == w):</span><br><span class="line">                                wordID = i</span><br><span class="line">                                feature_matrix[docID, wordID] += <span class="number">1</span></span><br><span class="line">        docID += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> feature_matrix, labels</span><br></pre></td></tr></table></figure>
<h3 id="2-计算概率"><a href="#2-计算概率" class="headerlink" title="2 计算概率"></a>2 计算概率</h3><p>$P(word|y =spam) = P(word, spam) / P(spam)$，$P(word, spam)$即每个单词在垃圾邮件中出现的频数，$P(spam)$即垃圾邮件中所有单词的词频总和。正常邮件同理，得到概率矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计垃圾邮件中的所有词频和，以及各个词的频数</span></span><br><span class="line">spam_words = <span class="number">0</span></span><br><span class="line">non_spam_words = <span class="number">0</span></span><br><span class="line">spam_mails = []</span><br><span class="line">non_spam_mails = []</span><br><span class="line"><span class="keyword">for</span> i,line <span class="keyword">in</span> enumerate(feature_matrix):</span><br><span class="line">    <span class="keyword">if</span> labels[i] == <span class="number">1</span>:</span><br><span class="line">        non_spam_words += int(feature_matrix[i].sum())</span><br><span class="line">        non_spam_mails.append(feature_matrix[i])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        spam_mails.append(feature_matrix[i])</span><br><span class="line">        spam_words += int(feature_matrix[i].sum())</span><br><span class="line">print(<span class="string">"spam total words are &#123;&#125;, non-spam total words are &#123;&#125;"</span>.format(spam_words, non_spam_words))</span><br><span class="line"></span><br><span class="line">spam_mails = np.array(spam_mails)</span><br><span class="line">non_spam_mails = np.array(non_spam_mails)</span><br><span class="line"></span><br><span class="line">weight_matrix = np.zeros((<span class="number">2</span>,n_dim))</span><br><span class="line">weight_matrix[<span class="number">0</span>, :] = spam_mails.sum(axis = <span class="number">0</span>) / spam_words</span><br><span class="line">weight_matrix[<span class="number">1</span>, :] = non_spam_mails.sum(axis = <span class="number">0</span>) / non_spam_words</span><br></pre></td></tr></table></figure>
<p>为了避免概率矩阵中出现0的情况，使得计算概率时出现乘0，最后让权重矩阵每个分量加一个非常小的扰动</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sigma = <span class="number">1e-8</span></span><br><span class="line">weight_matrix += sigma</span><br></pre></td></tr></table></figure>
<h3 id="3-模型预测与评估"><a href="#3-模型预测与评估" class="headerlink" title="3 模型预测与评估"></a>3 模型预测与评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pred_labels = []</span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> test_feature:</span><br><span class="line">    spam_prob = <span class="number">1.0</span></span><br><span class="line">    non_spam_prob = <span class="number">1.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i,w <span class="keyword">in</span> enumerate(feature):</span><br><span class="line">        <span class="keyword">if</span> w != <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 计算P(w|spam) P(w|non-spam)</span></span><br><span class="line">            spam_prob += np.log10(weight_matrix[<span class="number">0</span>, i])</span><br><span class="line">            non_spam_prob += np.log10(weight_matrix[<span class="number">1</span>, i])</span><br><span class="line">    print(spam_prob, non_spam_prob)</span><br><span class="line">    <span class="keyword">if</span> spam_prob &gt; non_spam_prob:</span><br><span class="line">        pred_labels.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pred_labels.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>在计算概率的时候用到了一个trick，由于非常小的数不断连乘最后会趋于0，因此利用取对数的方法将乘法化为加法（极大似然估计就是这么干的）。同时取对数时不可以为0，上面加的非常小的扰动是很有必要的。</p>
<p>模型在测试集上的准确率、召回率、f1_score如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">recall_score : <span class="number">0.9923076923076923</span></span><br><span class="line">accuracy_score : <span class="number">0.9846153846153847</span></span><br><span class="line">f1_score : <span class="number">0.9847328244274809</span></span><br></pre></td></tr></table></figure>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Strang教授线性代数笔记" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/09/23/Strang%E6%95%99%E6%8E%88%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%AC%94%E8%AE%B0/"
    >Strang教授线性代数笔记</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/09/23/Strang%E6%95%99%E6%8E%88%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2020-09-23T02:44:37.995Z" itemprop="datePublished">2020-09-23</time>
</a>
      
      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h3 id="Strang教授线性代数笔记"><a href="#Strang教授线性代数笔记" class="headerlink" title="Strang教授线性代数笔记"></a>Strang教授线性代数笔记</h3><p>具体参见：<a href="https://github.com/zyj-nova/LinearAlgebraNotes" target="_blank" rel="noopener">https://github.com/zyj-nova/LinearAlgebraNotes</a></p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-浅谈逻辑斯蒂回归模型" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/08/03/%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"
    >浅谈逻辑斯蒂回归模型</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/08/03/%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2020-08-03T08:01:07.919Z" itemprop="datePublished">2020-08-03</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="浅谈逻辑斯蒂回归模型"><a href="#浅谈逻辑斯蒂回归模型" class="headerlink" title="浅谈逻辑斯蒂回归模型"></a>浅谈逻辑斯蒂回归模型</h2><h3 id="sigmod函数"><a href="#sigmod函数" class="headerlink" title="sigmod函数"></a>sigmod函数</h3><script type="math/tex; mode=display">
f(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">
f^\prime (x)= f(x)*(1-f(x))</script><p>​        逻辑斯蒂回归或者是softmax回归，说是回归其实是在解决二分类或者多分类的问题。逻辑斯蒂模型如下：</p>
<script type="math/tex; mode=display">
y = \frac{1}{1 + e^{-（w^Tx +b)}}</script><p>​        其中，$x \in R^n, w \in R^n$，$b$为偏置bias可以设置为0，$x$为样本的特征向量，$w$即要学习的参数。在给定一批有label的训练数据集后，我们可以根据极大似然估计来得到最优的参数值，之后，给定某个测试数据，当：y &gt;= 0.5，我们就把这个样本划为1类，否则就划为0类。</p>
<p>​        其实，完整的二项逻辑斯蒂回归模型是定义如下的条件概率分布：</p>
<script type="math/tex; mode=display">
P(Y = 1|x) = \frac{\exp(w*x + b)}{1 + \exp(w*x + b)}</script><script type="math/tex; mode=display">
P(Y = 0|x) = \frac{1}{1 + \exp(w*x + b)}</script><p>​        观察上述条件概率计算公式我们发现，当$w<em>x+b &gt; 0$的时候，$P(Y=1)$的概率一定高于$P(Y=0)$，我们就把其划分为1类。对于$P(Y=1)$的分布函数上下同除以分子$\exp$就得到了熟悉的逻辑斯蒂函数。$w</em>x+b &gt; 0$就等价于$y &gt; 0.5$，因此当逻辑斯蒂函数值$y &gt; 0.5$时就把样本划分到1类，在这个条件下，样本为1类的概率大于为0类的概率。</p>
<p>​        事实上，逻辑斯蒂函数是在计算样本为1类的概率，由于是二分类，不是1类就是0类。</p>
<h3 id="softmax回归模型"><a href="#softmax回归模型" class="headerlink" title="softmax回归模型"></a>softmax回归模型</h3><p>参考：<a href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" target="_blank" rel="noopener">https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/</a></p>
<p><strong>softmax函数</strong></p>
<script type="math/tex; mode=display">
\sigma (\pmb x)_i = \frac{e^{x_i}}{\sum_j^n e^{x_j}},i = 1,...,n,\pmb x=(x_1,...,x_n)</script><p>定义：</p>
<script type="math/tex; mode=display">
S_j = \frac{e^{x_j}}{\sum_{k=1}^ne^{x_k}}, j = 1,2,...,n</script><p>​        可以看到，softmax函数输入一个向量，输出是一个向量，是一个$\R^n \rightarrow \R^n$的函数。下面讨论其输入的各个分量的偏导，可以看到其输出分量对输入分量的偏导构成了一个$n \times n$的雅可比矩阵。</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{\partial S_i}{\partial x_j}</script><script type="math/tex; mode=display">
DS = \begin{pmatrix}D_1S_1&D_1S_2&...&D_1S_n\\...&...&...&...\\D_nS_1&..&...&D_nS_n\end{pmatrix}</script><p>现在计算$D_jS_i$:</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{\partial S_i}{\partial x_j} ,S_i =\frac{e^{x_i}}{\sum_{k=1}^ne^{x_k}}</script><p>$S_i$是个复合函数</p>
<script type="math/tex; mode=display">
S_i = \frac{g(x)}{h(x)},g(x) = e^{x_i},h(x)=\sum_{k=1}^ne^{x_k}</script><p>根据求导的除法法则：</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{g^\prime(x)h(x)-h^\prime(x)g(x)}{h^2(x)}</script><p>这里需要分类讨论，即$i =j$和$i\not=j$的情况：$i=j$时，$g^\prime(x_j)=e^{x_j}=e^{x_i},h^\prime(x_i)=e^{x_i} = e^{x_j}$；否则$i\not=j$，$g^\prime(x_i)=0,h^\prime(x_i)=e^{x_j}$.</p>
<p>$i=j$时带入得：</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{e^{x_i}\Sigma - e^{x_j}e^{x_i}}{\Sigma^2} = \frac{e^{x_i}}{\Sigma}\frac{\Sigma - e^{x_j}}{\Sigma} = S_i(1-S_j)</script><p>$i\not=j$时，带入得：</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{0 - e^{x_i}e^{x_j}}{\Sigma^2} = -S_iS_j</script><p>综合以上讨论，有：</p>
<script type="math/tex; mode=display">
D_jS_i = S_i(\delta_{ij}-S_j)</script><script type="math/tex; mode=display">
i = j,\delta_{ij} = 1;i \not=j,\delta_{ij}=0</script><p><strong>softmax回归模型</strong>        </p>
<p>​        值得注意的是，这里讲的softmax回归与softmax函数还是有一点不同的。softmax函数只是把给定的输入做了一个指数归一化；相比logistic回归模型的参数只是一个向量，softmax回归模型包含参数矩阵$W$，也就是每一个类别都对应一个n维向量，n为输入特征数量。</p>
<p>假设有$k$个类别，则权重矩阵：</p>
<script type="math/tex; mode=display">
W_{k \times n} = \begin{pmatrix}\pmb w_1^T\\...\\\pmb w_k^T\end{pmatrix}</script><p>输入为第$j$个类别的概率为：</p>
<script type="math/tex; mode=display">
P(y = j | \pmb x) = \frac{e^{\pmb x^T\pmb w_j}}{\sum_{i=1}^k e^{\pmb x^T\pmb w_i}}</script><p>模型针对给定输入预测类别的过程如下图所示：</p>
<p><img src="https://eli.thegreenplace.net/images/2016/softmax-layer-generic.png" alt=""></p>
<p>注：T表示类别数量。</p>
<h3 id="二者的关联"><a href="#二者的关联" class="headerlink" title="二者的关联"></a>二者的关联</h3><p>当$k = 2$的时候，</p>
<script type="math/tex; mode=display">
P(y = 1) = \frac{e^{\pmb x^T\pmb w_1}}{e^{\pmb x^T\pmb w_1} + e^{\pmb x^T\pmb w_0}} = \frac{e^{\pmb x^T(\pmb w_1 - \pmb w_0)}}{1 + e^{\pmb x^T(\pmb w_1 - \pmb w_0)}}</script><script type="math/tex; mode=display">
P(y = 0) = \frac{e^{\pmb x^T\pmb w_0}}{e^{\pmb x^T\pmb w_1} + e^{\pmb x^T\pmb w_0}} = \frac{1}{1 + e^{\pmb x^T(\pmb w_1 - \pmb w_0)}}</script><p>令$x^T(\pmb w_1 - \pmb w_0) = \pmb t$，那么：</p>
<script type="math/tex; mode=display">
P(y = 1) = \frac{e^{\pmb t}}{1 + e^{\pmb t}}</script><script type="math/tex; mode=display">
P(y = 0) = \frac{1}{1 + e^{\pmb t}}</script><p>我们可以发现，当类别数等于$2$时，softmax模型就会退化为logistic回归模型。</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-近期刷题总结记录" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/07/18/%E8%BF%91%E6%9C%9F%E5%88%B7%E9%A2%98%E6%80%BB%E7%BB%93%E8%AE%B0%E5%BD%95/"
    >近期刷题总结记录</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/07/18/%E8%BF%91%E6%9C%9F%E5%88%B7%E9%A2%98%E6%80%BB%E7%BB%93%E8%AE%B0%E5%BD%95/" class="article-date">
  <time datetime="2020-07-18T14:33:46.959Z" itemprop="datePublished">2020-07-18</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Leetcode/">Leetcode</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="近期刷题总结记录"><a href="#近期刷题总结记录" class="headerlink" title="近期刷题总结记录"></a>近期刷题总结记录</h2><h3 id="1-Leftmost-Digit"><a href="#1-Leftmost-Digit" class="headerlink" title="1. Leftmost Digit"></a>1. Leftmost Digit</h3><p>​        杭电1060题，给你一个数字$n$，求出$n^n$最高位数字$t$，$n$不超过$10,000,000,000$(10亿)。</p>
<p>​        具体思想如下：</p>
<p>​        设$M = n^n$（n为整数），两边取10的对数则有</p>
<script type="math/tex; mode=display">
\log_{10}M = \log_{10} n^n = k</script><p>​        直觉上，如果$M$不是10的整数幂次，那么$k$是一个浮点数。</p>
<script type="math/tex; mode=display">
k = A.B</script><p>​        $A$为$k$ 的整数部分，$B$ 则为$k$ 的小数部分。有</p>
<script type="math/tex; mode=display">
\log_{10} M = k = A.B \\
10^{A.B} = M \\
10^A*10^{B} = M(B < 1)</script><p>​        其实$A$就代表了$M$的位数（三位数$10^2$、四位数$10^3$以此类推），而$10^B$相当于前面的系数（有点类似科学计数法那个形式）。</p>
<p>​        我们观察幂函数$y = 10^x$ ，在$x = 1$时$y = 10$，我们这儿$B &lt; 1$，<strong>因此$10^B$的值介于$[1,10)$之间，并且是一个浮点数</strong>。我们可以这么想，$10^A*10^{B}$把$10^B$放大了$10^A$倍后得到了$M$，即 $n$ 的 $n$ 次幂。</p>
<p>​        好了，那么$M$的最高位到底是几？很明显是$10^B$这个数字的整数部分。如何取到$B$这个小数呢，$k = n \log_{10} n = A.B$，$k$减去其取整部分即可得到$B$。$10^B$再取整就得到了最高位。</p>
<p>​        程序代码如下</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> t,n;</span><br><span class="line">	<span class="keyword">double</span> k;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;t);</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">        k = n * <span class="built_in">log10</span>(<span class="number">1.0</span> * n); <span class="comment">// k = A.B</span></span><br><span class="line">        <span class="comment">// k减去对k取整部分得到小数部分</span></span><br><span class="line">        k = k - (__int64)k;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,(<span class="keyword">int</span>)<span class="built_in">pow</span>(<span class="number">10</span>,k));</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-Euler函数"><a href="#2-Euler函数" class="headerlink" title="2. Euler函数"></a>2. Euler函数</h3><p>​        在数论中，对正整数，欧拉函数是小于或等于n的正整数中与n互质的数的数目。比如$\phi(8) = 4$，因为1、3、5、7与8互质。注：互质为两者没有除1外的公因数。</p>
<p>​        具体定义如下：</p>
<p>​        我们知道，对于任意正整数n，都可以表示为多个质数的乘积，即如下形式：</p>
<script type="math/tex; mode=display">
n = p_1^{k_1}p_2^{k2}...p_n^{k_n}</script><p>​        ，其中，$p_i$均为质数。</p>
<p>​        那么欧拉函数$\phi(n)$等于</p>
<script type="math/tex; mode=display">
\phi(n) = n\prod_{i=1}^n (1 - \frac{1}{p_i})</script><p>​        性质：</p>
<p>​        ① 欧拉函数是积性函数，$\phi(mn)=\phi(m)*\phi(n)$</p>
<p>​        ② 若p为质数，$\phi(p)=p-1$</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="comment">// 欧拉函数:统计小于等于n中与n互质的数的个数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">euler</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ans = n;</span><br><span class="line">    <span class="keyword">int</span> tmp = n;</span><br><span class="line">    <span class="comment">// n在循环过程中会不断发生变化。</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= <span class="built_in">sqrt</span>(<span class="number">1.0</span> * tmp); i++ )&#123;<span class="comment">//枚举所有能够被n整除的质因数</span></span><br><span class="line">        <span class="keyword">if</span>(n % i == <span class="number">0</span>)&#123;</span><br><span class="line">            ans = ans * (i - <span class="number">1</span>) / i;<span class="comment">// 欧拉公式，不断累乘</span></span><br><span class="line">            <span class="keyword">while</span>(n % i == <span class="number">0</span>)&#123;</span><br><span class="line">                n /= i; <span class="comment">// 消除质数因子的过程</span></span><br><span class="line">                <span class="comment">//将质数i不断的左除过去</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">         <span class="comment">// 新的n = n / p1 / p1 / p1 ... = p2*p2*p2...*pn</span></span><br><span class="line">        <span class="comment">// p1 &lt; p2 &lt; ... &lt; pn</span></span><br><span class="line">        <span class="comment">// 再次循环通过枚举找到p2....pn</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// n不为1，表明还有因子，最后一个质因子可能不在[2,sqrt(n)]之间</span></span><br><span class="line">    <span class="keyword">if</span>(n &gt; <span class="number">1</span>)&#123;</span><br><span class="line">        ans = ans / n * (n - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> t,n;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;t);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,euler(n));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-找出缺失的数字"><a href="#3-找出缺失的数字" class="headerlink" title="3. 找出缺失的数字"></a>3. 找出缺失的数字</h3><p>​        题目大意如下，首先输入$n$个不同的数字，再次输入$n-1$个数字，找出第二次输入没有输入的那个数字。输入的数字的值介于$[0,1000000000]$，$2 &lt;= n &lt;= 400000$。</p>
<p>​        可以看到数据规模较大，无论是集合还是利用高斯公式求两次和都可能数据溢出。能不能使用$O(n)$时间复杂度以及$O(1)$空间解决这个问题？</p>
<p>​        下面介绍二进制中异或（<code>^</code>）的操作的一些性质。</p>
<p>​        异或就是相同为0，不同为1。即<code>1 ^ 1 = 0,0 ^ 0 = 0,1 ^ 0 = 1,0 ^ 1 = 1</code>。异或还有如下性质</p>
<ul>
<li><p><code>0 ^ x = x</code></p>
</li>
<li><p><code>x ^ x = 0</code></p>
</li>
<li><p>异或满足交换律，即<code>1 ^ 2 ^ 4 ^ 6 =  6 ^ 2 ^ 1 ^ 4</code></p>
<p>有了这些性质，对于这个题，我们设置<code>a = 0,b = 0</code>；</p>
<p><code>a = a ^ arr1[i], i from 0 to n-1</code>，<code>b = b ^ arr2[j], j  from 0 to n-2</code>。</p>
<p>最后所求数字即为<code>a^b</code>。</p>
<p>其实很好理解：</p>
<script type="math/tex; mode=display">
a = a_1 \bigoplus a_2 \bigoplus....\bigoplus a_n \\
b = b_1 \bigoplus b_2 \bigoplus....\bigoplus b_{n-1} \\</script><p>最后<code>a^b</code>，由<strong>异或交换律</strong>这个性质等价于，将前后两次相同的数字先做异或得到0，前后两次相同的数字就都被消掉了，最后剩下的那个数字再与0异或还是这个数字，也就是第二次没有出现的那个数字。</p>
<p>同理，找出在都是偶数次出现的数字中出现次数为奇数次的数字也可以利用异或算法解决。</p>
<p><code>a = 1 ^ 2 ^ 4 ^ 6;b = 4 ^ 6 ^ 1;</code></p>
<p> <code>a ^ b =1 ^ 2 ^ 4 ^ 6 ^ 4 ^ 6 ^ 1 = 1 ^ 1 ^ 4 ^ 4 ^ 6 ^ 6 ^ 2= 0 ^ 0 ^ 0 ^ 2 = 0 ^ 2 = 2</code></p>
</li>
</ul>
<h3 id="4-Rightmost-Digit"><a href="#4-Rightmost-Digit" class="headerlink" title="4. Rightmost Digit"></a>4. Rightmost Digit</h3><p>​        杭电1061题。与第1题相反的是，本题求正整数$n^n$最左边的数字。要用到快速幂取模算法。快速幂即比较快的计算n的幂次。相比于循环n次计算的$O(n)$算法，快速幂能够在$O(\log n)$求出结果。比如计算$3^{100}$，一般我们可以循环100次计算出结果。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> <span class="keyword">long</span> ans = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++)&#123;</span><br><span class="line">    ans *= <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​        我们观察发现:</p>
<script type="math/tex; mode=display">
3^{100} = 3^{50}*3^{50}\\
3^{50} = 3^{25}*3^{25}\\
3^{25} = 3^{12}*3^{12}*3\\
3^{12} = 3^6*3^6 \\
3^6 = 3^3*3^3\\
3^3 = 3^2 * 3\\
3^2 = 3 * 3</script><p>​        这样，我们要计算$3^{100}$，就可以先计算出$3^{50}$，然后自己和自己相乘就可以得到结果，对于$3^{25}$同理，然后这样递推下去就可得到结果。按照这个方法计算，我们只做了8次乘法便得到了结果，相比100次大大减少了时间。幂次不是奇数就是偶数，偶数次幂就等于自己乘自己，奇数次幂得额外再乘以一个基底数字。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 递归计算快速幂 a^b</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(b == <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> a;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> temp = f(a, b / <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">return</span> (b % <span class="number">2</span> == <span class="number">0</span> ? <span class="number">1</span> : a) * temp * temp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 非递归</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> base = a;</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>( b != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(b % <span class="number">2</span> == <span class="number">1</span>)&#123;</span><br><span class="line">            ans = ans * base;</span><br><span class="line">        &#125;</span><br><span class="line">        base = base * base;</span><br><span class="line">        b /= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​        回到这道题，两个数相乘，二者的最后一位分别相乘贡献了结果，因此我们可以在计算快速幂过程中不断对10取模就可以得到最终结果。</p>
<p>​        插入一点其他知识：</p>
<script type="math/tex; mode=display">
(a + b) \mod p = (a\mod p + b \mod p) \mod p</script><p>​        在计算斐波那契数列时会用到。将加号改为乘号同样适用。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 快速幂取余</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> base = a;</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>( b != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(b % <span class="number">2</span> == <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="comment">// 最后两位数相乘再对10取余 ans = ans * base ans % 10 = (ans%10)*(base%10)%10</span></span><br><span class="line">            ans = (ans % <span class="number">10</span>) * (base % <span class="number">10</span>) % <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 因为求最后一位数字，每次记录最后一位数字是几就可以避免越界</span></span><br><span class="line">        base = (base % <span class="number">10</span>)* (base % <span class="number">10</span>) % <span class="number">10</span>;</span><br><span class="line">        b /= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> t,n;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; t;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,f(n,n));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5-阶乘位数"><a href="#5-阶乘位数" class="headerlink" title="5. 阶乘位数"></a>5. 阶乘位数</h3><p>​        给定一个小于等于$10,000,000$的正整数，计算其阶乘的位数。小于1000的数字的阶乘可以通过模拟乘法来计算出来，但是当数字特别大数组也无法存下。这是就要用到斯特林公式。</p>
<p>​        一个十进制数 $n$ 的位数可以表示为：</p>
<script type="math/tex; mode=display">
(int)\log_{10} n + 1</script><p>这一点可以很明显的在$y = \log_{10} x$的函数图像上观察出来。</p>
<p>​        因此，$n!$的位数即：</p>
<script type="math/tex; mode=display">
\log_{10} n! + 1</script><script type="math/tex; mode=display">
\log_{10} n! = \log_{10} n * (n-1) * ... * 1 = \log_{10}n + \log_{10}n-1 + ...</script><p>​        只要循环计算1到n的对数再求和之后取整加一就是结果。</p>
<h3 id="6-背包问题"><a href="#6-背包问题" class="headerlink" title="6. 背包问题"></a>6. 背包问题</h3><p>参考文献：背包问题九讲，崔添翼。</p>
<h4 id="6-1-0-1背包"><a href="#6-1-0-1背包" class="headerlink" title="6.1 0-1背包"></a>6.1 0-1背包</h4><p><strong>问题描述：</strong>给定容量为 $V$ 的背包，有 $n$ 个物品，每个物品有价值 $w$、体积 $c$ 两个属性，求背包能装的物品最大价值。</p>
<p><strong>求解：</strong>二维数组 $dp$，$dp[i][j]$ 表示有 $i$ 个物品，背包容量为 $j$ 的情况下，此时的背包最大价值；$dp[n][V]$即为答案</p>
<p>状态转移公式：</p>
<script type="math/tex; mode=display">
dp[i][j] = \max(dp[i-1][j], dp[i-1][j - c[i]] + w[i])</script><p><strong>理解状态转移公式</strong>：</p>
<p>对于每个背包都有两种选择：装与不装（装的前提是当前容量 $j$ 能装下 $c[i]$），不装的话就是前 $i-1$ 个物品容量为 $j$ 时的价值 $dp[i][j]$；若选择装，则背包需要提前给当前物品留下 $c[i]$ 的空间。</p>
<p>代码实现（杭电2602号问题）：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> t, n, V;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1005</span>][<span class="number">1005</span>], v[<span class="number">1005</span>], w[<span class="number">1005</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;t);</span><br><span class="line">    <span class="keyword">while</span>(t--)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;n, &amp;V);</span><br><span class="line">        <span class="comment">// 输入n件物品的价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;v[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 输入n件物品的体积</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;w[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// dp[i][j]代表了到第i个物品，背包容量为j时的最大价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= V; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j &gt;= w[i])&#123;</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i<span class="number">-1</span>][j - w[i]] + v[i], dp[i<span class="number">-1</span>][j]);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, dp[n][V]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>空间压缩：</strong></p>
<script type="math/tex; mode=display">
dp[j] = \max(dp[j], dp[j - c[i]] + w[i])</script><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> t, n, V;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1005</span>], v[<span class="number">1005</span>], w[<span class="number">1005</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;t);</span><br><span class="line">    <span class="keyword">while</span>(t--)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;n, &amp;V);</span><br><span class="line">        <span class="comment">// 输入n件物品的价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;v[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memset</span>(dp, <span class="number">0</span>, <span class="keyword">sizeof</span>(dp));</span><br><span class="line">        <span class="comment">// 输入n件物品的体积</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;w[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// dp[j]代表背包容量为j时的最大价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= w[i]; j--)&#123;<span class="comment">//注意逆序</span></span><br><span class="line">                dp[j] = <span class="built_in">max</span>(dp[j], dp[j - w[i]] + v[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, dp[V]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>初始化的细节问题</strong></p>
<ul>
<li><p>当有些问题要求“恰好装满背包”时的最优解，这时在初始化的时候，dp数组除了dp[0]为0，其余dp[1…V]均设置为无穷大。可以这样理解：初始化的dp数组事实上就是在没有任何物品可以放入背包时的合法状态。如果要求背包恰好装满，那么此时只有容量为$0$的背包可以在什么也不装且价值为 $0$ 的情况下被“恰好装满”，其它容量的背包均没有合法的解，属于未定义的状态，应该被赋值为无穷大了。如果背包并非必须被装满，那么任何容量的背包都有一个合法解“什么都不装”，这个解的价值为0，所以初始时状态的值也就全部为0了。</p>
</li>
<li><p>还有一种情况就是求背包的最小价值，把转移方程中的$max$改为$min$即可。</p>
</li>
<li>有些题目中，只给了价值或者重量这一个数组，那么此时我们可以认为数值上价值与重量时一致的。比如只给了价值 $v$，此时01背包状态转移公式就成了：$dp[j+1] = \max(dp[j],dp[j - v[i]] + v[i]),j \in [V,v[i]]] $。</li>
</ul>
<p>杭电1114号问题就是以上两种情况的结合。题目大意就是给定存钱罐初始重量和装满时候的重量，然后给定几种不同价值和重量的硬币，问能够恰好装满存钱罐的最小价值是多少？每个种类的硬币可以无限使用，这是一个<strong>完全背包问题</strong>。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> m, n, t, N;</span><br><span class="line"><span class="keyword">int</span> v[<span class="number">502</span>], w[<span class="number">502</span>]; <span class="comment">// 硬币价值 和 硬币重量数组</span></span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">10005</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> inf = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;N);</span><br><span class="line">    <span class="keyword">while</span>(N--)&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//输入初始存钱罐重量和现在存钱罐重量</span></span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;m, &amp;n);</span><br><span class="line">        <span class="keyword">int</span> V = n - m; <span class="comment">// 钱币重量</span></span><br><span class="line">        <span class="comment">// 输入硬币种类</span></span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;t);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>,&amp;v[i], &amp;w[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 注意初始化</span></span><br><span class="line">        <span class="built_in">memset</span>(dp, inf, <span class="keyword">sizeof</span>(dp));</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 找到恰好能装满存钱罐重量的钱币价值中最小的那个</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = w[i]; j &lt;= V; j++)&#123;<span class="comment">// 注意这是顺序</span></span><br><span class="line">                dp[j] = min(dp[j - w[i]] + v[i], dp[j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(dp[V] == inf)&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"This is impossible.\n"</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"The minimum amount of money in the piggy-bank is %d.\n"</span>,dp[V]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="6-2-完全背包"><a href="#6-2-完全背包" class="headerlink" title="6.2 完全背包"></a>6.2 完全背包</h4><p><strong>定义</strong></p>
<p>有 $N$ 种物品和一个容量为 $V$ 的背包，每种物品都有<strong>无限件</strong>可用。放入第 $i$ 种物品的费用是 $c_i$，价值是 $w_i$。求解：将哪些物品装入背包，可使这些物品的耗费的费用总和不超过背包容量，且价值总和最大。</p>
<p><strong>求解</strong></p>
<p>虽然物品个数是无限的，但是实际上，由于背包容量有上限，每个物品最多选取的个数也是有限制的，这样可以转换成多重背包问题，进而可以转换成 01 背包问题。</p>
<script type="math/tex; mode=display">
dp[i][j] = \max(dp[i-1][j-kc_i] + kw_i|0 \le kc_i \le j)</script><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= v; j++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k*c[i] &lt;= j; k++)&#123;</span><br><span class="line">            dp[i][j] = max(dp[i<span class="number">-1</span>][j - k * c[i]] + w[i] * k, dp[i][j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>观察状态转移公式，我们可以用$dp[i][j - c_i]$去更新$dp[i][j]$而不用去枚举$k$了，因此状态转移公式就变成：</p>
<script type="math/tex; mode=display">
dp[i][j] = \max(dp[i-1][j],dp[i-1][j-c_i]+w_i)</script><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= v; j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j &gt;= c[i])&#123;</span><br><span class="line">            dp[i][j] = max(dp[i<span class="number">-1</span>][j], dp[i<span class="number">-1</span>][j-c[i]] + w[i]);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>空间压缩</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1010</span>];</span><br><span class="line"><span class="keyword">int</span> w[<span class="number">21</span>],c[<span class="number">21</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N, V;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; N &gt;&gt; V;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; w[i] &gt;&gt; c[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = c[i]; j &lt;= V; j++)&#123; <span class="comment">// 顺序</span></span><br><span class="line">            dp[j] = max(dp[j- c[i]] + w[i],dp[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; dp[V] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到完全背包就是在第二重循环遍历次序与01背包不同而已，01背包是逆序，完全背包是顺序。</p>
<h4 id="6-3-多重背包"><a href="#6-3-多重背包" class="headerlink" title="6.3 多重背包"></a>6.3 多重背包</h4><p><strong>定义</strong></p>
<p>有 $N$ 种物品和一个容量为 $V$ 的背包。第 $i$ 种物品最多有 $n_i$ 件可用，每件耗费的空间是 $c_i$，价值是 $w_i$。求解将哪些物品装入背包可使这些物品的耗费的空间总和不超过背包容量，且价值总和最大。</p>
<p><strong>求解</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">21</span>][<span class="number">1010</span>];</span><br><span class="line"><span class="keyword">int</span> w[<span class="number">21</span>],c[<span class="number">21</span>],n[<span class="number">21</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N,V;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; N &gt;&gt; V;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; w[i] &gt;&gt; c[i] &gt;&gt; n[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= V; j++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>;  k &lt;= n[i]; k++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j &gt;= c[i]*k)&#123;</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i<span class="number">-1</span>][j - c[i]*k] + w[i]*k, dp[i][j]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; dp[N][V] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>空间优化</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1010</span>];</span><br><span class="line"><span class="keyword">int</span> w[<span class="number">21</span>],c[<span class="number">21</span>],n[<span class="number">21</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N,V;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; N &gt;&gt; V;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; w[i] &gt;&gt; c[i] &gt;&gt; n[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= <span class="number">0</span>; j--)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt;= n[i]; k++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j &gt;= c[i]*k)&#123;</span><br><span class="line">                    dp[j] = <span class="built_in">max</span>(dp[j - c[i]*k] + w[i] * k,dp[j]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; dp[V] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>多重背包的二进制优化</strong></p>
<p>二进制思想即任意数字都可以由$2$的次幂的数字组合而来（用01表示），这是计算机的基础。利用这个思想，我们可以把每个物品数量分成$1,2,4,…,n - 2^K + 1$，每一组的体积和价值分别为$(c_i，w_i),(2c_i,2w_i)，(4c_i，4w_i)$，等等，通过这些组合（选择装与不装）一定可以组成$n$，这样就遍历了$1-n$内所有数字。通过拆分所有物品，就形成了一个新的价值和体积物品组，这些问题组我们可以选择装也可以选择不装，这样利用二进制优化就可以将多重背包转化为01背包问题。</p>
<p>代码实现：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 二进制优化</span></span><br><span class="line"><span class="keyword">int</span> ncnt = <span class="number">1</span>; <span class="comment">// 记录新的拆分组数</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= t; i++)&#123; <span class="comment">// t个物品</span></span><br><span class="line">    <span class="comment">// 将num[i]拆分</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">1</span>; k &lt;= num[i]; k &lt;&lt;=<span class="number">1</span>)&#123;</span><br><span class="line">        nv[ncnt] = k * v[i];<span class="comment">//新拆分的价值数组</span></span><br><span class="line">        nw[ncnt++] = k * w[i];<span class="comment">//新拆分的重量数组</span></span><br><span class="line">        num[i] -= k;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(num[i] &gt; <span class="number">0</span>)&#123;</span><br><span class="line">        nv[ncnt] = v[i] * num[i];</span><br><span class="line">        nw[ncnt++] = w[i] * num[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 01背包</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; ncnt; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= nw[i]; j--)&#123;</span><br><span class="line">        dp[j] = max(dp[j - nw[i]] + nv[i], dp[j]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>杭电1059、2844号题是完全背包的二进制优化。</p>
<h4 id="6-4-有约束的背包问题"><a href="#6-4-有约束的背包问题" class="headerlink" title="6.4 有约束的背包问题"></a>6.4 有约束的背包问题</h4><p>杭电3466号问题。题目大意是说，给定背包容量，每个物品的重量$P$，若要装入该物品要求的最小背包容量$Q$以及该物品的价值$V$，输出不超过背包容量下所能获取的最大物品价值。</p>
<p>相比于裸01背包，这个题目要求装入的时候必须考虑$P,Q$这两个条件，要用到贪心。即$A$物品$p_1=3,q_1=5$，$B$物品$p_2=4,q_2=3$；背包容量为$7$，若先装$A$，所需容量至少为$p_1 + q_2 = 7$，若先装$B$，所需容量至少为$p_2 + q_1 = 9$，因此应该选择容量小的。即：$p_1 + q_2 &lt; p_2 + q_1 \rightarrow q_1-p_1 &gt; q_2 - p_2$，即$q-p$大的先买。但是转移方程中，更新过程是逆向的，因此排序的时候应该按照$p-q$从小到大排序。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> n, V, dp[<span class="number">5005</span>];</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">item</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> p,q,v;</span><br><span class="line">&#125;items[<span class="number">5005</span>];</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(struct item a, struct item b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a.q - a.p &lt; b.q - b.p; <span class="comment">// 按照 q-p 从小到大排序</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(~<span class="built_in">scanf</span>(<span class="string">"%d %d"</span>,&amp;n,&amp;V))&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="comment">// p就是物品的重量 v是物品的价值， 背包容量必须大于p和q才能装入</span></span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;items[i].p, &amp;items[i].q, &amp;items[i].v);</span><br><span class="line">        &#125;</span><br><span class="line">        sort(items + <span class="number">1</span>, items + <span class="number">1</span> + n, cmp);</span><br><span class="line">        <span class="built_in">memset</span>(dp,<span class="number">0</span>,<span class="keyword">sizeof</span>(dp));</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= <span class="built_in">max</span>(items[i].p, items[i].q); j--)&#123;</span><br><span class="line">                dp[j] = <span class="built_in">max</span>(dp[j], dp[j - items[i].p] + items[i].v);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,dp[V]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>背包练习(以下题目皆来自杭电 on-line judge: <a href="http://acm.hdu.edu.cn/" target="_blank" rel="noopener">http://acm.hdu.edu.cn/</a> ，做完还不会背包来找我。</p>
<p><a href="http://acm.hdu.edu.cn/showproblem.php?pid=2602" target="_blank">2602</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1114" target="_blank">1114</a>  <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1171" target="_blank">1171</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=2844" target="_blank">2844</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1059" target="_blank">1059</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=2955" target="_blank">2955</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1203" target="_blank">1203</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=3466" target="_blank">3466</a></p>
<h3 id="7-三分法"><a href="#7-三分法" class="headerlink" title="7. 三分法"></a>7. 三分法</h3><p>二分法能够有效解决有序序列的问题，然而当一个序列先增大后减小（或者先减小后增大）即存在一个峰值时，就要用到三分的思想了。三分和二分有点像，三分是在left和right指针区间内，设置$mid1$、$mid2$两个指针，即$mid1$与$mid2$将$[left，right]$区间三等分。之后根据具体问题更新left和right指针位置，即可能：$left = mid1$或者$right = mid2$。</p>
<ul>
<li>例子：求解函数$f(x) = x^2 -4x + 3,x \in [0,5]$上的极小值。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">f</span><span class="params">(<span class="keyword">double</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x * x - <span class="number">4</span> * x + <span class="number">3</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">double</span> left = <span class="number">0.0</span>, right = <span class="number">5.0</span>, mid1, mid2;</span><br><span class="line">    <span class="keyword">while</span>(right - left &gt; <span class="number">1e-8</span>)&#123;</span><br><span class="line">        mid1 = left + (right - left) / <span class="number">3.0</span>;</span><br><span class="line">        mid2 = right - (right - left) / <span class="number">3.0</span>;</span><br><span class="line">        <span class="comment">// 求[0,5]区间f的极小值</span></span><br><span class="line">        <span class="keyword">if</span>(f(mid1) &gt; f(mid2))&#123; <span class="comment">// mid2对应的点更小</span></span><br><span class="line">            left = mid1;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            right = mid2;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">double</span> min_value = f(left);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%.6lf\n"</span>,min_value);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%.6lf\n"</span>,left);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Graph Neutral Network和Graph Embedding" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/07/07/Graph%20Neutral%20Network%E5%92%8CGraph%20Embedding/"
    >Graph Neutral Network和Graph Embedding</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/07/07/Graph%20Neutral%20Network%E5%92%8CGraph%20Embedding/" class="article-date">
  <time datetime="2020-07-07T09:14:50.127Z" itemprop="datePublished">2020-07-07</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="Graph-Neutral-Network和Graph-Embedding"><a href="#Graph-Neutral-Network和Graph-Embedding" class="headerlink" title="Graph Neutral Network和Graph Embedding"></a>Graph Neutral Network和Graph Embedding</h2><h3 id="0-概述"><a href="#0-概述" class="headerlink" title="0. 概述"></a>0. 概述</h3><h4 id="0-1-图的基础知识"><a href="#0-1-图的基础知识" class="headerlink" title="0.1 图的基础知识"></a>0.1 图的基础知识</h4><p>​        通常定义一个图 $G=(V,E)$，其中 $V$为 <strong>顶点（Vertices）</strong>集合，$E$为<strong>边（Edges）</strong>集合。对于一条边$e=(u,v)$ 包含两个<strong>端点（Endpoints）</strong> u 和 v，同时 u 可以称为 v 的<strong>邻居（Neighbor）</strong>。当所有的边为有向边时，图称之为<strong>有向（Directed）</strong>图，当所有边为无向边时，图称之为<strong>无向（Undirected）</strong>图。在无向图中，对于一个顶点 v，令$d(v) $表示连接的边的数量，称之为<strong>度（Degree）</strong>；有向图中又分为<strong>入度</strong>和<strong>出度</strong>。</p>
<p>​        对于一个无向图 $G=(V,E)$，其<strong>邻接矩阵（Adjacency Matrix）</strong>$A$通常定义为：</p>
<script type="math/tex; mode=display">
A_{ij} = \left\{  
             \begin{array}{**lr**}  
             1，i\not=j&if\ vi\ is\ adjacent\ to\ vj\\
             0, &otherwise 
             \end{array}  
\right.</script><p>​        <img src="/images/graph.png" alt="image-20200707174530868"></p>
<p>​        对于上面这个无向图，其度矩阵（degree matrix）如下。度矩阵$D$是一个对角矩阵。</p>
<p><img src="/images/image-20200707174724389.png" alt="image-20200707174724389"></p>
<p>​        图的拉普拉斯矩阵$L$定义为，在图论中，作为一个图的矩阵表示。拉普拉斯矩阵是对称的（Symmetric）。</p>
<script type="math/tex; mode=display">
L = D - A</script><p>​        另外一种更为常用的的是正则化的拉普拉斯矩阵（Symmetric normalized Laplacian）。</p>
<script type="math/tex; mode=display">
L^{sym} = D^{-\frac{1}{2}}LD^{-\frac{1}{2}} = I - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script><p>​        说明：$I$为单位矩阵；$D^{-\frac{1}{2}}$表示度矩阵对角线上的元素开平方根取倒数。</p>
<h4 id="0-2-二者不同"><a href="#0-2-二者不同" class="headerlink" title="0.2 二者不同"></a>0.2 二者不同</h4><p>​        图嵌入旨在通过保留图的网络<strong>拓扑结构和节点内容信息</strong>，将图中顶点表示为低维向量，以便使用简单的机器学习算法（例如，支持向量机分类）进行处理（摘自知乎：图神经网络（Graph Neural Networks）综述，作者：苏一。<a href="https://zhuanlan.zhihu.com/p/75307407）。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/75307407）。</a></p>
<p>​        图神经网络是用于处理图数据（非欧式空间）的神经网络结构。</p>
<p>​        图嵌入和图神经网络的区别与联系。</p>
<p><img src="https://d33wubrfki0l68.cloudfront.net/3c5b07652ee4f2012f3ce1b9cf25bf8282a4f9f4/abdc9/images/cn/2020-04-11-graph-embedding-and-gnn/graph-embedding-vs-graph-neural-networks.png" alt=""></p>
<h3 id="1-Graph-Embedding"><a href="#1-Graph-Embedding" class="headerlink" title="1. Graph Embedding"></a>1. Graph Embedding</h3><p>​        Embedding在数学上是一个函数，将一个空间的点映射到另一个空间，通常是从高维抽象的空间映射到低维具象的空间，并且在低维空间保持原有的语义。早在2003年，Bengio就发表论文论述word embedding想法，将词汇映射到实数向量空间。而2013年google连发两篇论文推出word embedding的神经网咯工具包：skip-gram、cbow（连续词袋模型），使得embedding技术成为深度学习的基本操作，进而导致万物皆可embedding。</p>
<p>​        而基于图的embedding又可以分为基于顶点（vertex）和基于图（graph）。前者主要是将给定的图数据中的vertex表示为单独的向量（vector），后者将整个图进行embedding表示，之后可以进行图的分类等工作。下面分别介绍。</p>
<h4 id="1-1-Vertex-Embedding"><a href="#1-1-Vertex-Embedding" class="headerlink" title="1.1 Vertex Embedding"></a>1.1 Vertex Embedding</h4><h5 id="1-1-1-DeepWalk"><a href="#1-1-1-DeepWalk" class="headerlink" title="1.1.1 DeepWalk"></a>1.1.1 DeepWalk</h5><p>Perozzi B, Al-Rfou R, Skiena S. Deepwalk: Online learning of social representations[C]//Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014: 701-710.</p>
<p><img src="/images/image-20200708200747355.png" alt="image-20200708200747355"></p>
<p>即：基于图上的随机游走进行节点采样，之后将采样到的节点集（每个节点feature使用one-hot表示或者随机向量）输入到skip-gram模型进行训练，得到节点的embedding。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RandomWalk</span><span class="params">(node,t)</span>:</span></span><br><span class="line">    walk = [node]        <span class="comment"># Walk starts from this node</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(t<span class="number">-1</span>):</span><br><span class="line">        <span class="comment"># 应先统计adj_list[node]对应的为1表示相邻的节点索引列表，从该列表中随机选取索引</span></span><br><span class="line">        adj_nodes = np.array(adj_list[node]).nonzero()[<span class="number">0</span>]<span class="comment"># nonzero返回一个元组</span></span><br><span class="line">        </span><br><span class="line">        node = adj_list[node][adj_nodes[random.randint(<span class="number">0</span>,len(adj_nodes)<span class="number">-1</span>)]]</span><br><span class="line">        walk.append(node)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> walk</span><br></pre></td></tr></table></figure>
<h5 id="1-1-2-LINE"><a href="#1-1-2-LINE" class="headerlink" title="1.1.2 LINE"></a>1.1.2 LINE</h5><p>Tang J, Qu M, Wang M, et al. Line: Large-scale information network embedding[C]//Proceedings of the 24th international conference on world wide web. 2015: 1067-1077.</p>
<p>​        相似度定义</p>
<p><strong>first-order proximity</strong></p>
<p>​        <img src="/images/image-20200709225053851.png" alt="image-20200709225053851"></p>
<p>​        1阶相似度用于描述图中成对顶点之间的局部相似度，形式化描述为若$u,v$之间存在直连边，则边权$w_{uv}$即为两个顶点的相似度，若不存在直连边，则1阶相似度为0。 如上图，6和7之间存在直连边，且边权较大，则认为两者相似且1阶相似度较高，而5和6之间不存在直连边，则两者间1阶相似度为0。</p>
<p><strong>second-order proximity</strong></p>
<p>​        仅有1阶相似度就够了吗？显然不够，如上图，虽然5和6之间不存在直连边，但是他们有很多相同的邻居顶点(1,2,3,4)，这其实也可以表明5和6是相似的，而2阶相似度就是用来描述这种关系的。 形式化定义为，令$p_u = (w_{u,1},…,w_{u,|V|})$ 表示顶点$u$与<strong>所有其他</strong>顶点间的1阶相似度，则  $u$ 与 $v$ 的2阶相似度可以通过 $p_{u}$ 和  $p_v$ 的相似度表示（两个顶点他们的邻居集合的相似程度）。若$u$与$v$之间不存在相同的邻居顶点，则2阶相似度为0。</p>
<p><strong>目标函数</strong></p>
<p>​        <strong>1st-order</strong>（用于无向图）</p>
<p>​        对于每条边集$E$中的任一条边$(i,j)$，邻接的两个节点$p_i,p_j$的联合分布定义为：</p>
<script type="math/tex; mode=display">
p(v_i,v_j) = \frac{1}{1 + \exp(-u_i·u_j)}</script><p>​        $u_i,u_j$即节点$v_i,v_j$的低维embedding表示。同时定义经验分布$\hat p$：</p>
<script type="math/tex; mode=display">
\hat p(i,j) = \frac{w_{ij}}{W},W = \sum_{(i,j)\in E}w_{ij}</script><p>​        那么，目标函数就是尽可能地减小这两个分布的差异，衡量两个分布差异可以使用KL散度来衡量，进而：</p>
<script type="math/tex; mode=display">
O_1 = - \sum_{(i,j)\in E}w_{ij}\log p(v_i,v_j)</script><p>​        <strong>2nd-order</strong></p>
<script type="math/tex; mode=display">
O_2 = - \sum_{(i,j)\in E} w_{ij}\log p(v_j|v_i)</script><p>​        当然也可以使用二者的结合。</p>
<h5 id="1-1-3-Node2Vec"><a href="#1-1-3-Node2Vec" class="headerlink" title="1.1.3 Node2Vec"></a>1.1.3 Node2Vec</h5><p>Grover A, Leskovec J. node2vec: Scalable feature learning for networks[C]//Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. 2016: 855-864.</p>
<p>​        本质上也是基于随机游走，提出了一种新的节点采样策略，有“导向”的游走，即加入了参数来控制从某个节点到其他节点的概率。采样得到的节点集，<strong>仍使用word2vec形式进行训练</strong>。</p>
<p>​        如下图。p（return parameter）、q（in-out parameter）为超参数。</p>
<p>​        下图中，现处于节点v，上一个节点是t，那么从节点v到节点$t$、$x_1$、$x_2$、$x_3$的概率$\pi_{vx}$计算公式如下：</p>
<script type="math/tex; mode=display">
\alpha(t,x) = \frac{1}{p},d_{t,x} = 0</script><script type="math/tex; mode=display">
\alpha(t,x) = 1,d_{t,x} = 1</script><script type="math/tex; mode=display">
\alpha(t,x) = \frac{1}{q},d_{t,x} = 2</script><script type="math/tex; mode=display">
\pi_{vx}=\alpha(t,x) w_{v,x}</script><p>​        其中，$d_{tx}$表示<strong>上一节点和下一个能到达的节点的距离</strong>，$w_{vx}$即节点v与next节点连接边的权重（无向图中为1）。下图中，节点$t$与$x_1$直接有边相连接，$d$为1；$t$与$x_2$、$x_3$距离为2（不相邻）；特别的，与上一个节点距离为0，概率为$\frac{1}{p}$。</p>
<p><img src="/images/image-20200709201700829.png" alt="image-20200709201700829"></p>
<p>​        基于这种随机游走策略，使得该模型可以体现网络的同质性（homophily）或结构性（structural equivalence）。网络的“同质性”指<strong>距离相近</strong>的节点的embedding应尽量相似；“结构性”指的是<strong>结构上相似</strong>的节点的embedding应尽量相似。</p>
<p>​        为了能让graph embedding的结果能够表达网络的“结构性”，需要让游走的过程更倾向与<strong>BFS</strong>，因为BFS会更多的在当前节点的领域中游走遍历；为了表达”同质性”，则需要让游走过程倾向于<strong>DFS</strong>。</p>
<p>​        现在讨论超参数p、q，q越大，则随机游走到远方节点的可能性更大，随机游走策略就近似于DFS；反之，近似于BFS。</p>
<h5 id="1-1-4-SDNE"><a href="#1-1-4-SDNE" class="headerlink" title="1.1.4 SDNE"></a>1.1.4 SDNE</h5><p>Wang D, Cui P, Zhu W. Structural deep network embedding[C]//Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. 2016: 1225-1234.</p>
<p>​        SDNE基于LINE研究的基础上，采用deep encoder-decoder模型进行embedding。</p>
<p>​        </p>
<h4 id="1-2-Graph-Embedding"><a href="#1-2-Graph-Embedding" class="headerlink" title="1.2 Graph Embedding"></a>1.2 Graph Embedding</h4><h3 id="2-Graph-Neural-Network"><a href="#2-Graph-Neural-Network" class="headerlink" title="2. Graph Neural Network"></a>2. Graph Neural Network</h3><p>​        图神经网络目前主要的任务包括：节点分类（node classification）、图分类（graph classification）、graph representation、link predication等等。</p>
<h4 id="2-1-Neighborhood-Aggravating"><a href="#2-1-Neighborhood-Aggravating" class="headerlink" title="2.1 Neighborhood Aggravating"></a>2.1 Neighborhood Aggravating</h4><p>​        <strong>neighborhood aggravating即用节点的neighbor feature更新下一个的hidden state。</strong></p>
<p>​        给定一张图，我们可以用与与节点邻接的节点集去表示该节点。下图中，A与B、C、D相邻接，而B又与A、C邻接等等，每个节点都可以使用与其相邻接的节点进行表示（aggravating）（每个节点都有一个初始状态【特征】，当前节点状态用其他节点的上一个状态【特征】表示）。通过其邻接节点聚合，这个节点就可以学到图的结构。</p>
<p><img src="/images/g1.png" alt="image-20200707215031730" style="zoom:67%;" /></p>
<p><img src="E:\hexo\themes\ayer\source\images\image-20200707215123250.png" alt="image-20200707215123250" style="zoom:67%;" /></p>
<p><img src="/images/image-20200707215155395.png" alt="image-20200707215155395" style="zoom:67%;" /></p>
<p>​        因此，我们可以定义任意层的网络来聚合各个节点的信息。（如下图，注：并非完整）</p>
<p><img src="/images/image-20200707215535544.png" alt="image-20200707215535544" style="zoom: 67%;" /></p>
<p>​        注：图中方块表示任意的聚合函数。</p>
<p>​        数学表示如下：</p>
<script type="math/tex; mode=display">
h_v^k = \sigma(W_k\sum_{u \in N(v)}\frac{h_u^{k-1}}{|N(v)|} + B_kh_v^{k-1}),k > 0</script><script type="math/tex; mode=display">
h_v^0 = x_v</script><p>​        其中，k表示第k层（layer）；v表示节点；$\sigma$表示激活函数；$h_u$表示节点$u$的状态；$N(v)$表示节点$v$的邻接节点集合；$|N(v)|$即邻接节点数量；$W、B$分别为权重矩阵和偏置（bias）。</p>
<p>​        因此，我们只需定义一个聚合函数（sum、mean、max-pooling等），以及损失函数（比如：基于节点分类的交叉熵损失函数等），就可以开始迭代训练，最终得到各个节点的embedding向量。</p>
<h4 id="2-2-Graph-Convolution-Networks"><a href="#2-2-Graph-Convolution-Networks" class="headerlink" title="2.2 Graph Convolution Networks"></a>2.2 Graph Convolution Networks</h4><p>​        卷积网络大致分类如下图。</p>
<p><img src="/images/image-20200707223107082.png" alt="image-20200707223107082"></p>
<h5 id="2-2-1-Spatial-based"><a href="#2-2-1-Spatial-based" class="headerlink" title="2.2.1 Spatial-based"></a>2.2.1 Spatial-based</h5><p>​        空间卷积网络也是基于neighborhood aggravating的思想。</p>
<ul>
<li><p>Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.</p>
<p>这篇论文提出了著名的GCN模型，采用的图卷积网络在neighborhood aggravating上做出了一点改动。</p>
</li>
</ul>
<script type="math/tex; mode=display">
h_v^k = \sigma(W_k\sum_{u\in N(v)\cup v}\frac{h_u^{k-1}}{\sqrt{|N(u)||N(v)|}})</script><p>​        <strong>矩阵形式</strong>如下：</p>
<script type="math/tex; mode=display">
H^{(k+1)} = \sigma(D^{-\frac{1}{2}}\hat AD^{\frac{1}{2}}H^{(k)}W_k) \\
\hat A = A + I</script><p>​        其中$A$为图邻接矩阵，$D$为度矩阵，$I$为单位矩阵。</p>
<p>​        下面看一下具体的矩阵形式</p>
<p><img src="/images/g2.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面这张图的邻接矩阵如下</span></span><br><span class="line">&gt; A = torch.tensor(np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]]))</span><br><span class="line"><span class="comment"># 度矩阵 D，即邻接矩阵每一行求和</span></span><br><span class="line">&gt; D = torch.diag(torch.sum(torch.tensor(adj),<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">&gt; X = torch.tensor(np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]))</span><br><span class="line"><span class="comment"># 每个节点的邻居节点聚合等价于A*X</span></span><br><span class="line">&gt; torch.mm(A,X)</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]], dtype=torch.int32)</span><br><span class="line"><span class="comment"># 从结果可以看出相当于对每个节点的所有邻居节点特征求和</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义权重矩阵 W，将input维度映射到output维度，本例input=3</span></span><br><span class="line">&gt; out = <span class="number">4</span></span><br><span class="line">&gt; W = W = torch.randn(<span class="number">3</span>,out)</span><br><span class="line"><span class="comment"># 完成聚合 AXW</span></span><br><span class="line">&gt; output = torch.mm(torch.mm(A,X).type_as(torch.FloatTensor()),W)</span><br><span class="line"><span class="comment"># 经过激活函数,得到这一阶段的hidden（下一阶段的输入）</span></span><br><span class="line">&gt; hidden = F.relu(output)</span><br><span class="line"><span class="comment"># hidden [num_of_vertex, hidden_dims]</span></span><br></pre></td></tr></table></figure>
<p>​        但是Kipf等人在他的论文中，对邻接矩阵$A$进行了标准化。即上文采用的公式。下面实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义图卷积</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCNConv</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, A, in_channels, out_channels)</span>:</span></span><br><span class="line">        super(GCNConv, self).__init__()</span><br><span class="line">        self.A_hat = A+torch.eye(A.size(<span class="number">0</span>))</span><br><span class="line">        self.D     = torch.diag(torch.sum(A,<span class="number">1</span>))</span><br><span class="line">        self.D     = self.D.inverse().sqrt() <span class="comment"># D是对角矩阵，对角线元素开根号</span></span><br><span class="line">        self.A_hat = torch.mm(torch.mm(self.D, self.A_hat), self.D)</span><br><span class="line">        self.W     = nn.Parameter(torch.rand(in_channels,out_channels, requires_grad=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        out = torch.relu(torch.mm(torch.mm(self.A_hat, X), self.W))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="comment"># 定义GCN  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,A, input_dims, nhid, out_dims)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = GCNConv(A,input_dims, nhid)</span><br><span class="line">        self.conv2 = GCNConv(A,nhid, out_dims)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,X)</span>:</span></span><br><span class="line">        H  = self.conv1(X)</span><br><span class="line">        H2 = self.conv2(H)</span><br><span class="line">        <span class="keyword">return</span> H2</span><br></pre></td></tr></table></figure>
<h5 id="2-2-2-Spectral-based"><a href="#2-2-2-Spectral-based" class="headerlink" title="2.2.2 Spectral-based"></a>2.2.2 Spectral-based</h5><p>​         谱图卷积网络基于图的信号处理，即图的拉普拉斯矩阵进行傅里叶变化（Fourier Transform）与逆变化（Inverse Fourier Transform），</p>
<p>​    <strong>图上的傅里叶变换</strong></p>
<p>​        傅里叶变换可以将<strong>时域</strong>信号转为<strong>频域信号</strong>。时域即信号大小随时间而改变，其图像在二维平面中，横轴为时间，纵轴为信号大小，其图像是连续的；而频域图像，横轴代表频率大小，纵轴代表信号大小，其图像是离散的。频域图像本质上描述了一段信号中包含的具体成分（不同频率信号的叠加）如何。</p>
<p>​        而图上的傅里叶变换用到了图的拉普拉斯矩阵$L$，因为$L$是半正定矩阵，因此其特征值都非负。对其进行特征值分解有：</p>
<script type="math/tex; mode=display">
L = U \Lambda U^T</script><p>，其中$U$为特征向量，$\Lambda$为特征值矩阵，是一个对角矩阵，$\lambda_1,\lambda_2…$即特征值，且$\lambda_1&lt;=\lambda_2&lt;=…&lt;=\lambda_n$。$\lambda$也代表了图上的频率。</p>
<p>​        下面直接给出图上傅里叶变换公式，具体推导略去。</p>
<p>​        给定图中某顶点$v$，其对应的信号（特征）为$x$，那么其傅里叶变换即：</p>
<script type="math/tex; mode=display">
\hat x = U^Tx</script><p>​        可知$\hat x$为频域上信号，要重新换变换顶点域，就需要逆傅里叶变换即：</p>
<script type="math/tex; mode=display">
y = U\hat x</script><p><strong>谱图卷积</strong></p>
<p>​        要进行卷积，就需要在频域上进行卷积，即将顶点上的信号进行傅里叶变换后，定义对应的滤波器（滤波函数）$g_\theta(\lambda)$对其进行处理，再利用逆傅里叶变换还原为顶点域中。这里的滤波器或者说关于$\lambda$的滤波函数就是我们要通过训练学习的。说是关于$\lambda$的函数，就是说根据不同的$\lambda$给出不同的<strong>相应</strong>$\theta$。</p>
<p>​        由此，图上信号卷积即：</p>
<script type="math/tex; mode=display">
y = U\hat x = Ug_{\theta}(\Lambda)U^Tx</script><p>​        但上面这样定义仍存在几个问题，一是当图太大时矩阵分解计算复杂度太高，二是这并非是局部的（localized）。进而又提出了ChebNet模型（Defferrard M, Bresson X, Vandergheynst P. Convolutional neural networks on graphs with fast localized spectral filtering[C]. Advances in neural information processing systems. 2016: 3844-3852.）。</p>
<p>​        第一类切比雪夫多项式定义如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
T_0(x) &= 1 \\
T_1(x) &= x \\
T_n(x) &= 2xT_{n-1}(x) - T_{n-2}(x)
\end{aligned}</script><p>​        代码参考：<a href="https://github.com/dovelism/Traffic_Data_Projection_Using_GCN/" target="_blank" rel="noopener">https://github.com/dovelism/Traffic_Data_Projection_Using_GCN/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChebConv</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    The ChebNet convolution operation.</span></span><br><span class="line"><span class="string">    :param in_c: int, number of input channels.</span></span><br><span class="line"><span class="string">    :param out_c: int, number of output channels.</span></span><br><span class="line"><span class="string">    :param K: int, the order of Chebyshev Polynomial.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, out_c, K, bias=True, normalize=True)</span>:</span></span><br><span class="line">        super(ChebConv, self).__init__()</span><br><span class="line">        self.normalize = normalize</span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(K + <span class="number">1</span>, <span class="number">1</span>, in_c, out_c))  <span class="comment"># [K+1, 1, in_c, out_c]</span></span><br><span class="line">        init.xavier_normal_(self.weight)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.Tensor(<span class="number">1</span>, <span class="number">1</span>, out_c))</span><br><span class="line">            init.zeros_(self.bias)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">"bias"</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        self.K = K + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, graph)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param inputs: the input data, [B, N, C]</span></span><br><span class="line"><span class="string">        :param graph: the graph structure, [N, N]</span></span><br><span class="line"><span class="string">        :return: convolution result, [B, N, D]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        L = ChebConv.get_laplacian(graph, self.normalize)  <span class="comment"># [N, N]</span></span><br><span class="line">        mul_L = self.cheb_polynomial(L).unsqueeze(<span class="number">1</span>)   <span class="comment"># [K, 1, N, N]</span></span><br><span class="line"></span><br><span class="line">        result = torch.matmul(mul_L, inputs)  <span class="comment"># [K, B, N, C]</span></span><br><span class="line">        result = torch.matmul(result, self.weight)  <span class="comment"># [K, B, N, D]</span></span><br><span class="line">        result = torch.sum(result, dim=<span class="number">0</span>) + self.bias  <span class="comment"># [B, N, D]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cheb_polynomial</span><span class="params">(self, laplacian)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute the Chebyshev Polynomial, according to the graph laplacian.</span></span><br><span class="line"><span class="string">        :param laplacian: the graph laplacian, [N, N].</span></span><br><span class="line"><span class="string">        :return: the multi order Chebyshev laplacian, [K, N, N].</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        N = laplacian.size(<span class="number">0</span>)  <span class="comment"># [N, N]</span></span><br><span class="line">        multi_order_laplacian = torch.zeros([self.K, N, N], device=laplacian.device, dtype=torch.float)  <span class="comment"># [K, N, N]</span></span><br><span class="line">        multi_order_laplacian[<span class="number">0</span>] = torch.eye(N, device=laplacian.device, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.K == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> multi_order_laplacian</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            multi_order_laplacian[<span class="number">1</span>] = laplacian</span><br><span class="line">            <span class="keyword">if</span> self.K == <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">return</span> multi_order_laplacian</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>, self.K):</span><br><span class="line">                    multi_order_laplacian[k] = <span class="number">2</span> * torch.mm(laplacian, multi_order_laplacian[k<span class="number">-1</span>]) - multi_order_laplacian[k<span class="number">-2</span>]</span><br><span class="line">        <span class="keyword">return</span> multi_order_laplacian</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_laplacian</span><span class="params">(graph, normalize)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        return the laplacian of the graph.</span></span><br><span class="line"><span class="string">        :param graph: the graph structure without self loop, [N, N].</span></span><br><span class="line"><span class="string">        :param normalize: whether to used the normalized laplacian.</span></span><br><span class="line"><span class="string">        :return: graph laplacian.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> normalize:</span><br><span class="line">            D = torch.diag(torch.sum(graph, dim=<span class="number">-1</span>) ** (<span class="number">-1</span> / <span class="number">2</span>))</span><br><span class="line">            L = torch.eye(graph.size(<span class="number">0</span>), device=graph.device, dtype=graph.dtype) - torch.mm(torch.mm(D, graph), D)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            D = torch.diag(torch.sum(graph, dim=<span class="number">-1</span>))</span><br><span class="line">            L = D - graph</span><br><span class="line">        <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChebNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, hid_c, out_c, K)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param in_c: int, number of input channels.</span></span><br><span class="line"><span class="string">        :param hid_c: int, number of hidden channels.</span></span><br><span class="line"><span class="string">        :param out_c: int, number of output channels.</span></span><br><span class="line"><span class="string">        :param K:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ChebNet, self).__init__()</span><br><span class="line">        self.conv1 = ChebConv(in_c=in_c, out_c=hid_c, K=K)</span><br><span class="line">        self.conv2 = ChebConv(in_c=hid_c, out_c=out_c, K=K)</span><br><span class="line">        self.act = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, data, device)</span>:</span></span><br><span class="line">        graph_data = data[<span class="string">"graph"</span>].to(device)[<span class="number">0</span>]  <span class="comment"># [N, N]</span></span><br><span class="line">        flow_x = data[<span class="string">"flow_x"</span>].to(device)  <span class="comment"># [B, N, H, D]</span></span><br><span class="line"></span><br><span class="line">        B, N = flow_x.size(<span class="number">0</span>), flow_x.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        flow_x = flow_x.view(B, N, <span class="number">-1</span>)  <span class="comment"># [B, N, H*D]</span></span><br><span class="line"></span><br><span class="line">        output_1 = self.act(self.conv1(flow_x, graph_data))</span><br><span class="line">        output_2 = self.act(self.conv2(output_1, graph_data))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_2.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, hid_c, out_c)</span>:</span></span><br><span class="line">        super(GCN, self).__init__()</span><br><span class="line">        self.linear_1 = nn.Linear(in_c, hid_c)</span><br><span class="line">        self.linear_2 = nn.Linear(hid_c, out_c)</span><br><span class="line">        self.act = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, data, device)</span>:</span></span><br><span class="line">        graph_data = data[<span class="string">"graph"</span>].to(device)[<span class="number">0</span>]  <span class="comment"># [N, N]</span></span><br><span class="line">        graph_data = GCN.process_graph(graph_data)</span><br><span class="line">        flow_x = data[<span class="string">"flow_x"</span>].to(device)  <span class="comment"># [B, N, H, D]</span></span><br><span class="line">        B, N = flow_x.size(<span class="number">0</span>), flow_x.size(<span class="number">1</span>)</span><br><span class="line">        flow_x = flow_x.view(B, N, <span class="number">-1</span>)  <span class="comment"># [B, N, H*D]  H = 6, D = 1</span></span><br><span class="line"></span><br><span class="line">        output_1 = self.linear_1(flow_x)  <span class="comment"># [B, N, hid_C]</span></span><br><span class="line">        output_1 = self.act(torch.matmul(graph_data, output_1))  <span class="comment"># [N, N], [B, N, Hid_C]</span></span><br><span class="line"></span><br><span class="line">        output_2 = self.linear_2(output_1)</span><br><span class="line">        output_2 = self.act(torch.matmul(graph_data, output_2))  <span class="comment"># [B, N, 1, Out_C]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_2.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_graph</span><span class="params">(graph_data)</span>:</span></span><br><span class="line">        N = graph_data.size(<span class="number">0</span>)</span><br><span class="line">        matrix_i = torch.eye(N, dtype=graph_data.dtype, device=graph_data.device)</span><br><span class="line">        graph_data += matrix_i  <span class="comment"># A~ [N, N]</span></span><br><span class="line"></span><br><span class="line">        degree_matrix = torch.sum(graph_data, dim=<span class="number">-1</span>, keepdim=<span class="literal">False</span>)  <span class="comment"># [N]</span></span><br><span class="line">        degree_matrix = degree_matrix.pow(<span class="number">-1</span>)</span><br><span class="line">        degree_matrix[degree_matrix == float(<span class="string">"inf"</span>)] = <span class="number">0.</span>  <span class="comment"># [N]</span></span><br><span class="line"></span><br><span class="line">        degree_matrix = torch.diag(degree_matrix)  <span class="comment"># [N, N]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.mm(degree_matrix, graph_data)  <span class="comment"># D^(-1) * A = \hat(A)</span></span><br></pre></td></tr></table></figure>
<p>​        在推出ChebNet后，Kipf等人进一步将公式化简，推出一阶（k=1）ChebNet模型（Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.），这就有了上面我们模型中采用的那个公式。</p>
<script type="math/tex; mode=display">
H^{(k+1)} = \sigma(D^{-\frac{1}{2}}\hat AD^{\frac{1}{2}}H^{(k)}W_k) \\
\hat A = A + I</script><h4 id="2-2-Graph-Attention-Networks"><a href="#2-2-Graph-Attention-Networks" class="headerlink" title="2.2 Graph Attention Networks"></a>2.2 Graph Attention Networks</h4><p>Veličković P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017.</p>
<p>​        attention通俗的讲就是输入两个向量，然后输出一个分数；是一种思想，可以有不同的实现。</p>
<p>​        GAT在空间卷积的基础上，引入了注意力机制，注意力机制也赋予了模型一定的可解释性。加入了attention后，我们aggravating的时候就需要计算当前节点和邻接节点的权重，然后进行聚合。</p>
<p>​        节点v第k个状态更新公式如下：</p>
<script type="math/tex; mode=display">
h_v^k = \sigma (\sum_{u \in N(v)}\alpha_{vu}h_u)</script><p>​        其中$\alpha$就是计算得到的attention权重。</p>
<p><img src="/images/image-20200707230726126.png" alt="image-20200707230726126" style="zoom:80%;" /></p>
<h4 id="2-3-Graph-SAGE"><a href="#2-3-Graph-SAGE" class="headerlink" title="2.3 Graph SAGE"></a>2.3 Graph SAGE</h4><p>Hamilton W, Ying Z, Leskovec J. Inductive representation learning on large graphs[C]. Advances in neural information processing systems. 2017: 1024-1034.</p>
<p>SAGE即Sample and Aggregate。GCN（Kipf.2016）模型存在如下缺点：</p>
<ul>
<li>对于静态图有效，当图结构发生改变（比如：节点的删除和更新）时，需重新训练模型。</li>
<li>当训练数据集非常时，内存无法全部加载图数据。</li>
</ul>
<p>该篇论文提出了名为inductive learning的模型，能更好的适应动态图，同时针对新加入的节点也可以快速的生成node embedding。具体思想是不是去学习一个固定的embedding，而是学习一个聚合函数（aggregator），学习到的聚合函数需满足对称性（symmetry property），因为对称性确保了模型可以被训练而且可以应用于任意顺序的顶点邻居特征集合上。常见的聚合函数有：mean、lstm、pooling等。</p>
<p><img src="/images/sage.png" alt=""></p>
<p>可以看出模型针对每个节点每次聚合固定数量的邻居，而不必把所有数据加载到内存中。</p>
<p><em>K**</em>是网络的层数，也代表着每个顶点能够聚合的邻接点的跳数，因为每增加一层，可以聚合更远的一层邻居的信息**。</p>
<p><img src="/images/khops.png" alt="image-20200920183029770" style="zoom:67%;" /></p>
<p>当新的节点加入到图中时，已经训练好的聚合函数（参数固定了）会聚合新节点的邻居然后生成该节点的embedding。</p>
<p>值得注意的是，当图中大量的节点有更新行为，图结构改变较大时，已经训练好的模型也需要重新训练，即模型退化为静态模型了。</p>
<p>具体实现代码参见：<a href="https://github.com/dsgiitr/graph_nets，具体架构就是三个组件：Aggregator、Encoder、SupervisedModel。Aggregator用来聚合邻居节点的信息，给定节点列表以及对应的邻居节点，首先进行采样然后使用feature函数进行聚合；Encoder包含图的一系列信息，比如邻接矩阵、采样大小等，根据GraphSAGE论文中提出的模型，需要concatenate自身节点信息与Aggregator聚合的邻居节点信息，然后矩阵相乘经过非线性激活函数完成对整个图节点的embedding；SupervisedModel即最终模型，包含一个encoder以及损失函数，完成最终的任务。代码中定义了2个aggregator和2个encoder，实际上是2层的一个编码网络（hops=2）。具体示意图如下：" target="_blank" rel="noopener">https://github.com/dsgiitr/graph_nets，具体架构就是三个组件：Aggregator、Encoder、SupervisedModel。Aggregator用来聚合邻居节点的信息，给定节点列表以及对应的邻居节点，首先进行采样然后使用feature函数进行聚合；Encoder包含图的一系列信息，比如邻接矩阵、采样大小等，根据GraphSAGE论文中提出的模型，需要concatenate自身节点信息与Aggregator聚合的邻居节点信息，然后矩阵相乘经过非线性激活函数完成对整个图节点的embedding；SupervisedModel即最终模型，包含一个encoder以及损失函数，完成最终的任务。代码中定义了2个aggregator和2个encoder，实际上是2层的一个编码网络（hops=2）。具体示意图如下：</a></p>
<p><img  src="/images/GraphSAGE.png" style="zoom:50%;" /></p>
<p>参考资料：</p>
<p>李宏毅，深度学习</p>
<p><a href="http://snap.stanford.edu/proj/embeddings-www/" target="_blank" rel="noopener">http://snap.stanford.edu/proj/embeddings-www/</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/56478167" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/56478167</a></p>
<p><a href="https://github.com/shenweichen/GraphEmbedding" target="_blank" rel="noopener">https://github.com/shenweichen/GraphEmbedding</a></p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-交叉熵" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/07/01/%E4%BA%A4%E5%8F%89%E7%86%B5/"
    >交叉熵</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/07/01/%E4%BA%A4%E5%8F%89%E7%86%B5/" class="article-date">
  <time datetime="2020-07-01T14:23:46.523Z" itemprop="datePublished">2020-07-01</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><ul>
<li><p>熵与交叉熵的定义：</p>
<p>信息论中熵的定义：</p>
<script type="math/tex; mode=display">
H(X) = -\sum_i p(x_i)\log p(x_i)</script><p>，其中$X$表示一个分布，$x_i$为该分布中的样本，$p(x_i)$即该样本的概率。</p>
<p><strong>在信息论中，熵是表示信息不确定性的度量。熵越大，表明信息的不确定性越大。</strong></p>
<p>两个分布$p$、$q$的交叉熵定义如下：</p>
<script type="math/tex; mode=display">
H(p,q) = -\sum_i p_i\log(q_i)</script></li>
<li><p>多分类问题中的交叉熵损失函数</p>
<p>在神经网路多分类任务时，最后一层采用$soft\max$层输出预测概率。</p>
<script type="math/tex; mode=display">
soft\max(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}</script><p>输出概率向量表示为$p$，训练$ont-hot$标签向量为$L$，交叉熵定义为$H(L,p)$。</p>
<p>则分类的损失函数定义为</p>
<script type="math/tex; mode=display">
J = \frac{1}{N}\sum_{i=1}^N H(L_i,p_i)</script><p>因为$L_i$、$q_i$表示一个样本，所以计算交叉熵即计算$-L_ilog(q_i)$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">p = np.array([<span class="number">0.2</span>,<span class="number">0.5</span>,<span class="number">0.3</span>])</span><br><span class="line">target = np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">H = <span class="number">-1</span> * np.sum(target * np.log(p)) / <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>当分类为<strong>二分类时</strong>，最后一层通常采用$sigmod$函数（逻辑斯蒂”回归”）。</p>
<script type="math/tex; mode=display">
sigmod(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">
J = -\frac{1}{N}\sum_{i=1}^N [y_i\log(\hat y_i) +(1-y_i)\log(1-\hat y_i)]</script><script type="math/tex; mode=display">
\hat y(x) = \frac{1}{1 + e^{-W*x}}</script><p>即为交叉熵损失函数的一个特殊形式。</p>
<p>采用交叉熵损失函数的原因是该损失函数为凸函数，从而可以进行凸优化。</p>
</li>
<li><p>KL（Kullback–Leibler）散度（Divergence）和JS（Jensen–Shannon）散度</p>
<p>KL散度用来衡量两个概率分布$P$、$Q$的<strong>差异</strong>。</p>
<script type="math/tex; mode=display">
D_{KL}(P||Q)= \sum P\log(\frac{P}{Q}) = -H(P) + H(P,Q)</script><p>注意：$D_{KL}(P||Q) \not= D_{KL}(Q||P)$。KL散度是不对称的！</p>
<p>JS散度定义如下：</p>
<script type="math/tex; mode=display">
JSD(P||Q) = \frac{1}{2}D_{KL}(P||M) + \frac{1}{2}D_{KL}(Q||M)</script><script type="math/tex; mode=display">
M = \frac{1}{2}(P + Q)</script><p>JS散度是对称的。</p>
</li>
</ul>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Dropout理解与实现" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/06/30/Dropout%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/"
    >Dropout理解与实现</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/06/30/Dropout%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/" class="article-date">
  <time datetime="2020-06-30T12:34:33.033Z" itemprop="datePublished">2020-06-30</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="Dropout理解与实现"><a href="#Dropout理解与实现" class="headerlink" title="Dropout理解与实现"></a>Dropout理解与实现</h2><ul>
<li><p>作用：</p>
<p>正则化的一种手段，训练过程中避免神经网络的过拟合。</p>
<p>在某一层中，随机使一部分神经元失活（输出为0），<strong>导致这部分神经元对下一层输入的贡献为0</strong>。</p>
<p>数学表达如下：</p>
<p>第$l + 1$层的输入：</p>
<script type="math/tex; mode=display">
z_i^{(l+1)} = w_i^{(l+1)}y^l + b_i^{(l+1)}</script><script type="math/tex; mode=display">
y_i^{(l+1)} = f(z_i^{(l+1)})</script><p>$f$为激活函数。</p>
<p>若在该层使用dropout，则：</p>
<script type="math/tex; mode=display">
r^{(l+1)} = Bernouli(p)</script><script type="math/tex; mode=display">
\hat y^{(l+1)} = r^{(l+1)} * y_i^{(l+1)}</script><p>$r^{(l+1)}$是一个mask向量，只包含0、1，其中为0表示该神经元失活了。$\hat y^{(l+1)}$即作为下一层的输入。</p>
<p><img src="/images/dropout.png" alt=""></p>
<p>即等价于：</p>
<p><img src="https://pic2.zhimg.com/v2-64930dc0337f42bcdbec488fd5337e95_r.jpg" alt=""></p>
</li>
<li><p>实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">	x: 输入</span></span><br><span class="line"><span class="string">	keep_prob: 留存概率</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, keep_prob)</span>:</span></span><br><span class="line">    mask = np.random.binomial(<span class="number">1</span>, keep_prob, size=x.shape)</span><br><span class="line">    x *= mask</span><br><span class="line">    x = x / keep_prob <span class="comment"># 对余下的神经元进行rescale</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, W1, W2, W3, training=False)</span>:</span></span><br><span class="line">    z1 = np.dot(x, W1)</span><br><span class="line">    y1 = np.tanh(z1)</span><br><span class="line">    z2 = np.dot(y1, W2)</span><br><span class="line">    y2 = np.tanh(z2)</span><br><span class="line">    <span class="comment"># Dropout in layer 2 </span></span><br><span class="line">    <span class="keyword">if</span> training:</span><br><span class="line">        m2 = np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>, size=z2.shape) <span class="comment"># 生成mask</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        m2 = <span class="number">0.5</span> <span class="comment"># 训练中没有rescale，测试时需要平衡训练中失活的神经元数量</span></span><br><span class="line">    y2 *= m2 <span class="comment"># 乘以mask，为0即代表失活</span></span><br><span class="line">    z3 = np.dot(y2, W3)</span><br><span class="line">    y3 = z3 <span class="comment"># linear output</span></span><br><span class="line">    <span class="keyword">return</span> y1, y2, y3, m2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, W1, W2, W3, training=False)</span>:</span></span><br><span class="line">    z1 = np.dot(x, W1)</span><br><span class="line">    y1 = np.tanh(z1)</span><br><span class="line">    z2 = np.dot(y1, W2)</span><br><span class="line">    y2 = np.tanh(z2)</span><br><span class="line">    <span class="comment"># Dropout in layer 2</span></span><br><span class="line">    <span class="keyword">if</span> training:</span><br><span class="line">   		y2 = dropout(y2,<span class="number">0.5</span>) <span class="comment"># 训练阶段已经进行了rescale</span></span><br><span class="line">    z3 = np.dot(y2, W3)</span><br><span class="line">    y3 = z3 <span class="comment"># linear output</span></span><br><span class="line">    <span class="keyword">return</span> y1, y2, y3, m2</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意：</p>
<p>测试中不需要dropout。dropout一般用于全连接层之前，对卷积层的效果一般。</p>
</li>
</ul>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Flink流式处理框架学习" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/06/29/Flink%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/"
    >Flink流式处理框架学习</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/06/29/Flink%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2020-06-29T14:46:15.156Z" itemprop="datePublished">2020-06-29</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="Flink流式处理框架学习入门"><a href="#Flink流式处理框架学习入门" class="headerlink" title="Flink流式处理框架学习入门"></a>Flink流式处理框架学习入门</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><h3 id="2-安装与部署"><a href="#2-安装与部署" class="headerlink" title="2. 安装与部署"></a>2. 安装与部署</h3><h3 id="3-Source与Sink"><a href="#3-Source与Sink" class="headerlink" title="3. Source与Sink"></a>3. Source与Sink</h3><h3 id="4-DataStream编程模型API"><a href="#4-DataStream编程模型API" class="headerlink" title="4. DataStream编程模型API"></a>4. DataStream编程模型API</h3><h3 id="5-Checkpoint与State状态管理"><a href="#5-Checkpoint与State状态管理" class="headerlink" title="5. Checkpoint与State状态管理"></a>5. Checkpoint与State状态管理</h3><h3 id="6-Flink中的Windows"><a href="#6-Flink中的Windows" class="headerlink" title="6. Flink中的Windows"></a>6. Flink中的Windows</h3><h3 id="7-Flink中的Time"><a href="#7-Flink中的Time" class="headerlink" title="7.  Flink中的Time"></a>7.  Flink中的Time</h3>
      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
  </article>
  

  
  <nav class="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2019-2020
        张永剑
      </li>
      <li>
        
          Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
      <aside class="sidebar">
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="张永剑的博客"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>







<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>



<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>