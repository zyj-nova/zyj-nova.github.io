<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="description" content="天空如此辽阔，大地不过是必经之路" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     张永剑的博客
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<link rel="alternate" href="/atom.xml" title="张永剑的博客" type="application/atom+xml">
</head>

</html>

<body>
  <div id="app">
    <main class="content">
      
<div id="main">
  <section class="outer">
  <article class="articles">
    
    
    
    
    <article id="post-Strang教授线性代数笔记" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/08/27/Strang%E6%95%99%E6%8E%88%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%AC%94%E8%AE%B0/"
    >Strang教授线性代数笔记</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/08/27/Strang%E6%95%99%E6%8E%88%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2020-08-27T03:47:51.456Z" itemprop="datePublished">2020-08-27</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="Strang-教授线性代数笔记"><a href="#Strang-教授线性代数笔记" class="headerlink" title="Strang 教授线性代数笔记"></a>Strang 教授线性代数笔记</h2><p><strong>说明</strong>：单位矩阵：$I$；声明的向量都是列向量，$\vec a$、$\pmb a$都表示列向量。</p>
<p><strong>课程视频</strong>：<a href="https://www.youtube.com/playlist?list=PL6839449936471E0C" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PL6839449936471E0C</a></p>
<p>pdf版参见：<a href="https://github.com/zyj-nova/LinearAlgebraNotes" target="_blank" rel="noopener">https://github.com/zyj-nova/LinearAlgebraNotes</a></p>
<h3 id="1-矩阵消元、矩阵的秩与转置（基础）"><a href="#1-矩阵消元、矩阵的秩与转置（基础）" class="headerlink" title="1.  矩阵消元、矩阵的秩与转置（基础）"></a>1.  矩阵消元、矩阵的秩与转置（基础）</h3><h4 id="1-1-矩阵的初等变换"><a href="#1-1-矩阵的初等变换" class="headerlink" title="1.1 矩阵的初等变换"></a>1.1 矩阵的初等变换</h4><p><strong>矩阵初等行变换</strong>：</p>
<ul>
<li><p>对换两行</p>
</li>
<li><p>以数$k \not= 0$乘某一行中所有元（数乘）</p>
</li>
<li><p>把某一行所有元的$k$倍加到另一行对应的元上去</p>
<p>把“行”换成“列”，即为矩阵的初等列变换。矩阵的初等行变换和初等列变换统称为<strong>初等变换</strong>。</p>
</li>
</ul>
<p><strong>初等矩阵</strong>：由单位矩阵$I$经过一次初等变换得到的矩阵成为初等矩阵。</p>
<p><strong>性质</strong>：设矩阵$A$是一个 $m \times n$ 矩阵，则矩阵$A$的初等行变换可以表示为<strong>左乘</strong>多个 $m$ 阶初等矩阵$E$，即：</p>
<p>$E_1E_2···E_nA$；矩阵$A$的初等列变换可以表示为<strong>右乘</strong>多个 $n$ 阶初等矩阵$E$，即：$AE_1E_2···E_n$。</p>
<p>注意：这里的左右指矩阵$A$的左右。</p>
<p><strong>例子</strong>：</p>
<p>$A=\left[<br> \begin{matrix}<br>   a &amp; b\\<br>   c &amp; d<br>  \end{matrix}<br>\right]$，交换单位阵的两行得到初等矩阵 $E=\left[<br> \begin{matrix}<br>   0 &amp; 1\\<br>   1 &amp; 0<br>  \end{matrix}<br>\right]$，则$EA = \left[ \begin{matrix} c &amp; d\ a &amp; b\end{matrix} \right]$，即交换了矩阵$A$的两行。</p>
<p>$E$也可以视为交换单位阵的两列得到，那么$AE=\left[ \begin{matrix} b &amp; a\ d &amp; c\end{matrix}\right]$，即交换$A$的两列。</p>
<p><strong>行阶梯形矩阵</strong></p>
<p>非零矩阵满足：</p>
<ol>
<li>非零行在零行（一行全为0）的上面</li>
<li>非零行的第一个非零元素所在列在上一行（如果存在）的第一个非零元素所在列的右面。</li>
</ol>
<p>例如：$B = \begin{pmatrix} 1 &amp;1 &amp;-2 &amp;1 &amp;4 \ 0&amp; 1 &amp;-1 &amp;1 &amp;0 \ 0&amp; 0 &amp;0 &amp;1&amp; -3\ 0&amp; 0 &amp;0 &amp;0 &amp;0 \end{pmatrix}$</p>
<p><strong>行最简形矩阵</strong></p>
<p>在阶梯形矩阵中，若<strong>非零行的第一个非零元素全是1，且非零行的第一个元素1所在列的其余元素全为零</strong>，就称该矩阵为行最简形矩阵。</p>
<p>利用<strong>初等行变换</strong>，把一个矩阵化为行阶梯形矩阵和行最简形矩阵，是一种很重要的运算。</p>
<p>例如对上述行阶梯矩阵 $B$ 进行<strong>初等行变换</strong>得到：$\begin{pmatrix} 1 &amp; 0 &amp; -1 &amp; 0 &amp; 4 \ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 3 \ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -3 \ 0 &amp;0&amp;0&amp;0&amp;0\end{pmatrix}$</p>
<p><strong>行最简形矩阵(或者行阶梯形矩阵)的非零行个数即为矩阵的</strong>秩<strong>（rank）</strong>。</p>
<p>若通过<strong>初等行变换</strong>可以得到形如$R = \begin{pmatrix} I_{r\times r} &amp; F_{r\times (n-r)} \ O &amp; O \end{pmatrix}$的简化矩阵（上述$B$就不符合以下情况），那么就有如下结论：</p>
<p><strong>很重要!!!!</strong>：$RN =0 \Rightarrow N = \begin{pmatrix} -F \\I \end{pmatrix}$，$I$的阶数等于自由变量的个数。可以看出对系数矩阵初等变换为形如$R$，解向量就可以写成$N$的形式。</p>
<p>接着考虑$\begin{pmatrix} I &amp; F \end{pmatrix}\begin{pmatrix} x_{pivot} \ x_{free} \end{pmatrix} = 0 \Rightarrow x_{pivot} = -Fx_{free}$</p>
<p><strong>利用初等行变换求方阵的逆矩阵</strong>（高斯-约旦消元法）（Lecture 10的内容）</p>
<p>$A$为可逆 $n$ 阶<strong>方阵</strong>，将矩阵$(A,I)$经过初等<strong>行</strong>变换可以得到$(I,E)$，$I$ 为单位矩阵，则$E$为矩阵的逆矩阵。实际上，矩阵$E$是由单位阵变换而来的，在对$(A,I)$进行行变换的过程中，$E$记录了整个变换过程。</p>
<script type="math/tex; mode=display">
R(A,I) = (I,E)\\
RA=I,RI=E \Rightarrow R = E = A^{-1}</script><p>$R$表示初等行变换矩阵。</p>
<p><strong>从初等变换角度理解矩阵乘法</strong>：$A_{m \times n} B_{n \times p} = C_{m \times p}$</p>
<p>从$A$的角度，相当于$A$右乘矩阵$B$中的每个列向量，$C$中每列都是$A$中列向量的线性组合。</p>
<p>从$B$的角度，相当于$B$左乘$A$中每个行向量，$C$中每行都是$B$中行向量的线性组合。</p>
<p><strong>注意：</strong>矩阵的初等行变化不会改变线性方程组的解，<strong>初等列变换会改变解</strong>（交换两列相当于把两个未知数的系数交换了。方程组也就变了）。</p>
<h4 id="1-2-矩阵的转置"><a href="#1-2-矩阵的转置" class="headerlink" title="1.2 矩阵的转置"></a>1.2 矩阵的转置</h4><p>$A$为$m \times n$ 的矩阵，$A^T$称为$A$的转置矩阵，且$(A)_{ij} = (A^T)_{ji}$。（行变列，列变行）。</p>
<p><strong>性质</strong>：</p>
<ul>
<li>$(A + B)^T = A^T + B^T$</li>
<li>$(\lambda A)^T = \lambda A^T$</li>
<li>$(AB)^T = B^TA^T$</li>
<li><strong>若 $A$ 为方阵，且$A = A^T$，则 $A$ 为对称矩阵</strong></li>
<li>$A^TA$是一个$n$阶对称阵，$(A^TA)^T = A^T(A^T)^T = A^TA$</li>
</ul>
<h4 id="1-3-矩阵的逆"><a href="#1-3-矩阵的逆" class="headerlink" title="1.3 矩阵的逆"></a>1.3 矩阵的逆</h4><p>定义：$AB = BA = I$，则$A$、$B$互为逆矩阵。</p>
<p><strong>可逆矩阵又称为非奇异矩阵，不可逆矩阵被称为奇异矩阵</strong>。</p>
<p><strong>方阵$A$可逆的充要条件是$|A|\not=0$</strong>。</p>
<p><strong>方阵A可逆的充要条件是A可以通过初等行变换得到单位阵</strong>。</p>
<ul>
<li>证明：A可逆 $\Leftrightarrow$ 存在可逆矩阵 $P$，$PA = I$ $\Leftrightarrow$ $A$ 行变换得到 $I$</li>
</ul>
<p><strong>逆矩阵的性质</strong></p>
<p>若$n$阶矩阵$A$可逆，有</p>
<ul>
<li><p>$(A^{-1})^{-1} = A$</p>
</li>
<li><p>数$\lambda \not= 0$，$(\lambda A)^{-1} = \frac{A}{\lambda}$</p>
</li>
<li><p>$B$ 与$A$ 同阶数，且可逆，$(AB)^{-1} = B^{-1}A^{-1}$</p>
<p>证明：$P(AB) = I \Rightarrow$ $PABB^{-1} = IB^{-1} \Rightarrow PA = B^{-1} \Rightarrow P = B^{-1}A^{-1}$</p>
</li>
<li><p>$A^T$亦可逆，且$(A^T)^{-1} = (A^{-1})^T$</p>
</li>
<li><p>$|A| \not= 0$</p>
</li>
<li><p>$|A^{-1}| = \frac{1}{|A|}$</p>
<p>证明：$A$可逆 $\Rightarrow AA^{-1} = E \Rightarrow |A|*|A^{-1}| = |E| = 1 \Rightarrow |A^{-1}| = \frac{1}{|A|} $</p>
</li>
<li><p>$A$满秩矩阵。由于$|A|\not= 0$，则$A$的最高阶子式为自身且不为0，因此$R(A) = n$，即满秩。</p>
</li>
</ul>
<h4 id="1-4-矩阵与向量乘法"><a href="#1-4-矩阵与向量乘法" class="headerlink" title="1.4 矩阵与向量乘法"></a>1.4 矩阵与向量乘法</h4><ul>
<li>$Ax$相当于$A$的<strong>列向量</strong>的线性组合，结果是列向量 $x_1c_1 + x_2c_2 + …$ 对应列乘以$\vec x$的分量然后所有列求和</li>
<li>$xA$相当于$A$的<strong>行向量</strong>的线性组合，结果是行向量 $x_1r_1 + x_2r_2+ …$对应行乘以$\vec x$的分量然后所有行求和</li>
</ul>
<h4 id="1-5-矩阵的秩"><a href="#1-5-矩阵的秩" class="headerlink" title="1.5 矩阵的秩"></a>1.5 矩阵的秩</h4><h3 id="2-向量空间、列空间、零空间（进阶）"><a href="#2-向量空间、列空间、零空间（进阶）" class="headerlink" title="2. 向量空间、列空间、零空间（进阶）"></a>2. 向量空间、列空间、零空间（进阶）</h3><p>给定$m \times n$的矩阵$A$</p>
<h4 id="2-1-向量空间的定义（只讨论实数）"><a href="#2-1-向量空间的定义（只讨论实数）" class="headerlink" title="2.1 向量空间的定义（只讨论实数）"></a>2.1 向量空间的定义（只讨论实数）</h4><p><strong>定义</strong>：以向量为元素的集合$V$称为向量空间（vector space）。</p>
<p><strong>向量空间必须满足</strong>：</p>
<ul>
<li>对加法运算封闭，任意两个向量相加得到的向量仍在空间内</li>
<li>对数乘运算封闭，乘以任意实数仍在空间内</li>
<li>包含一个零向量（当非零向量数乘0得到零向量，因此必须包含）</li>
</ul>
<p>$\R^n$表示含有 $n$ 个元素的向量且基向量个数为$n$（维度为$n$）组成的向量空间。</p>
<p><strong>子空间（subspace）</strong> 满足以上三条性质却又不包含所有向量的空间。（向量空间内的向量空间）</p>
<p>例子：$\R^2$的子空间：整个二维平面；二维平面内，<strong>过原点</strong>的任意直线；零向量$\vec 0$ = $(0,0)$。</p>
<p>向量空间$P$，两个$S$中子空间$S,I$。<strong>两个子空间的并并不是一个子空间，两个子空间的交是一个子空间。</strong></p>
<p>即：$S \cap I \in P, S \cup I \notin P$.</p>
<h4 id="2-2-列空间"><a href="#2-2-列空间" class="headerlink" title="2.2 列空间"></a>2.2 列空间</h4><p><strong>定义</strong>：矩阵$A$的列向量的<strong>线性组合</strong>（数乘和加法运算）构成了矩阵$A$的列空间，记为$C(A)$。</p>
<p>同理，定义矩阵$A$的行向量的线性组合，构成了$A$的行空间，记为$R(A)$。</p>
<p>考虑$A=\begin{pmatrix} 1&amp;3\\2&amp;3\\4&amp;1\end{pmatrix}$，$C(A)$构成了三维空间中的一个过原点的<strong>平面</strong>。</p>
<p>考虑$A = \begin{pmatrix}1&amp;1&amp;2\\2&amp;1&amp;3\\3&amp;1&amp;4\\4&amp;1&amp;5 \end{pmatrix}$，$C(A)$是$\R^4$的一个子空间（每一列含有4个元素）。$C(A)$的维数为2，是四维空间的一个平面（第三列是前两列的线性组合，因此前两列作为列空间的基）。</p>
<p>考虑线性方程组$Ax=b$，是否始终有解？答案是当$\vec b$不在$C(A)$内时，方程无解。因为$Ax$相当于矩阵列向量的线性组合，若$\vec b$不在列空间，必然无解。只有$b$是$A$中各列的线性组合时，即$\vec b \in C(A)$，方程才有解。</p>
<h4 id="2-3-零空间（核）null-space"><a href="#2-3-零空间（核）null-space" class="headerlink" title="2.3 零空间（核）null space"></a>2.3 零空间（核）null space</h4><p>定义：$Null(A) = \{x | Ax=0，x \in \R^n \}$，齐次线性方程组的解构成了$A$的零空间。</p>
<p><strong>定理</strong>：零空间的维数$\dim N(A) = n - R(A) = n - \dim C(A)$，$n$为列数；<strong>也就是自由变量的个数</strong>，主元的个数就是矩阵的秩。</p>
<h3 id="3-向量组的线性相关性、基、维数"><a href="#3-向量组的线性相关性、基、维数" class="headerlink" title="3. 向量组的线性相关性、基、维数"></a>3. 向量组的线性相关性、基、维数</h3><h5 id="3-1-向量组的线性相关性（independence）"><a href="#3-1-向量组的线性相关性（independence）" class="headerlink" title="3.1 向量组的线性相关性（independence）"></a>3.1 向量组的线性相关性（independence）</h5><p><strong>定义</strong>：给定向量组$A:\pmb{a_1},\pmb{a_2},···,\pmb{a_m}$，如果存在<strong>不全为0</strong>的数$k_1,k_2,···,k_m$，使</p>
<script type="math/tex; mode=display">
k_1\pmb{a_1} + k_2\pmb{a_2} + ··· + k_m\pmb{a_m} = \pmb{0}</script><p>则称向量组$A$是线性相关的（linearly dependent），否则称它线性无关（linearly independent）。</p>
<p>若向量组线性无关，则向量组向量的线性组合始终不为$\vec 0$（不包括系数全为0的情况）</p>
<p><strong>向量组能线性组合得到零向量：相关；线性组合得不到零向量：无关。</strong></p>
<p>随之可以与线性方程组的解联系起来。设向量组构成的矩阵$A_{ n \times m}$，线性方程组$Ax=\vec 0$，$x \in \R^m$</p>
<ul>
<li><strong>线性相关</strong>：$N(A)$有非零解。$R(A) &lt; m$，含有自由变量。</li>
<li><strong>线性无关</strong>：$N(A)$只包含一个零向量。$R(A) = m$，所有分量都是主元，没有自由变量。</li>
</ul>
<h5 id="3-2-基、维数"><a href="#3-2-基、维数" class="headerlink" title="3.2 基、维数"></a>3.2 基、维数</h5><p><strong>定理：</strong>对于给定的向量空间，基向量的个数都是一样的。</p>
<p>向量空间$V$中一组线性无关的向量组称为$V$的一个基，<strong>基向量的个数称为向量空间的维数</strong>。</p>
<p>因此我们说$A=\begin{pmatrix} 1&amp;3\\2&amp;3\\4&amp;1\end{pmatrix}$的列空间$C(A)$是三维空间的一个平面，因为$\dim C(A) = 2$。</p>
<p>“基向量的个数不多也不少。”</p>
<p><strong>基向量的性质</strong>：</p>
<ul>
<li>线性无关</li>
<li>线性组合生成(spanning)整个空间</li>
</ul>
<p><strong>若向量组构成的矩阵为方阵，若向量组线性无关，则方阵可逆（满秩矩阵$|A|\not = 0$）。</strong></p>
<hr>
<p><strong>例子：</strong>求矩阵$A = \begin{pmatrix} 1&amp;0&amp;1\\1&amp;-2&amp;-1\\2&amp;1&amp;3\\1&amp;0&amp;1\end{pmatrix}$的<strong>零空间</strong>的一组（个）基（a basis）。</p>
<p>就是求$Ax=0$的一个解。</p>
<p>对其进行行变换最终得到：$\begin{pmatrix}1&amp;0&amp;1\\0&amp;1&amp;1\\0&amp;0&amp;0\\0&amp;0&amp;0 \end{pmatrix}$，可知向量$\begin{pmatrix} -1\-1\\1\end{pmatrix}$可以作为$N(A)$的一个基向量（<strong>不需要乘常数</strong>）</p>
<p>基向量个数为1，因此$\dim N(A) = 1, R(A) = 2$。</p>
<hr>
<p><strong>注意！！！！</strong>教授称之为<strong>Great theorem</strong>的：</p>
<p><strong>矩阵的秩 = 主元个数 = 矩阵列空间的维数</strong>，因为<strong>主列可以构成一组基</strong>，所以主元个数就是列空间维数。</p>
<hr>
<p><strong>例子：</strong>$A = \begin{pmatrix} 1&amp;2&amp;3&amp;1\\1&amp;1&amp;2&amp;1\\1&amp;2&amp;3&amp;1\end{pmatrix}$，显然第一列与第二列线性无关可以作为列空间的一组基。经过行变换可以得到</p>
<script type="math/tex; mode=display">
\begin{pmatrix}1 &0& 1 &1\\0& 1 &1& 0\\0&0&0&0\end{pmatrix}</script><p>可以看出第一列与第二列是主列，$R(A) = 2=$主元个数。</p>
<hr>
<h3 id="4-线性方程组的解"><a href="#4-线性方程组的解" class="headerlink" title="4. 线性方程组的解"></a>4. 线性方程组的解</h3><h4 id="4-1-齐次线性方程组"><a href="#4-1-齐次线性方程组" class="headerlink" title="4.1 齐次线性方程组"></a>4.1 齐次线性方程组</h4><p>考虑下列方程组：</p>
<script type="math/tex; mode=display">
a_{11}x_1 + a_{12}x_2 + a_{13}x3 = 0 \\
a_{21}x_1 + a_{22}x_2 + a_{23}x3 = 0 \\
a_{31}x_1 + a_{32}x_2 + a_{33}x3 = 0</script><p>可以写成矩阵形式：</p>
<script type="math/tex; mode=display">
x_1\begin{pmatrix} a_{11}\\ a_{21}\\ a_{31}\end{pmatrix} + x_2\begin{pmatrix} a_{12}\\ a_{22}\\ a_{32}\end{pmatrix} + x_3\begin{pmatrix} a_{13}\\ a_{23}\\ a_{33}\end{pmatrix} =b</script><script type="math/tex; mode=display">
Ax = 0</script><p>其中$A=\begin{pmatrix} a_{11} &amp; a_{12}&amp; a_{13}\ a_{21} &amp; a_{22} &amp; a_{23} \ a_{31} &amp; a_{32} &amp; a_{33}\end{pmatrix}$，$\vec x=\begin{pmatrix} x_1 \ x_2 \ x_3\end{pmatrix}$</p>
<p>对$A$进行初等变换为行阶梯形矩阵，那么非零行<strong>第一个非零元素对应的列</strong>对应的未知变量我们称为<strong>主元</strong>（pivot），其余列称为<strong>自由变量</strong>。 </p>
<p>例子：若方程系数矩阵$A = \begin{pmatrix} 1 &amp; 1 &amp;-2 &amp; 1 &amp; 4 \ 0&amp; 1 &amp;-1 &amp;1 &amp;0 \ 0&amp; 0 &amp;0 &amp;1&amp; -3\ 0&amp; 0 &amp;0 &amp;0 &amp;0 \end{pmatrix}$，那么主元就是$x_1,x_2,x_4$，自由变量就是$x_3,x_5$。</p>
<ul>
<li>例子：给定$A$，求$Ax=0$.</li>
</ul>
<script type="math/tex; mode=display">
A=\begin{pmatrix} 1&2&3 \\ 2&4&6 \\2&6&8 \\ 2&8&10 \end{pmatrix}</script><script type="math/tex; mode=display">
A  ～ \begin{pmatrix} 1&2&3 \\ 0&0&0 \\ 0 &2& 2 \\ 0& 4 &4\end{pmatrix} ～ \begin{pmatrix} 1&0&1 \\ 0&1&1 \\ 0 &0& 0 \\ 0& 0 &0\end{pmatrix}</script><p>可知$x_1,x_2$为主元，$x_3$为自由变量，令$x_3=1$（为0时解为0向量无意义），得到$x = (-1,-1,1)^T$。</p>
<p>记得初等变换讲过的解的形式：$N = \begin{pmatrix} -F \\I \end{pmatrix}$，这儿的$I$为1阶，即只有元素$1$。$I$ 的阶数等于自由变量的个数。</p>
<p>$N(A) = c\pmb x,c\in R$ 。<strong>注意</strong>：当求零空间的基向量时才是$\pmb x$。任意倍的$\vec x$生成了零空间。</p>
<p><strong>求解步骤</strong>：</p>
<ul>
<li><strong>将自由变量依次设置为1，其余自由变量为0</strong>；带入$Ax=0$求出主元的值</li>
<li>有几个自由变量，零空间就有几个向量线性组合生成</li>
<li>自由变量个数$n - R(A)$</li>
</ul>
<h4 id="4-2-非齐次线性方程组"><a href="#4-2-非齐次线性方程组" class="headerlink" title="4.2 非齐次线性方程组"></a>4.2 非齐次线性方程组</h4><script type="math/tex; mode=display">
A\pmb x = \pmb b</script><p>增广矩阵 $(A,\pmb b)$，若$R(A) = R(A,\pmb b)$ $\Leftrightarrow$ 线性方程组有解。</p>
<p><strong>求解步骤：</strong></p>
<ul>
<li><p>将所有自由变量设为0，带入$Ax=b$，求解主元的值，得到一个特解$x_p$；($Ax_p = b$)</p>
</li>
<li><p>求解$Ax=0$，即求$A$的零空间$x_{n}$（$Ax_n = 0$）</p>
</li>
<li><p>得到通解$x_{n} + x_p$（$Ax_n + Ax_p = b$）这就是非其次线性方程组的解结构</p>
<p>注意：特解不需要乘常数$c$</p>
</li>
</ul>
<p><strong>矩阵的秩与解的个数</strong>（仅讨论有解的情况即$R(A) = R(A,b)$）：</p>
<p>可知$0 &lt;= R(A) &lt;= \min \{m,n\}$</p>
<ul>
<li>若$R(A) = R(A,b)= n$，即<strong>列满秩</strong>，此时自由变量个数为0。非零行个数为$n$，相当于$n$个方程，$n$个未知变量，且 $N(A) = \pmb 0$，$Ax=b$仅有一解 $x_p$</li>
<li>若$R(A) = R(A,b) &lt; n$，此时存在自由变量，因此有无限解，$x_p + x_n$，$x_n$是一系列向量的线性组合。</li>
<li>若$R(A) = R(A,b) = m &lt; n$，即<strong>行满秩</strong>，合第二种情况相同，无限解</li>
</ul>
<p>考虑当$R(A) \not= R(A,b)$的情形，此时$A$中非零行个数不等于$\pmb b$的非零行个数，方程无解。</p>
<hr>
<ul>
<li>例子</li>
</ul>
<p>给定</p>
<script type="math/tex; mode=display">
A = \begin{pmatrix} 1&1&2\\1&2&-1\\1&3&1 \end{pmatrix},\pmb b = \begin{pmatrix} 1\\-2\\5\end{pmatrix}</script><p>求$A\pmb x = \pmb b$的解。</p>
<script type="math/tex; mode=display">
\begin{pmatrix} 1&1&2&1\\1&2&-1&-2\\1&3&1&5 \end{pmatrix}～\begin{pmatrix} 1&1&2&1\\0&1&-3&-3\\0&0&1&2 \end{pmatrix}(r_3-r_2,r_2-r_1,r_3-r_2,r_3/2)～\begin{pmatrix} 1&0&0&-6\\0&1&0&3\\0&0&1&2 \end{pmatrix}</script><p>得$\pmb x = (-6,3,2)^T$。列满秩矩阵只有一个特解。</p>
<p>给定</p>
<script type="math/tex; mode=display">
A = \begin{pmatrix} 2&3&1\\1&-2&4\\3&8&-2\\4&-1&9\end{pmatrix},\pmb b = \begin{pmatrix}4\\-5\\13\\-6\end{pmatrix}</script><p>求解$A\pmb x = \pmb b$，</p>
<p>经过行变换可得</p>
<script type="math/tex; mode=display">
\begin{pmatrix} 1&0&2&-1\\0&1&-1&2\\0&0&0&0\\0&0&0&0\end{pmatrix}</script><p>主元为$x_1,x_2$，自由变量为$x_3$，令$x_3 = 0$，得$x_1=-1,x_2=2$，特解$\pmb x_p = (-1,2,0)^T$</p>
<p>当$x_3 = 1$，得$(-2,1,1)^T$.</p>
<p>因此</p>
<script type="math/tex; mode=display">
\pmb x = \begin{pmatrix}-1\\2\\0\end{pmatrix} + c\begin{pmatrix}-2\\1\\1\end{pmatrix},c\in \R</script><h3 id="5-四个基本子空间"><a href="#5-四个基本子空间" class="headerlink" title="5. 四个基本子空间"></a>5. 四个基本子空间</h3><p>给定矩阵$A_{m \times n}$</p>
<p><strong>列空间$C(A)$</strong></p>
<p><strong>零空间</strong>$N(A)$：$Ax=\vec 0, x\in \R^n$</p>
<p><strong>行空间$C(A^T)$</strong>：矩阵$A$经过初等变换后，非零行即可作为行空间的一组基。</p>
<p><strong>$A^T$的零空间(左零空间)$N(A^T)$</strong>：$A^Tx = \vec 0, x \in \R^m$或者写成$x^TA = 0$这也是为什么叫左零空间的原因。</p>
<p>考虑 $A$ 到其行最简形矩阵 $R$ 的变换过程，现在我们想知道$A$乘以一个什么矩阵可以变到 $R$，即$EA = R$，求$E$？</p>
<p>注意 $A$ 是任意矩阵（方阵或者长方形矩阵）！</p>
<p>考虑求逆矩阵时在方阵后加上一个同阶方阵然后行变换的过程，同样在$A$后加一个$m$阶单位阵，有</p>
<script type="math/tex; mode=display">
(A_{m\times n},I_{m \times m}) \rightarrow (R_{m \times n},E_{m \times m})</script><p>$R$为矩阵 $A$ 的行阶梯矩阵，此时$E$就是我们要求的那个变换矩阵，即$EA = R$，此时 $R$ 中的零行对应矩阵 $E$ 的行就是 $N(A^T)$ 的一个基向量。</p>
<p><strong>四个子空间的正交性(orthogonality)</strong></p>
<ul>
<li><p>$C(A)$&perp;$N(A^T)$</p>
<p>证明：设$\vec v \in C(A)，\pmb v = A\pmb x，\pmb v \in \R^m$，即列空间的线性组合，$\vec u \in N(A^T),A^Tu=\vec 0$，$\vec u \in \R^m$$v^Tu=(Ax)^Tu=x^TA^Tu=x^T(A^Tu) = 0$.所以 $u$ 和 $v$ 正交，由于两个空间的任意向量都相互正交，因此这两个空间正交。</p>
</li>
<li><p>$C(A^T)$&perp;$N(A)$</p>
<p>证明：显然$N(A) = \{x|A\pmb{x}=\pmb{0}\}$，即 $\pmb{x}$ 与 $A$ 中每个行向量正交$\pmb{r_i}\pmb{x}=0$，$\pmb{r_i}$是行向量。且行空间由行向量的线性组合得到，因此 $\pmb{x}$ 与行空间任意向量正交。即$\pmb{a}= c_1\pmb{r_1} + c_2\pmb{r_2} \in C(A^T),\pmb{a}^T\pmb{x} = (c_1\pmb{r_1} + c_2\pmb{r_2})\pmb{x} = 0$.</p>
</li>
</ul>
<p><img src="https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/Math-Linear-Algebra-Chapter-4-1/4spaces.png" alt=""></p>
<h3 id="6-正交向量与投影"><a href="#6-正交向量与投影" class="headerlink" title="6. 正交向量与投影"></a>6. 正交向量与投影</h3><h4 id="6-1-正交向量"><a href="#6-1-正交向量" class="headerlink" title="6.1 正交向量"></a>6.1 正交向量</h4><p>给定列向量 $\vec u,\vec v \in \R^n$，若$\vec u^T\vec v = \vec v^T\vec u = 0$，则$\vec u,\vec v$ 正交。零向量$\pmb{0}$与任意向量正交（orthogonal）。</p>
<p>$||\vec u||$表示向量$\vec u$的模长，即$||\vec u|| = \sqrt{\vec u^T\vec u}$。注意：$\vec u \vec u^T$结果是一个$n \times n$的矩阵！</p>
<h4 id="6-2-正交子空间与正交基"><a href="#6-2-正交子空间与正交基" class="headerlink" title="6.2 正交子空间与正交基"></a>6.2 正交子空间与正交基</h4><p>子空间$S、T$，若 $S$ 与 $T$ 正交，则 $S$ 中任意向量与 $T$ 的任意向量正交。四个基本子空间的正交性上一章已经讨论过。</p>
<p>若空间的一组基向量两两正交，就是一组正交基，在此条件上，若每个向量的模长为$1$，则称这组基向量为<strong>标准正交基</strong>。</p>
<p>例子：$\R^3$的一组标准正交基：$\begin{pmatrix} 1\\0\\0 \end{pmatrix},\begin{pmatrix} 0\\1\\0 \end{pmatrix},\begin{pmatrix} 0\\0\\1 \end{pmatrix}$</p>
<h4 id="6-3-投影与投影矩阵"><a href="#6-3-投影与投影矩阵" class="headerlink" title="6.3 投影与投影矩阵"></a>6.3 投影与投影矩阵</h4><p><img src="/images/坐标系.png" alt=""></p>
<p>根据上图可得：</p>
<script type="math/tex; mode=display">
\pmb p = x\pmb a,x\in R\\
\pmb a \perp \pmb e \Rightarrow \pmb a^T(\pmb b - x\pmb a) = 0\Rightarrow x\pmb a^T\pmb a = \pmb a^T\pmb b</script><script type="math/tex; mode=display">
x = \frac{\pmb a^T\pmb b}{\pmb a^T \pmb a}</script><script type="math/tex; mode=display">
\pmb p = \pmb a \frac{\pmb a^T\pmb b}{\pmb a^T \pmb a}</script><p>我们称</p>
<script type="math/tex; mode=display">
P = \frac{\pmb a\pmb a^T}{\pmb a^T \pmb a}</script><p>为由 $\pmb b$ 到 $\pmb a$ 的投影变换矩阵，即 $\pmb p = P\pmb b$。直观上理解，任意向量与 $P$ 相乘，结果都会在 $P$ 的列空间内。</p>
<p>投影矩阵 $P$ 的性质：</p>
<ul>
<li>$P^2 = P$，幂等，$P(P\pmb b) = P\pmb b$</li>
<li>$ P^T = P$，对称</li>
</ul>
<p><strong>Q</strong>：现在考虑这个问题：线性方程组$A\pmb x=\pmb b$无解的时候，即$\pmb b \notin C(A)$，如何求一个近似（最优解）？</p>
<p><strong>A</strong>：有了投影我们就可以把 $\pmb b$ 投影到$C(A)$中得到 $\hat{\pmb b}$，从而$A\hat{\pmb x} = \hat{\pmb b}$ 有解，$\hat{\pmb x}$ 即为一个近似解。</p>
<p>那么问题随之而来，如何得到 $\hat{\pmb b}$ ？也就是问这里的投影矩阵 $P$ 如何求得？</p>
<p>观察上图，我们发现有个误差向量$\pmb e = \pmb b - A\hat{\pmb x}$，在本例中，它是垂直于 $C(A)$ 的，在讲四个子空间时我们说过$C(A) \perp N(A^T)$，因此，$\pmb e \in N(A^T)$，有：</p>
<script type="math/tex; mode=display">
A^T(\pmb b - A\hat{\pmb x}) = 0</script><script type="math/tex; mode=display">
A^T\pmb b = A^TA\hat{\pmb x}</script><p>我们希望这个方程有解，也就是求出$\hat{\pmb x} = (A^TA)^{-1}A^T\pmb b$。问题是$A^TA$的可逆性需要进一步讨论，即可能是可逆的也可能是不可逆的。那么$A^TA$这个方阵可逆的条件是什么？<strong>答案是当且仅当$A$各列线性无关时，$A^TA$可逆</strong></p>
<p>（$A$为$m \times n$的矩阵，$A^TA$就是一个$n \times n$的对称方阵）。下面给出证明。</p>
<p><strong>证明：</strong></p>
<p>不妨先假设$A^TA\pmb x = \pmb 0$，即有：$\pmb x^TA^TA\pmb x = \pmb 0 \Rightarrow (A\pmb x)^TA\pmb x = \pmb 0$，即$||A\pmb x|| = 0 \Rightarrow A\pmb x = \pmb 0$</p>
<p>又$A$各列线性无关，所以$A\pmb x = \pmb 0$只有零解（线性无关的定义），所以$\pmb x = \pmb 0$。即$A^TA\pmb x = \pmb 0$只有零解，$A^TA$中各列线性无关，$R(A^TA) = n$，满秩矩阵一定可逆（秩的定义，$|A^TA|\not = 0$），得证。</p>
<hr>
<p>好了，我们已经得到了$\hat{\pmb x}$的表达式，现在就可以求出投影矩阵了。</p>
<script type="math/tex; mode=display">
\hat{\pmb b} = A\hat{\pmb x} = A(A^TA)^{-1}A^T\pmb b</script><p>这里的投影矩阵</p>
<script type="math/tex; mode=display">
P =A(A^TA)^{-1}A^T</script><p>$P$依旧满足投影矩阵的两个性质。特别的，当$\pmb b \perp C(A)$，$P\pmb b = \pmb 0$。</p>
<h4 id="6-4-最小二乘-Least-Squares"><a href="#6-4-最小二乘-Least-Squares" class="headerlink" title="6.4 最小二乘(Least Squares)"></a>6.4 最小二乘(Least Squares)</h4><p>最小二乘的思想最小化理论测与观测值的平方和：$\min \sum_0 ^n y - y_i$</p>
<p>给定一系列二维平面上的点，要拟合一条直线$y = kx + b$，$k,b$未知；就用到了最小二乘。下面先讨论使用最小二乘拟合直线，再进一步探讨<strong>最小二乘与投影的奇妙联系</strong>。</p>
<p><img src="https://pic7.zhimg.com/v2-4c65ed9cfa6aea8a65e98fda23ae27f3_r.jpg" style="zoom:80%;" /></p>
<script type="math/tex; mode=display">
kx_1 + b = y_1 \\
kx_2 + b = y_2 \\
...\\
kx_m + b = y_m \\</script><p>写成矩阵形式就是</p>
<script type="math/tex; mode=display">
A = \begin{pmatrix}x_1 & 1\\x_2 & 1\\..&..\\x_m& 1\end{pmatrix},\pmb b= \begin{pmatrix}y_1\\y_2 \\..\\y_m\end{pmatrix},\pmb x = \begin{pmatrix} k \\ b\end{pmatrix}</script><p>求解方程组$A\pmb x = \pmb b$。接下来我们讨论这个方程<strong>无解</strong>的时候该怎么办？即根本没有一条直线同时经过上图中所有蓝色的点！为了选取最合适的$\pmb x$，让该等式”尽量成立”，引入残差平方和函数$H$：</p>
<script type="math/tex; mode=display">
min(H) = \arg \min_{\pmb x} ||A\pmb x - \pmb b||^2_2</script><p>（下标表示2范式，上标表示平方）。这也就是最小二乘法的思想。当 $\pmb x$ 取最优值的时候，$A\pmb x$恰好对应图中线上橙色的点，而 $b$ 则对应图中蓝色的点，$e$ 的值则应红色的线长。</p>
<p>现在的问题是如何求最优的 $\pmb x$？我们可以求关于$\pmb x$的导数，并令其结果等于0，即：</p>
<script type="math/tex; mode=display">
 y = ||A\pmb x - \pmb b||^2_2 = (A\pmb x - \pmb b)^T(A\pmb x - \pmb b) = \pmb x^TA^TA\pmb x -\pmb x^TA^T\pmb b - \pmb b^TA\pmb x + b^Tb</script><script type="math/tex; mode=display">
\frac{\partial y }{\partial \pmb x} = 2A^TA\pmb x - 2 A^T\pmb b = 0</script><p>即最优$\pmb x$满足$A^TA\pmb x = A^T\pmb b$，是不是有点眼熟？</p>
<p><strong>投影与最小二乘的联系</strong></p>
<p>最小二乘的思想是想如何选取参数 $x$ 使得 $H$ 最小。而从向量投影的角度来看这个问题，$H = ||\pmb b - A\pmb x||^2$ 就是向量 $\pmb e$ 长度的平方(参考6.3的图)，如何才能使 $\pmb e$ 的长度最小呢？$b$ 和$x_1$，$x_2$都是固定的，当然是$e$ 垂直$x_1$，$x_2$平面的时候长度最小！换句话说：最小二乘法的解与矩阵投影时对变量求解的目标是一致的！最小二乘的求解实质上就是$Ax=b$没有解，我们就把 $b$ 投影到$C(A)$内，此时误差向量或者说$H$最小（垂直于$C(A)$）。</p>
<h3 id="7-正交矩阵与正交化法"><a href="#7-正交矩阵与正交化法" class="headerlink" title="7. 正交矩阵与正交化法"></a>7. 正交矩阵与正交化法</h3><h4 id="7-1-正交矩阵（只讨论实矩阵）"><a href="#7-1-正交矩阵（只讨论实矩阵）" class="headerlink" title="7.1 正交矩阵（只讨论实矩阵）"></a>7.1 正交矩阵（只讨论实矩阵）</h4><p><strong>注意：</strong>Strang教授在视频中讲正交矩阵时，谈到了非方阵的正交矩阵，而张贤达的《矩阵分析与应用》一书中对正交矩阵的定义仅限于方阵。下面以教授视频教授的为主。</p>
<p><strong>定义：</strong>$A$为$m \times n$，若$A^TA = I$，则$A$为正交矩阵，即$A$中的列向量两两正交且模长为$1$！（列向量是标准正交向量组）当$A$为$n$阶方阵，可得$A^T = A^{-1}$。</p>
<p>举个正交矩阵的例子：$\begin{pmatrix} \cos \theta &amp; -\sin \theta \ \sin \theta &amp; \cos \theta \end{pmatrix}$</p>
<p>当某个向量投影到$C(A)$中时，投影矩阵$P = A(A^TA)^{-1}A^T = AA^T$,当$A$为方阵，$A^TA = AA^T =P = I$，投影矩阵是单位阵。</p>
<p>考虑$A^TA\hat{\pmb x} = A^T\pmb b$当$A$为正交矩阵时，$\hat{\pmb x} = A^T\pmb b$，$\hat{\pmb x_i} = \pmb a_i\pmb b$，$\hat{\pmb x}$的第$i$个分量等于$A$中第$i$列的列向量与$\pmb b$的内积。可以看到当$A$为正交矩阵时，求解$A\pmb x = \pmb b$形式非常简单，那给定一个非正交矩阵$A$，如何把它化为正交矩阵呢？这就是下一节讨论的正交化法。</p>
<h4 id="7-2-Gram-Schmidt-正交化法"><a href="#7-2-Gram-Schmidt-正交化法" class="headerlink" title="7.2 Gram-Schmidt 正交化法"></a>7.2 Gram-Schmidt 正交化法</h4><p>首先考虑两个不垂直的向量$\pmb a,\pmb b$，如图所示：</p>
<p><img src="E:\chrome download\坐标系.png" style="zoom:67%;" /></p>
<p>现在的问题是，在这两个向量构成的空间中，求出一组标准正交基，即求$\pmb p,\pmb q$使得$\pmb p \perp \pmb q$？</p>
<p>不妨令$\pmb p = \pmb a$，那么与$\pmb a$垂直的向量该如何求？之前投影讲到，$\pmb b$投影到$\pmb a$上时，误差向量$\pmb e$是垂直于$\pmb a$的。</p>
<script type="math/tex; mode=display">
\pmb q = \pmb e = \pmb b - \frac{\pmb a \pmb a^T}{\pmb a^T\pmb a}\pmb b = \pmb b - \frac{\pmb a^T \pmb b}{\pmb a^T\pmb a}\pmb a \\
\pmb q \perp \pmb a</script><p>$\pmb a^T\pmb b$是一个常数，那么$\pmb a$放在左右也就无所谓了，写成内积形式即：</p>
<script type="math/tex; mode=display">
\pmb b - \frac{<\pmb a,\pmb b>}{<\pmb a,\pmb a>}\pmb a</script><p>如果三个向量呢，第三个向量减去$\pmb a$、$\pmb b$上的分量就可以了。之后对所得向量除以模长即可得到标准正交向量组。这就是Gram-Schmidt正交化。</p>
<p>我们可以看到，本质上正交化就是求给定矩阵$A$列空间$C(A)$的一组标准正交基。</p>
<h4 id="7-3-QR分解"><a href="#7-3-QR分解" class="headerlink" title="7.3 QR分解"></a>7.3 QR分解</h4><p>给定向量组$\pmb x_1,\pmb x_2…,\pmb x_n$，向量组构成的矩阵$A$，对向量组正交化后得到$\pmb q_1,\pmb q_2,…,\pmb q_n$，正交向量组组成矩阵$Q$，那$A$与$Q$有何关联？事实上$A = QR$，矩阵$R$是一个上三角矩阵，由于$\pmb q_i$是$C(A)$的一组标准正交基，因此$A$中的每个列向量都是由$ \pmb q_i$的线性组合构成，$R$ 恰好是 $\pmb q_i$ 线性组合的系数。</p>
<script type="math/tex; mode=display">
R = \begin{pmatrix} \pmb q_1^T\pmb x_1& \pmb q_1^T\pmb x_2 &..&\pmb q_1^T\pmb x_n\\
0&\pmb q_2^T\pmb x_2&..&\pmb q_2^T\pmb x_n\\
..&..&..&..\\
0&0 &..& \pmb q_n^T\pmb x_n\end{pmatrix}</script><script type="math/tex; mode=display">
Q = \begin{pmatrix}\pmb q_1 & \pmb q_2&..&\pmb q_n\end{pmatrix}</script><h3 id="8-行列式及其性质"><a href="#8-行列式及其性质" class="headerlink" title="8. 行列式及其性质"></a>8. 行列式及其性质</h3><h3 id="9-方阵的特征值与特征向量"><a href="#9-方阵的特征值与特征向量" class="headerlink" title="9. 方阵的特征值与特征向量"></a>9. 方阵的特征值与特征向量</h3><p><strong>注意：特征值与特征向量都是针对方阵而言！</strong></p>
<h4 id="9-1-方阵的特征值"><a href="#9-1-方阵的特征值" class="headerlink" title="9.1 方阵的特征值"></a>9.1 方阵的特征值</h4><p>给定 $n$ 阶方阵$A$，若存在<strong>非零向量</strong> $\pmb x\in \R^n$，以及实数 $\lambda \in \R$，有：</p>
<script type="math/tex; mode=display">
A\pmb x = \lambda \pmb x</script><p>则称$\lambda $为$A$的特征值（eigenvalue），非零向量$\pmb x$为 $A$ 对应于 $\lambda $ 的特征向量（eigenvector）。特征向量经过矩阵$A$的变换后，方向没有发生改变，只是模长发生了改变，这就是特征向量的神奇之处。</p>
<p>给定 $n$ 阶方阵$A$，现在考虑任意向量到$C(A)$的投影矩阵$P = A(A^TA)^{-    1}A^T$的特征向量与特征值？上面讲过特征向量是经过矩阵变化后方向不发生改变的，那什么向量经过$P$的变换方向不变？答案就是任意位于$C(A)$内的向量，很容易理解在$C(A)$内的向量到$C(A)$的投影是<strong>向量本身</strong>，而且特征值为$1$。</p>
<script type="math/tex; mode=display">
P\pmb x = \pmb x,x\in C(A)</script><p>考虑置换矩阵$A = \begin{pmatrix}0&amp;1\\1&amp;0\end{pmatrix}$的特征值和对应的特征向量。</p>
<p>显然$\pmb x = (1,1)^T$是，交换行后方向没发生变化，$\lambda = 1$；$\pmb x = (1,-1)^T,\lambda = -1$。</p>
<p>很重要的两个定理：</p>
<ul>
<li>$n$阶方阵$A$的特征值之和等于对角线元素之和（迹trace）。</li>
<li>$\lambda_1 + \lambda_2 + … \lambda_n = |A|$ 特征值之和等于$|A|$</li>
</ul>
<p><strong>如何求解方阵的特征值和特征向量</strong></p>
<script type="math/tex; mode=display">
(A-\lambda I)\pmb x = \pmb 0,\pmb x \not= \pmb 0</script><p>即对于上述方程称之为<strong>特征方程</strong>，若存在非零解，矩阵$A-\lambda I$务必<strong>不是满秩矩阵</strong>（必须存在自由变量），降秩矩阵也就意味着是奇异矩阵（不可逆矩阵），即$|A - \lambda I|=0$，现在只需计算行列式的值就可求出$\lambda $，之后再带回原方程，得到对应的特征向量。</p>
<ul>
<li>例子：给定$A=\begin{pmatrix}3&amp;1\\1&amp;3\end{pmatrix}$，求对应的特征值与特征向量。</li>
</ul>
<script type="math/tex; mode=display">
|A-\lambda I| = \left|\begin{matrix}3-\lambda & 1\\1&3-\lambda \end{matrix}\right | = 0,\lambda_1 = 4,\lambda_2 = 2</script><p>当$\lambda_1 = 4$，对应的方程：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}-1&1\\1&-1\end{pmatrix}\begin{pmatrix}x_1\\x_2\end{pmatrix} = \begin{pmatrix}0\\0\end{pmatrix}</script><p>解得$\pmb x_1 = (1,1)^T$;</p>
<p>当$\lambda_2 = 2$，解得$\pmb x_2 = (1,-1)^T$。</p>
<p><strong>显然，s若$\pmb x_i$是矩阵$A$对应于$\lambda_i$的特征向量，则$k\pmb x_i(k\not=0)$也是对应于$\lambda_i$的特征向量</strong>。</p>
<p><strong>$A$与$A^T$的特征值相同</strong>：$|A-\lambda I| = |(A - \lambda I)^T| = |A^T - \lambda I| = 0$</p>
<p><strong>旋转矩阵的特征值与特征向量</strong></p>
<p><strong>上三角矩阵和下三角矩阵的特征值</strong></p>
<p>当 $A$ 为一个上三角矩阵时，根据行列式性质 $|A - \lambda I|$ 的值恰好为$(a_{11}-\lambda_1)(a_{22}-\lambda_2)…(a_{nn}-\lambda_n) = 0$，因此上/下三角矩阵的特征值即为对角线元素。</p>
<h4 id="9-2-相似矩阵"><a href="#9-2-相似矩阵" class="headerlink" title="9.2 相似矩阵"></a>9.2 相似矩阵</h4><p>在线性空间中，相似矩阵就是同一个矩阵的不同基下的表示。</p>
<p>设$A,B$是$n$阶矩阵，若存在可逆矩阵$P$，使得$P^{-1}AP = B$成立，称$A$与$B$相似。</p>
<script type="math/tex; mode=display">
A\pmb x = \lambda \pmb x, B = P^{-1}AP</script><script type="math/tex; mode=display">
P^{-1}APP^{-1}\pmb x = \lambda P^{-1}\pmb x \Rightarrow B(P^{-1}\pmb x) = \lambda (P^{-1}\pmb x)</script><p>由此可以看出，相似矩阵的特征值相同，$\pmb x$为$A$的特征向量，那么B的特征向量等于$M^{-1}\pmb x$。</p>
<h4 id="9-3-矩阵对角化和矩阵幂乘"><a href="#9-3-矩阵对角化和矩阵幂乘" class="headerlink" title="9.3 矩阵对角化和矩阵幂乘"></a>9.3 矩阵对角化和矩阵幂乘</h4><p>讨论下面内容的<strong>大前提</strong>：$n$阶方阵$A$存在$n$个线性无关的特征向量$\Leftrightarrow$ $A$有$n$个互不相等的特征值（没有重复的特征值）。如果存在重复的特征值，<strong>可能存在也可能不存在</strong>$n$个线性无关的特征向量。（存在相同的$k$个特征值，求出来的特征向量可能是同一个，或者个数小于$k$）</p>
<p>$S^{-1}AS = \Lambda$（$A,\Lambda$ 相似），$S$是 $A$ 对应的特征向量按列组成的矩阵，$\Lambda $是由特征值组成的对角阵。这一过程被称为矩阵的对角化。需要注意：<strong>矩阵$A$可以被对角化的充要条件是$A$存在$n$个线性无关的特征向量（$S$可逆）</strong>。谨记！</p>
<script type="math/tex; mode=display">
AS = A\begin{pmatrix}\pmb x_1 & \pmb x_2&..&\pmb x_n \end{pmatrix} = \begin{pmatrix}A\pmb x_1&A\pmb x_2 &..&A\pmb x_n\end{pmatrix} = \begin{pmatrix}\lambda_1\pmb x_1&\lambda_2\pmb x_2 &..& \lambda_n\pmb x_n\end{pmatrix} = S\Lambda</script><script type="math/tex; mode=display">
\Lambda = \begin{pmatrix}\lambda_1 & 0&0&..&0\\0&\lambda_2&0&..&0 \\..&..&..&..&..\\0&0&0&..&\lambda_n\end{pmatrix}</script><script type="math/tex; mode=display">
AS = S\Lambda \Rightarrow S^{-1}AS = \Lambda \Leftrightarrow A = S\Lambda S^{-1}</script><p><strong>矩阵幂乘</strong></p>
<script type="math/tex; mode=display">
A^2 = AA = S\Lambda S^{-1}S\Lambda S^{-1} = S \Lambda^2S^{-1}</script><script type="math/tex; mode=display">
A^n = S\Lambda^nS^{-1}</script><p>若$A\pmb x = \lambda \pmb x$，那么$A^2\pmb x = \lambda A\pmb x = \lambda^2\pmb x$。即$A$的$n$次幂对应的特征向量没有发生改变，特征值为$\Lambda $的$n$次幂。</p>
<h3 id="10-对称矩阵与正定矩阵"><a href="#10-对称矩阵与正定矩阵" class="headerlink" title="10. 对称矩阵与正定矩阵"></a>10. 对称矩阵与正定矩阵</h3><h4 id="10-1-对称矩阵的特征值与特征向量"><a href="#10-1-对称矩阵的特征值与特征向量" class="headerlink" title="10.1 对称矩阵的特征值与特征向量"></a>10.1 对称矩阵的特征值与特征向量</h4><p>$A = A^T$，则称$n$阶方阵为对称矩阵。</p>
<ul>
<li><p>对称矩阵的特征值都是实数</p>
</li>
<li><p>若$\lambda_1 \not= \lambda_2$，$\pmb x_1,\pmb x_2$为其对应特征向量，则 $\pmb x_1$ 与 $\pmb x_2$正交，也就是说特征值不同的特征向量两两正交，当特征值重复的时候，求出的特征向量未必正交（但是与其他不同特征值的特征向量仍然正交），但可以通过正交化，化为正交向量。也就是说，特征矩阵总存在一个由特征向量组成的正交矩阵。</p>
<script type="math/tex; mode=display">
A\pmb x_1 = \lambda_1\pmb x_1,A\pmb x_2 = \lambda_2 \pmb x_2\\
(A\pmb x_1)^T = \pmb x_1^TA^T = \lambda_1\pmb x_1^T\\
\lambda_1\pmb x_1^T \pmb x_2 = \pmb x_1^TA \pmb x_2 =  \lambda_2\pmb x_1^T \pmb x_2\\
(\lambda_1 - \lambda_2)x_1^T \pmb x_2 = 0</script><p>若$\lambda_1 \not= \lambda_2$,$x_1^T \pmb x_2 =0$，即$\pmb x_1,\pmb x_2$正交。</p>
</li>
</ul>
<p>在将矩阵对角化时：$S^{-1}AS = \Lambda \Rightarrow A = S\Lambda S^{-1}$，若$A$为对称矩阵，那么$S$中列向量两两正交，1可知$S^TS = I$，进而$S^T = S^{-1}$。因此，$A = S\Lambda S^{-1} = S\Lambda S^T$。这就是<strong>谱分解</strong>。</p>
<p>例子：图的拉普拉斯矩阵$L$就是一个对称矩阵，$L = U\Lambda U^T$，利用$U$可以对节点进行图上傅里叶变换和反傅里叶变换。</p>
<h4 id="10-2-正定矩阵（positive-definite-matrix）"><a href="#10-2-正定矩阵（positive-definite-matrix）" class="headerlink" title="10.2 正定矩阵（positive definite matrix）"></a>10.2 正定矩阵（positive definite matrix）</h4><p>讨论完对称矩阵的特征值，我们知道一定是实数，我们另一个比较关心的是特征值的正负问题，这就讲到了正定矩阵。下面给出正定矩阵的定义。</p>
<p>给定对称矩阵$A$，非零向量$\pmb x$，若满足：</p>
<script type="math/tex; mode=display">
\pmb x^TA\pmb x > 0, \pmb x \not= \pmb 0</script><p>则称$A$为正定矩阵。当$\pmb x^TA\pmb x \ge 0$，我们称$A$为半正定矩阵(positive semi-definite matrix)。$\pmb x^TA\pmb x$为二次型。</p>
<p><strong>正定矩阵的性质：</strong></p>
<ul>
<li>特征值均为正数，也就意味着行列式值为正数</li>
<li>$A,B$为正定矩阵，$A+B$也为正定矩阵</li>
</ul>
<p>若$A_{m \times n}$，考虑$A^TA$是$n$阶方阵，是对称矩阵，且至少是半正定矩阵（矩阵元素全为$0$的时候）。</p>
<script type="math/tex; mode=display">
\pmb x^TA^TA\pmb x = (A\pmb x)^T(A\pmb x) = ||A\pmb x||^2 \ge 0</script><h3 id="11-奇异值分解"><a href="#11-奇异值分解" class="headerlink" title="11. 奇异值分解"></a>11. 奇异值分解</h3><h3 id="12-线性变换"><a href="#12-线性变换" class="headerlink" title="12 线性变换"></a>12 线性变换</h3><p>设$T$是一个从$V_n$到$U_m$的映射，若$T$满足：</p>
<ul>
<li>$T(\lambda v) = \lambda T(v) $</li>
<li>$T(u + v) = T(u) + T(v)$</li>
</ul>
<p>就称从$V_n$到$V_m$的线性映射或线性变换。</p>
<p><strong>坐标</strong>（coordinate）：$\pmb x_1,\pmb x_2,…,\pmb x_n$是线性空间$V_n$的一组基，对于$\pmb u \in V_n$，$\pmb u = \alpha_1\pmb x_1 + \alpha_2\pmb x_2 + … + \alpha_n\pmb x_n$（基向量的线性组合），$\pmb \alpha =(\alpha_1,\alpha_2,…,\alpha_n)^T$称为这个基下的坐标。</p>
<p><strong>例子：</strong>对于$T(\pmb x) = A\pmb x,\pmb x \in \R^n，A_{m \times n}$，就是一个线性变换，考虑$A$是如何构成的？</p>
<p>对于$\pmb x \in \R^n$，有一组基$\pmb v_i$描述，坐标为$\pmb \alpha$，对于$T(\pmb x) \in \R^m$，有一组基$\pmb w_i$描述，坐标为$\pmb \beta$。</p>
<p>可知$\pmb x = \pmb \alpha^T \pmb v,T(\pmb v_1) = \beta_{11}\pmb w_1 + \beta_{21}\pmb w_2 + …$，即坐标构成了第一列，同理第二列、第三列….这是线性变换的矩阵形式。</p>
<p>注意：求导也是一种线性变换。</p>
<h3 id="13-矩阵与向量微分"><a href="#13-矩阵与向量微分" class="headerlink" title="13. 矩阵与向量微分"></a>13. 矩阵与向量微分</h3><p>根据函数值类型的不同可以分为：标量函数、向量函数、矩阵函数；根据输入类型的不同：标量、向量、矩阵，两两组合可以分为：标量函数对标量的导数、标量对向量的偏导（比如梯度）、标量对矩阵的偏导；向量对标量的偏导、向量对向量的偏导、向量对矩阵的偏导等等。</p>
<p>关于向量微分，转置与否对结果有很大影响，本章讨论时，统一以输入的形状为准，即当输入是列向量，偏导结果仍为列向量；输入是行向量，偏导结果仍为行向量。</p>
<h4 id="13-1-标量对向量的偏导"><a href="#13-1-标量对向量的偏导" class="headerlink" title="13.1 标量对向量的偏导"></a>13.1 标量对向量的偏导</h4><p>$f(\pmb x) \in \R,\pmb x \in \R^n$，$\frac{\partial f}{\partial \pmb x} = (\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},…\frac{\partial f}{\partial x_n})^T$称为梯度向量。</p>
<p><strong>黑塞矩阵 Hessian Matrix</strong></p>
<p>$\frac{\partial f}{\partial \pmb x^2}$称为黑塞矩阵，即$f$对多元变量的二阶偏导所构成的矩阵。黑塞矩阵与多元函数的极值判定密切相关。</p>
<script type="math/tex; mode=display">
H = \begin{pmatrix}\frac{\partial f^2}{\partial x_1^2}&\frac{\partial f^2}{\partial x_1\partial x_2}&...&\frac{\partial f^2}{\partial x_1\partial x_n}\\...&...&...&...\\\frac{\partial f^2}{\partial x_n\partial x_1} &\frac{\partial f^2}{\partial x_n \partial x_2}&...&\frac{\partial f^2}{\partial x_n^2}\end{pmatrix}</script><p>即：$H_{ij} = \frac{\partial f^2}{\partial x_i \partial x_j}$.</p>
<h4 id="13-2-向量对向量的偏导"><a href="#13-2-向量对向量的偏导" class="headerlink" title="13.2 向量对向量的偏导"></a>13.2 向量对向量的偏导</h4><p>注意向量对向量的偏导结果是一个矩阵。</p>
<p>$f(\pmb x) \in \R^m,\pmb x \in \R^n$，$\frac{\partial f}{\partial \pmb x} = \frac{\partial(y_1,y_2,…,y_m)}{\partial(x_1,x_2,…,x_n)}$称为雅可比矩阵</p>
<p><strong>雅可比矩阵  Jacobi Matrix</strong></p>
<script type="math/tex; mode=display">
\begin{pmatrix}\frac{\partial y_1}{\partial x_1}&\frac{\partial y_1}{\partial x_2}&...&\frac{\partial y_1}{\partial x_n}\\...&...&...&...\\\frac{\partial y_m}{\partial x_1} &\frac{\partial y_m}{\partial x_2}&...&\frac{\partial y_m}{\partial x_n}\end{pmatrix}</script><p>可知雅可比矩阵的形状：$m \times n$。</p>
<p><strong>二次型的偏导</strong></p>
<p>$f(\pmb x)= \pmb x^TP\pmb x + q^T\pmb x + r,r\in\R$</p>
<p>阅读中发现错误请联系：nefuzyj@163.com，不胜感激！</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-浅谈逻辑斯蒂回归模型" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/08/03/%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"
    >浅谈逻辑斯蒂回归模型</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/08/03/%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2020-08-03T08:01:07.919Z" itemprop="datePublished">2020-08-03</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="浅谈逻辑斯蒂回归模型"><a href="#浅谈逻辑斯蒂回归模型" class="headerlink" title="浅谈逻辑斯蒂回归模型"></a>浅谈逻辑斯蒂回归模型</h2><p>​        逻辑斯蒂回归或者是Softmax回归，说是回归其实是在解决二分类或者多分类的问题。逻辑斯蒂函数如下：</p>
<script type="math/tex; mode=display">
y = \frac{1}{1 + e^{-（w^Tx +b)}}</script><p>​        其中，$x \in R^n, w \in R^n$，$b$为偏置bias可以设置为0，$x$为样本的特征向量，$w$即要学习的参数。在给定一批有label的训练数据集后，我们可以根据极大似然估计来得到最优的参数值，之后，给定某个测试数据，当：y &gt;= 0.5，我们就把这个样本划为1类，否则就划为0类。</p>
<p>​        其实，完整的二项逻辑斯蒂回归模型是定义如下的条件概率分布：</p>
<script type="math/tex; mode=display">
P(Y = 1|x) = \frac{\exp(w*x + b)}{1 + \exp(w*x + b)}</script><script type="math/tex; mode=display">
P(Y = 0|x) = \frac{1}{1 + \exp(w*x + b)}</script><p>​        观察上述条件概率计算公式我们发现，当$w<em>x+b &gt; 0$的时候，$P(Y=1)$的概率一定高于$P(Y=0)$，我们就把其划分为1类。对于$P(Y=1)$的分布函数上下同除以分子$\exp$就得到了熟悉的逻辑斯蒂函数。$w</em>x+b &gt; 0$就等价于$y &gt; 0.5$，因此当逻辑斯蒂函数值$y &gt; 0.5$时就把样本划分到1类，在这个条件下，样本为1类的概率大于为0类的概率。</p>
<p>​        事实上，逻辑斯蒂函数是在计算样本为1类的概率，由于是二分类，不是1类就是0类。</p>
<p>​        </p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-近期刷题总结记录" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/07/18/%E8%BF%91%E6%9C%9F%E5%88%B7%E9%A2%98%E6%80%BB%E7%BB%93%E8%AE%B0%E5%BD%95/"
    >近期刷题总结记录</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/07/18/%E8%BF%91%E6%9C%9F%E5%88%B7%E9%A2%98%E6%80%BB%E7%BB%93%E8%AE%B0%E5%BD%95/" class="article-date">
  <time datetime="2020-07-18T14:33:46.959Z" itemprop="datePublished">2020-07-18</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Leetcode/">Leetcode</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="近期刷题总结记录"><a href="#近期刷题总结记录" class="headerlink" title="近期刷题总结记录"></a>近期刷题总结记录</h2><h3 id="1-Leftmost-Digit"><a href="#1-Leftmost-Digit" class="headerlink" title="1. Leftmost Digit"></a>1. Leftmost Digit</h3><p>​        杭电1060题，给你一个数字$n$，求出$n^n$最高位数字$t$，$n$不超过$10,000,000,000$(10亿)。</p>
<p>​        具体思想如下：</p>
<p>​        设$M = n^n$（n为整数），两边取10的对数则有</p>
<script type="math/tex; mode=display">
\log_{10}M = \log_{10} n^n = k</script><p>​        直觉上，如果$M$不是10的整数幂次，那么$k$是一个浮点数。</p>
<script type="math/tex; mode=display">
k = A.B</script><p>​        $A$为$k$ 的整数部分，$B$ 则为$k$ 的小数部分。有</p>
<script type="math/tex; mode=display">
\log_{10} M = k = A.B \\
10^{A.B} = M \\
10^A*10^{B} = M(B < 1)</script><p>​        其实$A$就代表了$M$的位数（三位数$10^2$、四位数$10^3$以此类推），而$10^B$相当于前面的系数（有点类似科学计数法那个形式）。</p>
<p>​        我们观察幂函数$y = 10^x$ ，在$x = 1$时$y = 10$，我们这儿$B &lt; 1$，<strong>因此$10^B$的值介于$[1,10)$之间，并且是一个浮点数</strong>。我们可以这么想，$10^A*10^{B}$把$10^B$放大了$10^A$倍后得到了$M$，即 $n$ 的 $n$ 次幂。</p>
<p>​        好了，那么$M$的最高位到底是几？很明显是$10^B$这个数字的整数部分。如何取到$B$这个小数呢，$k = n \log_{10} n = A.B$，$k$减去其取整部分即可得到$B$。$10^B$再取整就得到了最高位。</p>
<p>​        程序代码如下</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> t,n;</span><br><span class="line">	<span class="keyword">double</span> k;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;t);</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">        k = n * <span class="built_in">log10</span>(<span class="number">1.0</span> * n); <span class="comment">// k = A.B</span></span><br><span class="line">        <span class="comment">// k减去对k取整部分得到小数部分</span></span><br><span class="line">        k = k - (__int64)k;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,(<span class="keyword">int</span>)<span class="built_in">pow</span>(<span class="number">10</span>,k));</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-Euler函数"><a href="#2-Euler函数" class="headerlink" title="2. Euler函数"></a>2. Euler函数</h3><p>​        在数论中，对正整数，欧拉函数是小于或等于n的正整数中与n互质的数的数目。比如$\phi(8) = 4$，因为1、3、5、7与8互质。注：互质为两者没有除1外的公因数。</p>
<p>​        具体定义如下：</p>
<p>​        我们知道，对于任意正整数n，都可以表示为多个质数的乘积，即如下形式：</p>
<script type="math/tex; mode=display">
n = p_1^{k_1}p_2^{k2}...p_n^{k_n}</script><p>​        ，其中，$p_i$均为质数。</p>
<p>​        那么欧拉函数$\phi(n)$等于</p>
<script type="math/tex; mode=display">
\phi(n) = n\prod_{i=1}^n (1 - \frac{1}{p_i})</script><p>​        性质：</p>
<p>​        ① 欧拉函数是积性函数，$\phi(mn)=\phi(m)*\phi(n)$</p>
<p>​        ② 若p为质数，$\phi(p)=p-1$</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="comment">// 欧拉函数:统计小于等于n中与n互质的数的个数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">euler</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ans = n;</span><br><span class="line">    <span class="keyword">int</span> tmp = n;</span><br><span class="line">    <span class="comment">// n在循环过程中会不断发生变化。</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= <span class="built_in">sqrt</span>(<span class="number">1.0</span> * tmp); i++ )&#123;<span class="comment">//枚举所有能够被n整除的质因数</span></span><br><span class="line">        <span class="keyword">if</span>(n % i == <span class="number">0</span>)&#123;</span><br><span class="line">            ans = ans * (i - <span class="number">1</span>) / i;<span class="comment">// 欧拉公式，不断累乘</span></span><br><span class="line">            <span class="keyword">while</span>(n % i == <span class="number">0</span>)&#123;</span><br><span class="line">                n /= i; <span class="comment">// 消除质数因子的过程</span></span><br><span class="line">                <span class="comment">//将质数i不断的左除过去</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">         <span class="comment">// 新的n = n / p1 / p1 / p1 ... = p2*p2*p2...*pn</span></span><br><span class="line">        <span class="comment">// p1 &lt; p2 &lt; ... &lt; pn</span></span><br><span class="line">        <span class="comment">// 再次循环通过枚举找到p2....pn</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// n不为1，表明还有因子，最后一个质因子可能不在[2,sqrt(n)]之间</span></span><br><span class="line">    <span class="keyword">if</span>(n &gt; <span class="number">1</span>)&#123;</span><br><span class="line">        ans = ans / n * (n - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> t,n;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;t);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,euler(n));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-找出缺失的数字"><a href="#3-找出缺失的数字" class="headerlink" title="3. 找出缺失的数字"></a>3. 找出缺失的数字</h3><p>​        题目大意如下，首先输入$n$个不同的数字，再次输入$n-1$个数字，找出第二次输入没有输入的那个数字。输入的数字的值介于$[0,1000000000]$，$2 &lt;= n &lt;= 400000$。</p>
<p>​        可以看到数据规模较大，无论是集合还是利用高斯公式求两次和都可能数据溢出。能不能使用$O(n)$时间复杂度以及$O(1)$空间解决这个问题？</p>
<p>​        下面介绍二进制中异或（<code>^</code>）的操作的一些性质。</p>
<p>​        异或就是相同为0，不同为1。即<code>1 ^ 1 = 0,0 ^ 0 = 0,1 ^ 0 = 1,0 ^ 1 = 1</code>。异或还有如下性质</p>
<ul>
<li><p><code>0 ^ x = x</code></p>
</li>
<li><p><code>x ^ x = 0</code></p>
</li>
<li><p>异或满足交换律，即<code>1 ^ 2 ^ 4 ^ 6 =  6 ^ 2 ^ 1 ^ 4</code></p>
<p>有了这些性质，对于这个题，我们设置<code>a = 0,b = 0</code>；</p>
<p><code>a = a ^ arr1[i], i from 0 to n-1</code>，<code>b = b ^ arr2[j], j  from 0 to n-2</code>。</p>
<p>最后所求数字即为<code>a^b</code>。</p>
<p>其实很好理解：</p>
<script type="math/tex; mode=display">
a = a_1 \bigoplus a_2 \bigoplus....\bigoplus a_n \\
b = b_1 \bigoplus b_2 \bigoplus....\bigoplus b_{n-1} \\</script><p>最后<code>a^b</code>，由<strong>异或交换律</strong>这个性质等价于，将前后两次相同的数字先做异或得到0，前后两次相同的数字就都被消掉了，最后剩下的那个数字再与0异或还是这个数字，也就是第二次没有出现的那个数字。</p>
<p>同理，找出在都是偶数次出现的数字中出现次数为奇数次的数字也可以利用异或算法解决。</p>
<p><code>a = 1 ^ 2 ^ 4 ^ 6;b = 4 ^ 6 ^ 1;</code></p>
<p> <code>a ^ b =1 ^ 2 ^ 4 ^ 6 ^ 4 ^ 6 ^ 1 = 1 ^ 1 ^ 4 ^ 4 ^ 6 ^ 6 ^ 2= 0 ^ 0 ^ 0 ^ 2 = 0 ^ 2 = 2</code></p>
</li>
</ul>
<h3 id="4-Rightmost-Digit"><a href="#4-Rightmost-Digit" class="headerlink" title="4. Rightmost Digit"></a>4. Rightmost Digit</h3><p>​        杭电1061题。与第1题相反的是，本题求正整数$n^n$最左边的数字。要用到快速幂取模算法。快速幂即比较快的计算n的幂次。相比于循环n次计算的$O(n)$算法，快速幂能够在$O(\log n)$求出结果。比如计算$3^{100}$，一般我们可以循环100次计算出结果。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">long</span> <span class="keyword">long</span> ans = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++)&#123;</span><br><span class="line">    ans *= <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​        我们观察发现:</p>
<script type="math/tex; mode=display">
3^{100} = 3^{50}*3^{50}\\
3^{50} = 3^{25}*3^{25}\\
3^{25} = 3^{12}*3^{12}*3\\
3^{12} = 3^6*3^6 \\
3^6 = 3^3*3^3\\
3^3 = 3^2 * 3\\
3^2 = 3 * 3</script><p>​        这样，我们要计算$3^{100}$，就可以先计算出$3^{50}$，然后自己和自己相乘就可以得到结果，对于$3^{25}$同理，然后这样递推下去就可得到结果。按照这个方法计算，我们只做了8次乘法便得到了结果，相比100次大大减少了时间。幂次不是奇数就是偶数，偶数次幂就等于自己乘自己，奇数次幂得额外再乘以一个基底数字。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 递归计算快速幂 a^b</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(b == <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> a;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> temp = f(a, b / <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">return</span> (b % <span class="number">2</span> == <span class="number">0</span> ? <span class="number">1</span> : a) * temp * temp;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 非递归</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> base = a;</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>( b != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(b % <span class="number">2</span> == <span class="number">1</span>)&#123;</span><br><span class="line">            ans = ans * base;</span><br><span class="line">        &#125;</span><br><span class="line">        base = base * base;</span><br><span class="line">        b /= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>​        回到这道题，两个数相乘，二者的最后一位分别相乘贡献了结果，因此我们可以在计算快速幂过程中不断对10取模就可以得到最终结果。</p>
<p>​        插入一点其他知识：</p>
<script type="math/tex; mode=display">
(a + b) \mod p = (a\mod p + b \mod p) \mod p</script><p>​        在计算斐波那契数列时会用到。将加号改为乘号同样适用。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 快速幂取余</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> base = a;</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>( b != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(b % <span class="number">2</span> == <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="comment">// 最后两位数相乘再对10取余 ans = ans * base ans % 10 = (ans%10)*(base%10)%10</span></span><br><span class="line">            ans = (ans % <span class="number">10</span>) * (base % <span class="number">10</span>) % <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 因为求最后一位数字，每次记录最后一位数字是几就可以避免越界</span></span><br><span class="line">        base = (base % <span class="number">10</span>)* (base % <span class="number">10</span>) % <span class="number">10</span>;</span><br><span class="line">        b /= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> t,n;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; t;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,f(n,n));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5-阶乘位数"><a href="#5-阶乘位数" class="headerlink" title="5. 阶乘位数"></a>5. 阶乘位数</h3><p>​        给定一个小于等于$10,000,000$的正整数，计算其阶乘的位数。小于1000的数字的阶乘可以通过模拟乘法来计算出来，但是当数字特别大数组也无法存下。这是就要用到斯特林公式。</p>
<p>​        一个十进制数 $n$ 的位数可以表示为：</p>
<script type="math/tex; mode=display">
(int)\log_{10} n + 1</script><p>这一点可以很明显的在$y = \log_{10} x$的函数图像上观察出来。</p>
<p>​        因此，$n!$的位数即：</p>
<script type="math/tex; mode=display">
\log_{10} n! + 1</script><script type="math/tex; mode=display">
\log_{10} n! = \log_{10} n * (n-1) * ... * 1 = \log_{10}n + \log_{10}n-1 + ...</script><p>​        只要循环计算1到n的对数再求和之后取整加一就是结果。</p>
<h3 id="6-背包问题"><a href="#6-背包问题" class="headerlink" title="6. 背包问题"></a>6. 背包问题</h3><p>参考文献：背包问题九讲，崔添翼。</p>
<h4 id="6-1-0-1背包"><a href="#6-1-0-1背包" class="headerlink" title="6.1 0-1背包"></a>6.1 0-1背包</h4><p><strong>问题描述：</strong>给定容量为 $V$ 的背包，有 $n$ 个物品，每个物品有价值 $w$、体积 $c$ 两个属性，求背包能装的物品最大价值。</p>
<p><strong>求解：</strong>二维数组 $dp$，$dp[i][j]$ 表示有 $i$ 个物品，背包容量为 $j$ 的情况下，此时的背包最大价值；$dp[n][V]$即为答案</p>
<p>状态转移公式：</p>
<script type="math/tex; mode=display">
dp[i][j] = \max(dp[i-1][j], dp[i-1][j - c[i]] + w[i])</script><p><strong>理解状态转移公式</strong>：</p>
<p>对于每个背包都有两种选择：装与不装（装的前提是当前容量 $j$ 能装下 $c[i]$），不装的话就是前 $i-1$ 个物品容量为 $j$ 时的价值 $dp[i][j]$；若选择装，则背包需要提前给当前物品留下 $c[i]$ 的空间。</p>
<p>代码实现（杭电2602号问题）：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> t, n, V;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1005</span>][<span class="number">1005</span>], v[<span class="number">1005</span>], w[<span class="number">1005</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;t);</span><br><span class="line">    <span class="keyword">while</span>(t--)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;n, &amp;V);</span><br><span class="line">        <span class="comment">// 输入n件物品的价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;v[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 输入n件物品的体积</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;w[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// dp[i][j]代表了到第i个物品，背包容量为j时的最大价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= V; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(j &gt;= w[i])&#123;</span><br><span class="line">                    dp[i][j] = <span class="built_in">max</span>(dp[i<span class="number">-1</span>][j - w[i]] + v[i], dp[i<span class="number">-1</span>][j]);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, dp[n][V]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>空间压缩：</strong></p>
<script type="math/tex; mode=display">
dp[j] = \max(dp[j], dp[j - c[i]] + w[i])</script><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> t, n, V;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1005</span>], v[<span class="number">1005</span>], w[<span class="number">1005</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;t);</span><br><span class="line">    <span class="keyword">while</span>(t--)&#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;n, &amp;V);</span><br><span class="line">        <span class="comment">// 输入n件物品的价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;v[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memset</span>(dp, <span class="number">0</span>, <span class="keyword">sizeof</span>(dp));</span><br><span class="line">        <span class="comment">// 输入n件物品的体积</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;w[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// dp[j]代表背包容量为j时的最大价值</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= w[i]; j--)&#123;<span class="comment">//注意逆序</span></span><br><span class="line">                dp[j] = <span class="built_in">max</span>(dp[j], dp[j - w[i]] + v[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, dp[V]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>初始化的细节问题</strong></p>
<ul>
<li><p>当有些问题要求“恰好装满背包”时的最优解，这时在初始化的时候，dp数组除了dp[0]为0，其余dp[1…V]均设置为无穷大。可以这样理解：初始化的dp数组事实上就是在没有任何物品可以放入背包时的合法状态。如果要求背包恰好装满，那么此时只有容量为$0$的背包可以在什么也不装且价值为 $0$ 的情况下被“恰好装满”，其它容量的背包均没有合法的解，属于未定义的状态，应该被赋值为无穷大了。如果背包并非必须被装满，那么任何容量的背包都有一个合法解“什么都不装”，这个解的价值为0，所以初始时状态的值也就全部为0了。</p>
</li>
<li><p>还有一种情况就是求背包的最小价值，把转移方程中的$max$改为$min$即可。</p>
</li>
<li>有些题目中，只给了价值或者重量这一个数组，那么此时我们可以认为数值上价值与重量时一致的。比如只给了价值 $v$，此时01背包状态转移公式就成了：$dp[j+1] = \max(dp[j],dp[j - v[i]] + v[i]),j \in [V,v[i]]] $。</li>
</ul>
<p>杭电1114号问题就是以上两种情况的结合。题目大意就是给定存钱罐初始重量和装满时候的重量，然后给定几种不同价值和重量的硬币，问能够恰好装满存钱罐的最小价值是多少？每个种类的硬币可以无限使用，这是一个<strong>完全背包问题</strong>。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> m, n, t, N;</span><br><span class="line"><span class="keyword">int</span> v[<span class="number">502</span>], w[<span class="number">502</span>]; <span class="comment">// 硬币价值 和 硬币重量数组</span></span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">10005</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> inf = <span class="number">0x3f3f3f3f</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;N);</span><br><span class="line">    <span class="keyword">while</span>(N--)&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//输入初始存钱罐重量和现在存钱罐重量</span></span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;m, &amp;n);</span><br><span class="line">        <span class="keyword">int</span> V = n - m; <span class="comment">// 钱币重量</span></span><br><span class="line">        <span class="comment">// 输入硬币种类</span></span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;t);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>,&amp;v[i], &amp;w[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 注意初始化</span></span><br><span class="line">        <span class="built_in">memset</span>(dp, inf, <span class="keyword">sizeof</span>(dp));</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 找到恰好能装满存钱罐重量的钱币价值中最小的那个</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; t; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = w[i]; j &lt;= V; j++)&#123;<span class="comment">// 注意这是顺序</span></span><br><span class="line">                dp[j] = min(dp[j - w[i]] + v[i], dp[j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(dp[V] == inf)&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"This is impossible.\n"</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"The minimum amount of money in the piggy-bank is %d.\n"</span>,dp[V]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="6-2-完全背包"><a href="#6-2-完全背包" class="headerlink" title="6.2 完全背包"></a>6.2 完全背包</h4><p><strong>定义</strong></p>
<p>有 $N$ 种物品和一个容量为 $V$ 的背包，每种物品都有<strong>无限件</strong>可用。放入第 $i$ 种物品的费用是 $c_i$，价值是 $w_i$。求解：将哪些物品装入背包，可使这些物品的耗费的费用总和不超过背包容量，且价值总和最大。</p>
<p><strong>求解</strong></p>
<p>虽然物品个数是无限的，但是实际上，由于背包容量有上限，每个物品最多选取的个数也是有限制的，这样可以转换成多重背包问题，进而可以转换成 01 背包问题。</p>
<script type="math/tex; mode=display">
dp[i][j] = \max(dp[i-1][j-kc_i] + kw_i|0 \le kc_i \le j)</script><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= v; j++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k*c[i] &lt;= j; k++)&#123;</span><br><span class="line">            dp[i][j] = max(dp[i<span class="number">-1</span>][j - k * c[i]] + w[i] * k, dp[i][j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>观察状态转移公式，我们可以用$dp[i][j - c_i]$去更新$dp[i][j]$而不用去枚举$k$了，因此状态转移公式就变成：</p>
<script type="math/tex; mode=display">
dp[i][j] = \max(dp[i-1][j],dp[i-1][j-c_i]+w_i)</script><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;= v; j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j &gt;= c[i])&#123;</span><br><span class="line">            dp[i][j] = max(dp[i<span class="number">-1</span>][j], dp[i<span class="number">-1</span>][j-c[i]] + w[i]);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>空间压缩</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1010</span>];</span><br><span class="line"><span class="keyword">int</span> w[<span class="number">21</span>],c[<span class="number">21</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N, V;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; N &gt;&gt; V;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="built_in">cin</span> &gt;&gt; w[i] &gt;&gt; c[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= N; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = c[i]; j &lt;= V; j++)&#123; <span class="comment">// 顺序</span></span><br><span class="line">            dp[j] = max(dp[j- c[i]] + w[i],dp[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; dp[V] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到完全背包就是在第二重循环遍历次序与01背包不同而已，01背包是逆序，完全背包是顺序。</p>
<h4 id="6-3-多重背包"><a href="#6-3-多重背包" class="headerlink" title="6.3 多重背包"></a>6.3 多重背包</h4><p><strong>定义</strong></p>
<p>有 $N$ 种物品和一个容量为 $V$ 的背包。第 $i$ 种物品最多有 $n_i$ 件可用，每件耗费的空间是 $c_i$，价值是 $w_i$。求解将哪些物品装入背包可使这些物品的耗费的空间总和不超过背包容量，且价值总和最大。</p>
<p><strong>求解</strong></p>
<p><strong>多重背包的二进制优化</strong></p>
<p>二进制思想即任意数字都可以由2的次幂的数字组合而来（用01表示），这是计算机的基础。利用这个思想，我们可以把每个物品数量分成1，2，4，…，n - 2^K + 1，每一组的体积和价值分别为（ci，wi），（2ci，2wi），（4ci，4wi），等等，通过这些组合（选择装与不装）一定可以组成n，这样就遍历了1-n内所有数字。通过拆分所有物品，就形成了一个新的价值和体积物品组，这些问题组我们可以选择装也可以选择不装，这样利用二进制优化就可以将多重背包转化为01背包问题。</p>
<p>代码实现：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 二进制优化</span></span><br><span class="line"><span class="keyword">int</span> ncnt = <span class="number">1</span>; <span class="comment">// 记录新的拆分组数</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= t; i++)&#123; <span class="comment">// t个物品</span></span><br><span class="line">    <span class="comment">// 将num[i]拆分</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">1</span>; k &lt;= num[i]; k &lt;&lt;=<span class="number">1</span>)&#123;</span><br><span class="line">        nv[ncnt] = k * v[i];<span class="comment">//新拆分的价值数组</span></span><br><span class="line">        nw[ncnt++] = k * w[i];<span class="comment">//新拆分的重量数组</span></span><br><span class="line">        num[i] -= k;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(num[i] &gt; <span class="number">0</span>)&#123;</span><br><span class="line">        nv[ncnt] = v[i] * num[i];</span><br><span class="line">        nw[ncnt++] = w[i] * num[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 01背包</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; ncnt; i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = V; j &gt;= nw[i]; j--)&#123;</span><br><span class="line">        dp[j] = max(dp[j - nw[i]] + nv[i], dp[j]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>杭电1059、2844号题是完全背包的二进制优化。</p>
<h4 id="6-4-有约束的背包问题"><a href="#6-4-有约束的背包问题" class="headerlink" title="6.4 有约束的背包问题"></a>6.4 有约束的背包问题</h4><p>背包练习(以下题目皆来自杭电 on-line judge: <a href="http://acm.hdu.edu.cn/" target="_blank" rel="noopener">http://acm.hdu.edu.cn/</a> ，做完还不会背包来找我。</p>
<p><a href="http://acm.hdu.edu.cn/showproblem.php?pid=2602" target="_blank">2602</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1114" target="_blank">1114</a>  <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1171" target="_blank">1171</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=2844" target="_blank">2844</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1059" target="_blank">1059</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=2955" target="_blank">2955</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=1203" target="_blank">1203</a> <a href="http://acm.hdu.edu.cn/showproblem.php?pid=3466" target="_blank">3466</a></p>
<h3 id="7-三分法"><a href="#7-三分法" class="headerlink" title="7. 三分法"></a>7. 三分法</h3><p>二分法能够有效解决有序序列的问题，然而当一个序列先增大后减小（或者先减小后增大）即存在一个峰值时，就要用到三分的思想了。三分和二分有点像，三分是在left和right指针区间内，设置mid1、mid2两个指针，即mid1与mid2将[left，right]区间三等分。之后根据具体问题更新left和right指针位置，即可能：left = mid1/right = mid2。</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Graph Neutral Network和Graph Embedding" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/07/07/Graph%20Neutral%20Network%E5%92%8CGraph%20Embedding/"
    >Graph Neutral Network和Graph Embedding</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/07/07/Graph%20Neutral%20Network%E5%92%8CGraph%20Embedding/" class="article-date">
  <time datetime="2020-07-07T09:14:50.127Z" itemprop="datePublished">2020-07-07</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="Graph-Neutral-Network和Graph-Embedding"><a href="#Graph-Neutral-Network和Graph-Embedding" class="headerlink" title="Graph Neutral Network和Graph Embedding"></a>Graph Neutral Network和Graph Embedding</h2><h3 id="0-概述"><a href="#0-概述" class="headerlink" title="0. 概述"></a>0. 概述</h3><h4 id="0-1-图的基础知识"><a href="#0-1-图的基础知识" class="headerlink" title="0.1 图的基础知识"></a>0.1 图的基础知识</h4><p>​        通常定义一个图 $G=(V,E)$，其中 $V$为 <strong>顶点（Vertices）</strong>集合，$E$为<strong>边（Edges）</strong>集合。对于一条边$e=(u,v)$ 包含两个<strong>端点（Endpoints）</strong> u 和 v，同时 u 可以称为 v 的<strong>邻居（Neighbor）</strong>。当所有的边为有向边时，图称之为<strong>有向（Directed）</strong>图，当所有边为无向边时，图称之为<strong>无向（Undirected）</strong>图。在无向图中，对于一个顶点 v，令$d(v) $表示连接的边的数量，称之为<strong>度（Degree）</strong>；有向图中又分为<strong>入度</strong>和<strong>出度</strong>。</p>
<p>​        对于一个无向图 $G=(V,E)$，其<strong>邻接矩阵（Adjacency Matrix）</strong>$A$通常定义为：</p>
<script type="math/tex; mode=display">
A_{ij} = \left\{  
             \begin{array}{**lr**}  
             1，i\not=j&if\ vi\ is\ adjacent\ to\ vj\\
             0, &otherwise 
             \end{array}  
\right.</script><p>​        <img src="/images/graph.png" alt="image-20200707174530868"></p>
<p>​        对于上面这个无向图，其度矩阵（degree matrix）如下。度矩阵$D$是一个对角矩阵。</p>
<p><img src="/images/image-20200707174724389.png" alt="image-20200707174724389"></p>
<p>​        图的拉普拉斯矩阵$L$定义为，在图论中，作为一个图的矩阵表示。拉普拉斯矩阵是对称的（Symmetric）。</p>
<script type="math/tex; mode=display">
L = D - A</script><p>​        另外一种更为常用的的是正则化的拉普拉斯矩阵（Symmetric normalized Laplacian）。</p>
<script type="math/tex; mode=display">
L^{sym} = D^{-\frac{1}{2}}LD^{-\frac{1}{2}} = I - D^{-\frac{1}{2}}AD^{-\frac{1}{2}}</script><p>​        说明：$I$为单位矩阵；$D^{-\frac{1}{2}}$表示度矩阵对角线上的元素开平方根取倒数。</p>
<h4 id="0-2-二者不同"><a href="#0-2-二者不同" class="headerlink" title="0.2 二者不同"></a>0.2 二者不同</h4><p>​        图嵌入旨在通过保留图的网络<strong>拓扑结构和节点内容信息</strong>，将图中顶点表示为低维向量，以便使用简单的机器学习算法（例如，支持向量机分类）进行处理（摘自知乎：图神经网络（Graph Neural Networks）综述，作者：苏一。<a href="https://zhuanlan.zhihu.com/p/75307407）。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/75307407）。</a></p>
<p>​        图神经网络是用于处理图数据（非欧式空间）的神经网络结构。</p>
<p>​        图嵌入和图神经网络的区别与联系。</p>
<p><img src="https://d33wubrfki0l68.cloudfront.net/3c5b07652ee4f2012f3ce1b9cf25bf8282a4f9f4/abdc9/images/cn/2020-04-11-graph-embedding-and-gnn/graph-embedding-vs-graph-neural-networks.png" alt=""></p>
<h3 id="1-Graph-Embedding"><a href="#1-Graph-Embedding" class="headerlink" title="1. Graph Embedding"></a>1. Graph Embedding</h3><p>​        Embedding在数学上是一个函数，将一个空间的点映射到另一个空间，通常是从高维抽象的空间映射到低维具象的空间，并且在低维空间保持原有的语义。早在2003年，Bengio就发表论文论述word embedding想法，将词汇映射到实数向量空间。而2013年google连发两篇论文推出word embedding的神经网咯工具包：skip-gram、cbow（连续词袋模型），使得embedding技术成为深度学习的基本操作，进而导致万物皆可embedding。</p>
<p>​        而基于图的embedding又可以分为基于顶点（vertex）和基于图（graph）。前者主要是将给定的图数据中的vertex表示为单独的向量（vector），后者将整个图进行embedding表示，之后可以进行图的分类等工作。下面分别介绍。</p>
<h4 id="1-1-Vertex-Embedding"><a href="#1-1-Vertex-Embedding" class="headerlink" title="1.1 Vertex Embedding"></a>1.1 Vertex Embedding</h4><h5 id="1-1-1-DeepWalk"><a href="#1-1-1-DeepWalk" class="headerlink" title="1.1.1 DeepWalk"></a>1.1.1 DeepWalk</h5><p>Perozzi B, Al-Rfou R, Skiena S. Deepwalk: Online learning of social representations[C]//Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014: 701-710.</p>
<p><img src="/images/image-20200708200747355.png" alt="image-20200708200747355"></p>
<p>即：基于图上的随机游走进行节点采样，之后将采样到的节点集（每个节点feature使用one-hot表示或者随机向量）输入到skip-gram模型进行训练，得到节点的embedding。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RandomWalk</span><span class="params">(node,t)</span>:</span></span><br><span class="line">    walk = [node]        <span class="comment"># Walk starts from this node</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(t<span class="number">-1</span>):</span><br><span class="line">        <span class="comment"># 应先统计adj_list[node]对应的为1表示相邻的节点索引列表，从该列表中随机选取索引</span></span><br><span class="line">        adj_nodes = np.array(adj_list[node]).nonzero()[<span class="number">0</span>]<span class="comment"># nonzero返回一个元组</span></span><br><span class="line">        </span><br><span class="line">        node = adj_list[node][adj_nodes[random.randint(<span class="number">0</span>,len(adj_nodes)<span class="number">-1</span>)]]</span><br><span class="line">        walk.append(node)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> walk</span><br></pre></td></tr></table></figure>
<h5 id="1-1-2-LINE"><a href="#1-1-2-LINE" class="headerlink" title="1.1.2 LINE"></a>1.1.2 LINE</h5><p>Tang J, Qu M, Wang M, et al. Line: Large-scale information network embedding[C]//Proceedings of the 24th international conference on world wide web. 2015: 1067-1077.</p>
<p>​        相似度定义</p>
<p><strong>first-order proximity</strong></p>
<p>​        <img src="/images/image-20200709225053851.png" alt="image-20200709225053851"></p>
<p>​        1阶相似度用于描述图中成对顶点之间的局部相似度，形式化描述为若$u,v$之间存在直连边，则边权$w_{uv}$即为两个顶点的相似度，若不存在直连边，则1阶相似度为0。 如上图，6和7之间存在直连边，且边权较大，则认为两者相似且1阶相似度较高，而5和6之间不存在直连边，则两者间1阶相似度为0。</p>
<p><strong>second-order proximity</strong></p>
<p>​        仅有1阶相似度就够了吗？显然不够，如上图，虽然5和6之间不存在直连边，但是他们有很多相同的邻居顶点(1,2,3,4)，这其实也可以表明5和6是相似的，而2阶相似度就是用来描述这种关系的。 形式化定义为，令$p_u = (w_{u,1},…,w_{u,|V|})$ 表示顶点$u$与<strong>所有其他</strong>顶点间的1阶相似度，则  $u$ 与 $v$ 的2阶相似度可以通过 $p_{u}$ 和  $p_v$ 的相似度表示（两个顶点他们的邻居集合的相似程度）。若$u$与$v$之间不存在相同的邻居顶点，则2阶相似度为0。</p>
<p><strong>目标函数</strong></p>
<p>​        <strong>1st-order</strong>（用于无向图）</p>
<p>​        对于每条边集$E$中的任一条边$(i,j)$，邻接的两个节点$p_i,p_j$的联合分布定义为：</p>
<script type="math/tex; mode=display">
p(v_i,v_j) = \frac{1}{1 + \exp(-u_i·u_j)}</script><p>​        $u_i,u_j$即节点$v_i,v_j$的低维embedding表示。同时定义经验分布$\hat p$：</p>
<script type="math/tex; mode=display">
\hat p(i,j) = \frac{w_{ij}}{W},W = \sum_{(i,j)\in E}w_{ij}</script><p>​        那么，目标函数就是尽可能地减小这两个分布的差异，衡量两个分布差异可以使用KL散度来衡量，进而：</p>
<script type="math/tex; mode=display">
O_1 = - \sum_{(i,j)\in E}w_{ij}\log p(v_i,v_j)</script><p>​        <strong>2nd-order</strong></p>
<script type="math/tex; mode=display">
O_2 = - \sum_{(i,j)\in E} w_{ij}\log p(v_j|v_i)</script><p>​        当然也可以使用二者的结合。</p>
<h5 id="1-1-3-Node2Vec"><a href="#1-1-3-Node2Vec" class="headerlink" title="1.1.3 Node2Vec"></a>1.1.3 Node2Vec</h5><p>Grover A, Leskovec J. node2vec: Scalable feature learning for networks[C]//Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. 2016: 855-864.</p>
<p>​        本质上也是基于随机游走，提出了一种新的节点采样策略，有“导向”的游走，即加入了参数来控制从某个节点到其他节点的概率。采样得到的节点集，<strong>仍使用word2vec形式进行训练</strong>。</p>
<p>​        如下图。p（return parameter）、q（in-out parameter）为超参数。</p>
<p>​        下图中，现处于节点v，上一个节点是t，那么从节点v到节点$t$、$x_1$、$x_2$、$x_3$的概率$\pi_{vx}$计算公式如下：</p>
<script type="math/tex; mode=display">
\alpha(t,x) = \frac{1}{p},d_{t,x} = 0</script><script type="math/tex; mode=display">
\alpha(t,x) = 1,d_{t,x} = 1</script><script type="math/tex; mode=display">
\alpha(t,x) = \frac{1}{q},d_{t,x} = 2</script><script type="math/tex; mode=display">
\pi_{vx}=\alpha(t,x) w_{v,x}</script><p>​        其中，$d_{tx}$表示<strong>上一节点和下一个能到达的节点的距离</strong>，$w_{vx}$即节点v与next节点连接边的权重（无向图中为1）。下图中，节点$t$与$x_1$直接有边相连接，$d$为1；$t$与$x_2$、$x_3$距离为2（不相邻）；特别的，与上一个节点距离为0，概率为$\frac{1}{p}$。</p>
<p><img src="/images/image-20200709201700829.png" alt="image-20200709201700829"></p>
<p>​        基于这种随机游走策略，使得该模型可以体现网络的同质性（homophily）或结构性（structural equivalence）。网络的“同质性”指<strong>距离相近</strong>的节点的embedding应尽量相似；“结构性”指的是<strong>结构上相似</strong>的节点的embedding应尽量相似。</p>
<p>​        为了能让graph embedding的结果能够表达网络的“结构性”，需要让游走的过程更倾向与<strong>BFS</strong>，因为BFS会更多的在当前节点的领域中游走遍历；为了表达”同质性”，则需要让游走过程倾向于<strong>DFS</strong>。</p>
<p>​        现在讨论超参数p、q，q越大，则随机游走到远方节点的可能性更大，随机游走策略就近似于DFS；反之，近似于BFS。</p>
<h5 id="1-1-4-SDNE"><a href="#1-1-4-SDNE" class="headerlink" title="1.1.4 SDNE"></a>1.1.4 SDNE</h5><p>Wang D, Cui P, Zhu W. Structural deep network embedding[C]//Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. 2016: 1225-1234.</p>
<p>​        SDNE基于LINE研究的基础上，采用deep encoder-decoder模型进行embedding。</p>
<p>​        </p>
<h4 id="1-2-Graph-Embedding"><a href="#1-2-Graph-Embedding" class="headerlink" title="1.2 Graph Embedding"></a>1.2 Graph Embedding</h4><h3 id="2-Graph-Neural-Network"><a href="#2-Graph-Neural-Network" class="headerlink" title="2. Graph Neural Network"></a>2. Graph Neural Network</h3><p>​        图神经网络目前主要的任务包括：节点分类（node classification）、图分类（graph classification）、graph representation、link predication等等。</p>
<h4 id="2-1-Neighborhood-Aggravating"><a href="#2-1-Neighborhood-Aggravating" class="headerlink" title="2.1 Neighborhood Aggravating"></a>2.1 Neighborhood Aggravating</h4><p>​        <strong>neighborhood aggravating即用节点的neighbor feature更新下一个的hidden state。</strong></p>
<p>​        给定一张图，我们可以用与与节点邻接的节点集去表示该节点。下图中，A与B、C、D相邻接，而B又与A、C邻接等等，每个节点都可以使用与其相邻接的节点进行表示（aggravating）（每个节点都有一个初始状态【特征】，当前节点状态用其他节点的上一个状态【特征】表示）。通过其邻接节点聚合，这个节点就可以学到图的结构。</p>
<p><img src="/images/g1.png" alt="image-20200707215031730" style="zoom:67%;" /></p>
<p><img src="E:\hexo\themes\ayer\source\images\image-20200707215123250.png" alt="image-20200707215123250" style="zoom:67%;" /></p>
<p><img src="/images/image-20200707215155395.png" alt="image-20200707215155395" style="zoom:67%;" /></p>
<p>​        因此，我们可以定义任意层的网络来聚合各个节点的信息。（如下图，注：并非完整）</p>
<p><img src="/images/image-20200707215535544.png" alt="image-20200707215535544" style="zoom: 67%;" /></p>
<p>​        注：图中方块表示任意的聚合函数。</p>
<p>​        数学表示如下：</p>
<script type="math/tex; mode=display">
h_v^k = \sigma(W_k\sum_{u \in N(v)}\frac{h_u^{k-1}}{|N(v)|} + B_kh_v^{k-1}),k > 0</script><script type="math/tex; mode=display">
h_v^0 = x_v</script><p>​        其中，k表示第k层（layer）；v表示节点；$\sigma$表示激活函数；$h_u$表示节点$u$的状态；$N(v)$表示节点$v$的邻接节点集合；$|N(v)|$即邻接节点数量；$W、B$分别为权重矩阵和偏置（bias）。</p>
<p>​        因此，我们只需定义一个聚合函数（sum、mean、max-pooling等），以及损失函数（比如：基于节点分类的交叉熵损失函数等），就可以开始迭代训练，最终得到各个节点的embedding向量。</p>
<h4 id="2-2-Graph-Convolution-Networks"><a href="#2-2-Graph-Convolution-Networks" class="headerlink" title="2.2 Graph Convolution Networks"></a>2.2 Graph Convolution Networks</h4><p>​        卷积网络大致分类如下图。</p>
<p><img src="/images/image-20200707223107082.png" alt="image-20200707223107082"></p>
<h5 id="2-2-1-Spatial-based"><a href="#2-2-1-Spatial-based" class="headerlink" title="2.2.1 Spatial-based"></a>2.2.1 Spatial-based</h5><p>​        空间卷积网络也是基于neighborhood aggravating的思想。</p>
<ul>
<li><p>Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.</p>
<p>这篇论文采用的图卷积网络在neighborhood aggravating上做出了一点改动。</p>
</li>
</ul>
<script type="math/tex; mode=display">
h_v^k = \sigma(W_k\sum_{u\in N(v)\cup v}\frac{h_u^{k-1}}{\sqrt{|N(u)||N(v)|}})</script><p>​        <strong>矩阵形式</strong>如下：</p>
<script type="math/tex; mode=display">
H^{(k+1)} = \sigma(D^{-\frac{1}{2}}\hat AD^{\frac{1}{2}}H^{(k)}W_k) \\
\hat A = A + I</script><p>​        其中$A$为图邻接矩阵，$D$为度矩阵，$I$为单位矩阵。</p>
<p>​        下面看一下具体的矩阵形式</p>
<p><img src="/images/g2.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面这张图的邻接矩阵如下</span></span><br><span class="line">&gt; A = torch.tensor(np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]]))</span><br><span class="line"><span class="comment"># 度矩阵 D，即邻接矩阵每一行求和</span></span><br><span class="line">&gt; D = torch.diag(torch.sum(torch.tensor(adj),<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">&gt; X = torch.tensor(np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]))</span><br><span class="line"><span class="comment"># 每个节点的邻居节点聚合等价于A*X</span></span><br><span class="line">&gt; torch.mm(A,X)</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]], dtype=torch.int32)</span><br><span class="line"><span class="comment"># 从结果可以看出相当于对每个节点的所有邻居节点特征求和</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义权重矩阵 W，将input维度映射到output维度，本例input=3</span></span><br><span class="line">&gt; out = <span class="number">4</span></span><br><span class="line">&gt; W = W = torch.randn(<span class="number">3</span>,out)</span><br><span class="line"><span class="comment"># 完成聚合 AXW</span></span><br><span class="line">&gt; output = torch.mm(torch.mm(A,X).type_as(torch.FloatTensor()),W)</span><br><span class="line"><span class="comment"># 经过激活函数,得到这一阶段的hidden（下一阶段的输入）</span></span><br><span class="line">&gt; hidden = F.relu(output)</span><br><span class="line"><span class="comment"># hidden [num_of_vertex, hidden_dims]</span></span><br></pre></td></tr></table></figure>
<p>​        但是Kipf等人在他的论文中，对邻接矩阵$A$进行了标准化。即上文采用的公式。下面实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义图卷积</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCNConv</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, A, in_channels, out_channels)</span>:</span></span><br><span class="line">        super(GCNConv, self).__init__()</span><br><span class="line">        self.A_hat = A+torch.eye(A.size(<span class="number">0</span>))</span><br><span class="line">        self.D     = torch.diag(torch.sum(A,<span class="number">1</span>))</span><br><span class="line">        self.D     = self.D.inverse().sqrt() <span class="comment"># D是对角矩阵，对角线元素开根号</span></span><br><span class="line">        self.A_hat = torch.mm(torch.mm(self.D, self.A_hat), self.D)</span><br><span class="line">        self.W     = nn.Parameter(torch.rand(in_channels,out_channels, requires_grad=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        out = torch.relu(torch.mm(torch.mm(self.A_hat, X), self.W))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="comment"># 定义GCN  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,A, input_dims, nhid, out_dims)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = GCNConv(A,input_dims, nhid)</span><br><span class="line">        self.conv2 = GCNConv(A,nhid, out_dims)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,X)</span>:</span></span><br><span class="line">        H  = self.conv1(X)</span><br><span class="line">        H2 = self.conv2(H)</span><br><span class="line">        <span class="keyword">return</span> H2</span><br></pre></td></tr></table></figure>
<h5 id="2-2-2-Spectral-based"><a href="#2-2-2-Spectral-based" class="headerlink" title="2.2.2 Spectral-based"></a>2.2.2 Spectral-based</h5><p>​         谱图卷积网络基于图的信号处理，即图的拉普拉斯矩阵进行傅里叶变化（Fourier Transform）与逆变化（Inverse Fourier Transform），</p>
<p>​    <strong>图上的傅里叶变换</strong></p>
<p>​        傅里叶变换可以将<strong>时域</strong>信号转为<strong>频域信号</strong>。时域即信号大小随时间而改变，其图像在二维平面中，横轴为时间，纵轴为信号大小，其图像是连续的；而频域图像，横轴代表频率大小，纵轴代表信号大小，其图像是离散的。频域图像本质上描述了一段信号中包含的具体成分（不同频率信号的叠加）如何。</p>
<p>​        而图上的傅里叶变换用到了图的拉普拉斯矩阵$L$，因为$L$是半正定矩阵，因此其特征值都非负。对其进行特征值分解有：</p>
<script type="math/tex; mode=display">
L = U \Lambda U^T</script><p>，其中$U$为特征向量，$\Lambda$为特征值矩阵，是一个对角矩阵，$\lambda_1,\lambda_2…$即特征值，且$\lambda_1&lt;=\lambda_2&lt;=…&lt;=\lambda_n$。$\lambda$也代表了图上的频率。</p>
<p>​        下面直接给出图上傅里叶变换公式，具体推导略去。</p>
<p>​        给定图中某顶点$v$，其对应的信号（特征）为$x$，那么其傅里叶变换即：</p>
<script type="math/tex; mode=display">
\hat x = U^Tx</script><p>​        可知$\hat x$为频域上信号，要重新换变换顶点域，就需要逆傅里叶变换即：</p>
<script type="math/tex; mode=display">
y = U\hat x</script><p><strong>谱图卷积</strong></p>
<p>​        要进行卷积，就需要在频域上进行卷积，即将顶点上的信号进行傅里叶变换后，定义对应的滤波器（滤波函数）$g_\theta(\lambda)$对其进行处理，再利用逆傅里叶变换还原为顶点域中。这里的滤波器或者说关于$\lambda$的滤波函数就是我们要通过训练学习的。说是关于$\lambda$的函数，就是说根据不同的$\lambda$给出不同的<strong>相应</strong>$\theta$。</p>
<p>​        由此，图上信号卷积即：</p>
<script type="math/tex; mode=display">
y = U\hat x = Ug_{\theta}(\Lambda)U^Tx</script><p>​        但上面这样定义仍存在几个问题，一是当图太大时矩阵分解计算复杂度太高，二是这并非是局部的（localized）。进而又提出了ChebNet模型（Defferrard M, Bresson X, Vandergheynst P. Convolutional neural networks on graphs with fast localized spectral filtering[C]. Advances in neural information processing systems. 2016: 3844-3852.）。</p>
<p>​        第一类切比雪夫多项式定义如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
T_0(x) &= 1 \\
T_1(x) &= x \\
T_n(x) &= 2xT_{n-1}(x) - T_{n-2}(x)
\end{aligned}</script><p>​        代码参考：<a href="https://github.com/dovelism/Traffic_Data_Projection_Using_GCN/" target="_blank" rel="noopener">https://github.com/dovelism/Traffic_Data_Projection_Using_GCN/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChebConv</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    The ChebNet convolution operation.</span></span><br><span class="line"><span class="string">    :param in_c: int, number of input channels.</span></span><br><span class="line"><span class="string">    :param out_c: int, number of output channels.</span></span><br><span class="line"><span class="string">    :param K: int, the order of Chebyshev Polynomial.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, out_c, K, bias=True, normalize=True)</span>:</span></span><br><span class="line">        super(ChebConv, self).__init__()</span><br><span class="line">        self.normalize = normalize</span><br><span class="line"></span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(K + <span class="number">1</span>, <span class="number">1</span>, in_c, out_c))  <span class="comment"># [K+1, 1, in_c, out_c]</span></span><br><span class="line">        init.xavier_normal_(self.weight)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.Tensor(<span class="number">1</span>, <span class="number">1</span>, out_c))</span><br><span class="line">            init.zeros_(self.bias)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">"bias"</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        self.K = K + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs, graph)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param inputs: the input data, [B, N, C]</span></span><br><span class="line"><span class="string">        :param graph: the graph structure, [N, N]</span></span><br><span class="line"><span class="string">        :return: convolution result, [B, N, D]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        L = ChebConv.get_laplacian(graph, self.normalize)  <span class="comment"># [N, N]</span></span><br><span class="line">        mul_L = self.cheb_polynomial(L).unsqueeze(<span class="number">1</span>)   <span class="comment"># [K, 1, N, N]</span></span><br><span class="line"></span><br><span class="line">        result = torch.matmul(mul_L, inputs)  <span class="comment"># [K, B, N, C]</span></span><br><span class="line">        result = torch.matmul(result, self.weight)  <span class="comment"># [K, B, N, D]</span></span><br><span class="line">        result = torch.sum(result, dim=<span class="number">0</span>) + self.bias  <span class="comment"># [B, N, D]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cheb_polynomial</span><span class="params">(self, laplacian)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute the Chebyshev Polynomial, according to the graph laplacian.</span></span><br><span class="line"><span class="string">        :param laplacian: the graph laplacian, [N, N].</span></span><br><span class="line"><span class="string">        :return: the multi order Chebyshev laplacian, [K, N, N].</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        N = laplacian.size(<span class="number">0</span>)  <span class="comment"># [N, N]</span></span><br><span class="line">        multi_order_laplacian = torch.zeros([self.K, N, N], device=laplacian.device, dtype=torch.float)  <span class="comment"># [K, N, N]</span></span><br><span class="line">        multi_order_laplacian[<span class="number">0</span>] = torch.eye(N, device=laplacian.device, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.K == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> multi_order_laplacian</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            multi_order_laplacian[<span class="number">1</span>] = laplacian</span><br><span class="line">            <span class="keyword">if</span> self.K == <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">return</span> multi_order_laplacian</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>, self.K):</span><br><span class="line">                    multi_order_laplacian[k] = <span class="number">2</span> * torch.mm(laplacian, multi_order_laplacian[k<span class="number">-1</span>]) - multi_order_laplacian[k<span class="number">-2</span>]</span><br><span class="line">        <span class="keyword">return</span> multi_order_laplacian</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_laplacian</span><span class="params">(graph, normalize)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        return the laplacian of the graph.</span></span><br><span class="line"><span class="string">        :param graph: the graph structure without self loop, [N, N].</span></span><br><span class="line"><span class="string">        :param normalize: whether to used the normalized laplacian.</span></span><br><span class="line"><span class="string">        :return: graph laplacian.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> normalize:</span><br><span class="line">            D = torch.diag(torch.sum(graph, dim=<span class="number">-1</span>) ** (<span class="number">-1</span> / <span class="number">2</span>))</span><br><span class="line">            L = torch.eye(graph.size(<span class="number">0</span>), device=graph.device, dtype=graph.dtype) - torch.mm(torch.mm(D, graph), D)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            D = torch.diag(torch.sum(graph, dim=<span class="number">-1</span>))</span><br><span class="line">            L = D - graph</span><br><span class="line">        <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChebNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, hid_c, out_c, K)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param in_c: int, number of input channels.</span></span><br><span class="line"><span class="string">        :param hid_c: int, number of hidden channels.</span></span><br><span class="line"><span class="string">        :param out_c: int, number of output channels.</span></span><br><span class="line"><span class="string">        :param K:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ChebNet, self).__init__()</span><br><span class="line">        self.conv1 = ChebConv(in_c=in_c, out_c=hid_c, K=K)</span><br><span class="line">        self.conv2 = ChebConv(in_c=hid_c, out_c=out_c, K=K)</span><br><span class="line">        self.act = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, data, device)</span>:</span></span><br><span class="line">        graph_data = data[<span class="string">"graph"</span>].to(device)[<span class="number">0</span>]  <span class="comment"># [N, N]</span></span><br><span class="line">        flow_x = data[<span class="string">"flow_x"</span>].to(device)  <span class="comment"># [B, N, H, D]</span></span><br><span class="line"></span><br><span class="line">        B, N = flow_x.size(<span class="number">0</span>), flow_x.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        flow_x = flow_x.view(B, N, <span class="number">-1</span>)  <span class="comment"># [B, N, H*D]</span></span><br><span class="line"></span><br><span class="line">        output_1 = self.act(self.conv1(flow_x, graph_data))</span><br><span class="line">        output_2 = self.act(self.conv2(output_1, graph_data))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_2.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c, hid_c, out_c)</span>:</span></span><br><span class="line">        super(GCN, self).__init__()</span><br><span class="line">        self.linear_1 = nn.Linear(in_c, hid_c)</span><br><span class="line">        self.linear_2 = nn.Linear(hid_c, out_c)</span><br><span class="line">        self.act = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, data, device)</span>:</span></span><br><span class="line">        graph_data = data[<span class="string">"graph"</span>].to(device)[<span class="number">0</span>]  <span class="comment"># [N, N]</span></span><br><span class="line">        graph_data = GCN.process_graph(graph_data)</span><br><span class="line">        flow_x = data[<span class="string">"flow_x"</span>].to(device)  <span class="comment"># [B, N, H, D]</span></span><br><span class="line">        B, N = flow_x.size(<span class="number">0</span>), flow_x.size(<span class="number">1</span>)</span><br><span class="line">        flow_x = flow_x.view(B, N, <span class="number">-1</span>)  <span class="comment"># [B, N, H*D]  H = 6, D = 1</span></span><br><span class="line"></span><br><span class="line">        output_1 = self.linear_1(flow_x)  <span class="comment"># [B, N, hid_C]</span></span><br><span class="line">        output_1 = self.act(torch.matmul(graph_data, output_1))  <span class="comment"># [N, N], [B, N, Hid_C]</span></span><br><span class="line"></span><br><span class="line">        output_2 = self.linear_2(output_1)</span><br><span class="line">        output_2 = self.act(torch.matmul(graph_data, output_2))  <span class="comment"># [B, N, 1, Out_C]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_2.unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_graph</span><span class="params">(graph_data)</span>:</span></span><br><span class="line">        N = graph_data.size(<span class="number">0</span>)</span><br><span class="line">        matrix_i = torch.eye(N, dtype=graph_data.dtype, device=graph_data.device)</span><br><span class="line">        graph_data += matrix_i  <span class="comment"># A~ [N, N]</span></span><br><span class="line"></span><br><span class="line">        degree_matrix = torch.sum(graph_data, dim=<span class="number">-1</span>, keepdim=<span class="literal">False</span>)  <span class="comment"># [N]</span></span><br><span class="line">        degree_matrix = degree_matrix.pow(<span class="number">-1</span>)</span><br><span class="line">        degree_matrix[degree_matrix == float(<span class="string">"inf"</span>)] = <span class="number">0.</span>  <span class="comment"># [N]</span></span><br><span class="line"></span><br><span class="line">        degree_matrix = torch.diag(degree_matrix)  <span class="comment"># [N, N]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.mm(degree_matrix, graph_data)  <span class="comment"># D^(-1) * A = \hat(A)</span></span><br></pre></td></tr></table></figure>
<p>​        在推出ChebNet后，Kipf等人进一步将公式化简，推出一阶（k=1）ChebNet模型（Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.），这就有了上面我们模型中采用的那个公式。</p>
<script type="math/tex; mode=display">
H^{(k+1)} = \sigma(D^{-\frac{1}{2}}\hat AD^{\frac{1}{2}}H^{(k)}W_k) \\
\hat A = A + I</script><h4 id="2-2-Graph-Attention-Networks"><a href="#2-2-Graph-Attention-Networks" class="headerlink" title="2.2 Graph Attention Networks"></a>2.2 Graph Attention Networks</h4><p>Veličković P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017.</p>
<p>​        attention通俗的讲就是输入两个向量，然后输出一个分数；是一种思想，可以有不同的实现。</p>
<p>​        GAT在空间卷积的基础上，引入了注意力机制，注意力机制也赋予了模型一定的可解释性。加入了attention后，我们aggravating的时候就需要计算当前节点和邻接节点的权重，然后进行聚合。</p>
<p>​        节点v第k个状态更新公式如下：</p>
<script type="math/tex; mode=display">
h_v^k = \sigma (\sum_{u \in N(v)}\alpha_{vu}h_u)</script><p>​        其中$\alpha$就是计算得到的attention权重。</p>
<p><img src="/images/image-20200707230726126.png" alt="image-20200707230726126" style="zoom:80%;" /></p>
<p>参考资料：</p>
<p>李宏毅，深度学习</p>
<p><a href="http://snap.stanford.edu/proj/embeddings-www/" target="_blank" rel="noopener">http://snap.stanford.edu/proj/embeddings-www/</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/56478167" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/56478167</a></p>
<p><a href="https://github.com/shenweichen/GraphEmbedding" target="_blank" rel="noopener">https://github.com/shenweichen/GraphEmbedding</a></p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-交叉熵" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/07/01/%E4%BA%A4%E5%8F%89%E7%86%B5/"
    >交叉熵</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/07/01/%E4%BA%A4%E5%8F%89%E7%86%B5/" class="article-date">
  <time datetime="2020-07-01T14:23:46.523Z" itemprop="datePublished">2020-07-01</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><ul>
<li><p>熵与交叉熵的定义：</p>
<p>信息论中熵的定义：</p>
<script type="math/tex; mode=display">
H(X) = -\sum_i p(x_i)\log p(x_i)</script><p>，其中$X$表示一个分布，$x_i$为该分布中的样本，$p(x_i)$即该样本的概率。</p>
<p><strong>在信息论中，熵是表示信息不确定性的度量。熵越大，表明信息的不确定性越大。</strong></p>
<p>两个分布$p$、$q$的交叉熵定义如下：</p>
<script type="math/tex; mode=display">
H(p,q) = -\sum_i p_i\log(q_i)</script></li>
<li><p>多分类问题中的交叉熵损失函数</p>
<p>在神经网路多分类任务时，最后一层采用$soft\max$层输出预测概率。</p>
<script type="math/tex; mode=display">
soft\max(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}</script><p>输出概率向量表示为$p$，训练$ont-hot$标签向量为$L$，交叉熵定义为$H(L,p)$。</p>
<p>则分类的损失函数定义为</p>
<script type="math/tex; mode=display">
J = \frac{1}{N}\sum_{i=1}^N H(L_i,p_i)</script><p>因为$L_i$、$q_i$表示一个样本，所以计算交叉熵即计算$-L_ilog(q_i)$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">p = np.array([<span class="number">0.2</span>,<span class="number">0.5</span>,<span class="number">0.3</span>])</span><br><span class="line">target = np.array([<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">H = <span class="number">-1</span> * np.sum(target * np.log(p)) / <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>当分类为<strong>二分类时</strong>，最后一层通常采用$sigmod$函数（逻辑斯蒂”回归”）。</p>
<script type="math/tex; mode=display">
sigmod(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">
J = -\frac{1}{N}\sum_{i=1}^N [y_i\log(\hat y_i) +(1-y_i)\log(1-\hat y_i)]</script><script type="math/tex; mode=display">
\hat y(x) = \frac{1}{1 + e^{-W*x}}</script><p>即为交叉熵损失函数的一个特殊形式。</p>
<p>采用交叉熵损失函数的原因是该损失函数为凸函数，从而可以进行凸优化。</p>
</li>
<li><p>KL（Kullback–Leibler）散度（Divergence）和JS（Jensen–Shannon）散度</p>
<p>KL散度用来衡量两个概率分布$P$、$Q$的距离。</p>
<script type="math/tex; mode=display">
D_{KL}(P||Q)= \sum P\log(\frac{P}{Q})</script><p>注意：$D_{KL}(P||Q) \not= D_{KL}(Q||P)$。</p>
<p>JSD定义如下：</p>
<script type="math/tex; mode=display">
JSD(P||Q) = \frac{1}{2}D_{KL}(P||M) + \frac{1}{2}D_{KL}(Q||M)</script><script type="math/tex; mode=display">
M = \frac{1}{2}(P + Q)</script></li>
</ul>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Dropout理解与实现" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/06/30/Dropout%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/"
    >Dropout理解与实现</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/06/30/Dropout%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/" class="article-date">
  <time datetime="2020-06-30T12:34:33.033Z" itemprop="datePublished">2020-06-30</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="Dropout理解与实现"><a href="#Dropout理解与实现" class="headerlink" title="Dropout理解与实现"></a>Dropout理解与实现</h2><ul>
<li><p>作用：</p>
<p>正则化的一种手段，训练过程中避免神经网络的过拟合。</p>
<p>在某一层中，随机使一部分神经元失活（输出为0），<strong>导致这部分神经元对下一层输入的贡献为0</strong>。</p>
<p>数学表达如下：</p>
<p>第$l + 1$层的输入：</p>
<script type="math/tex; mode=display">
z_i^{(l+1)} = w_i^{(l+1)}y^l + b_i^{(l+1)}</script><script type="math/tex; mode=display">
y_i^{(l+1)} = f(z_i^{(l+1)})</script><p>$f$为激活函数。</p>
<p>若在该层使用dropout，则：</p>
<script type="math/tex; mode=display">
r^{(l+1)} = Bernouli(p)</script><script type="math/tex; mode=display">
\hat y^{(l+1)} = r^{(l+1)} * y_i^{(l+1)}</script><p>$r^{(l+1)}$是一个mask向量，只包含0、1，其中为0表示该神经元失活了。$\hat y^{(l+1)}$即作为下一层的输入。</p>
<p><img src="/images/dropout.png" alt=""></p>
<p>即等价于：</p>
<p><img src="https://pic2.zhimg.com/v2-64930dc0337f42bcdbec488fd5337e95_r.jpg" alt=""></p>
</li>
<li><p>实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">	x: 输入</span></span><br><span class="line"><span class="string">	keep_prob: 留存概率</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(x, keep_prob)</span>:</span></span><br><span class="line">    mask = np.random.binomial(<span class="number">1</span>, keep_prob, size=x.shape)</span><br><span class="line">    x *= mask</span><br><span class="line">    x = x / keep_prob <span class="comment"># 对余下的神经元进行rescale</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, W1, W2, W3, training=False)</span>:</span></span><br><span class="line">    z1 = np.dot(x, W1)</span><br><span class="line">    y1 = np.tanh(z1)</span><br><span class="line">    z2 = np.dot(y1, W2)</span><br><span class="line">    y2 = np.tanh(z2)</span><br><span class="line">    <span class="comment"># Dropout in layer 2 </span></span><br><span class="line">    <span class="keyword">if</span> training:</span><br><span class="line">        m2 = np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>, size=z2.shape) <span class="comment"># 生成mask</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        m2 = <span class="number">0.5</span> <span class="comment"># 训练中没有rescale，测试时需要平衡训练中失活的神经元数量</span></span><br><span class="line">    y2 *= m2 <span class="comment"># 乘以mask，为0即代表失活</span></span><br><span class="line">    z3 = np.dot(y2, W3)</span><br><span class="line">    y3 = z3 <span class="comment"># linear output</span></span><br><span class="line">    <span class="keyword">return</span> y1, y2, y3, m2</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, W1, W2, W3, training=False)</span>:</span></span><br><span class="line">    z1 = np.dot(x, W1)</span><br><span class="line">    y1 = np.tanh(z1)</span><br><span class="line">    z2 = np.dot(y1, W2)</span><br><span class="line">    y2 = np.tanh(z2)</span><br><span class="line">    <span class="comment"># Dropout in layer 2</span></span><br><span class="line">    <span class="keyword">if</span> training:</span><br><span class="line">   		y2 = dropout(y2,<span class="number">0.5</span>) <span class="comment"># 训练阶段已经进行了rescale</span></span><br><span class="line">    z3 = np.dot(y2, W3)</span><br><span class="line">    y3 = z3 <span class="comment"># linear output</span></span><br><span class="line">    <span class="keyword">return</span> y1, y2, y3, m2</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意：</p>
<p>测试中不需要dropout。dropout一般用于全连接层之前，对卷积层的效果一般。</p>
</li>
</ul>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Flink流式处理框架学习" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/06/29/Flink%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/"
    >Flink流式处理框架学习</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/06/29/Flink%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2020-06-29T14:46:15.156Z" itemprop="datePublished">2020-06-29</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="Flink流式处理框架学习"><a href="#Flink流式处理框架学习" class="headerlink" title="Flink流式处理框架学习"></a>Flink流式处理框架学习</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><h3 id="2-安装与部署"><a href="#2-安装与部署" class="headerlink" title="2. 安装与部署"></a>2. 安装与部署</h3><h3 id="3-Source与Sink"><a href="#3-Source与Sink" class="headerlink" title="3. Source与Sink"></a>3. Source与Sink</h3><h3 id="4-DataStream编程模型API"><a href="#4-DataStream编程模型API" class="headerlink" title="4. DataStream编程模型API"></a>4. DataStream编程模型API</h3><h3 id="5-Checkpoint与State状态管理"><a href="#5-Checkpoint与State状态管理" class="headerlink" title="5. Checkpoint与State状态管理"></a>5. Checkpoint与State状态管理</h3><h3 id="6-Flink中的Windows"><a href="#6-Flink中的Windows" class="headerlink" title="6. Flink中的Windows"></a>6. Flink中的Windows</h3><h3 id="7-Flink中的Time"><a href="#7-Flink中的Time" class="headerlink" title="7.  Flink中的Time"></a>7.  Flink中的Time</h3>
      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Kafka入门教程" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/06/25/Kafka%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"
    >Kafka入门教程</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/06/25/Kafka%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/" class="article-date">
  <time datetime="2020-06-25T14:56:20.418Z" itemprop="datePublished">2020-06-25</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="Kafka入门教程"><a href="#Kafka入门教程" class="headerlink" title="Kafka入门教程"></a>Kafka入门教程</h2><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><h4 id="1-1-消息队列"><a href="#1-1-消息队列" class="headerlink" title="1.1 消息队列"></a>1.1 消息队列</h4><p>​        总的来说，消息队列可以分为点对点模式、发布订阅模式</p>
<p>​        （1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）</p>
<p>​        点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息， 而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者 接收处理，即使有多个消息监听者也是如此。</p>
<p>​        （2）发布订阅模式（一对多，数据生产后，推送给所有订阅者）</p>
<p>​        发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅 者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即 使当前订阅者不可用，处于离线状态。</p>
<h4 id="1-2-什么是Kafka"><a href="#1-2-什么是Kafka" class="headerlink" title="1.2 什么是Kafka"></a>1.2 什么是Kafka</h4><p>​        在流式计算中，Kafka 一般用来缓存数据，作为大数据处理系统中的一个中间件。</p>
<p>​        （1）Apache Kafka 是一个开源消息系统，由 Scala 写成。是由 Apache 软件基金会开发的 一个开源消息系统项目。</p>
<p>​        （2）Kafka 最初是由 LinkedIn 公司开发，并于 2011 年初开源。2012 年 10 月从 Apache Incubator 毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。</p>
<p>​        （3））Kafka 是一个分布式消息队列。Kafka 对消息保存时根据 Topic 进行归类，发送消息 者称为 Producer，消息接受者称为 Consumer，此外 Kafka 集群有多个 Kafka 实例组成，每个实例(server)称为 broker。</p>
<p>​        （4）无论是 Kafka 集群，还是 consumer 都依赖于 zookeeper 集群保存一些 meta 信息， 来保证系统可用性。</p>
<h4 id="1-3-Kafka架构"><a href="#1-3-Kafka架构" class="headerlink" title="1.3 Kafka架构"></a>1.3 Kafka架构</h4><p>​        （1）Producer ：消息生产者，就是向 Kafka broker 发消息的客户端；</p>
<p>​        （2）Consumer ：消息消费者，向 Kafka broker 取消息的客户端； </p>
<p>​        （3）Topic ：可以理解为一个队列；</p>
<p>​        （4）Consumer Group：这是Kafka用来实现一个topic消息的广播（发给所有的consumer） 和单播（发给任意一个 consumer）的手段。一个 topic 可以有多个 CG。topic 的消息会复制 （不是真的复制，是概念上的）到所有的 CG，但每个 partition 只会把消息发给该 CG 中的一 个 consumer。如果需要实现广播，只要每个 consumer 有一个独立的 CG 就可以了。要实现 单播只要所有的 consumer 在同一个 CG。用 CG 还可以将 consumer 进行自由的分组而不需 要多次发送消息到不同的 topic；</p>
<p>​        （5）Broker ：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker 可以容纳多个 topic；</p>
<p>​        （6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。partition 中的每条消息 都会被分配一个有序的 id（offset）。Kafka 只保证按一个 partition 中的顺序将消息发给 consumer，不保证一个 topic 的整体（多个 partition 间）的顺序； </p>
<p>​        （7）Offset：Kafka 的存储文件都是按照 offset.kafka 来命名，用 offset 做名字的好处是方便查 找。例如你想找位于 2049 的位置，只要找到 2048.kafka 的文件即可。当然 the first offset 就 是 00000000000.kafka。</p>
<p>​    <img src="C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200702161839173.png" alt="image-20200702161839173"></p>
<h3 id="2-Kafka集群部署"><a href="#2-Kafka集群部署" class="headerlink" title="2. Kafka集群部署"></a>2. Kafka集群部署</h3><p>​        Kafka集群依赖ZooKeeper，因此，启动Kafka集群前，应启动zookeeper集群。否则会报错。</p>
<h4 id="2-1-集群部署"><a href="#2-1-集群部署" class="headerlink" title="2.1 集群部署"></a>2.1 集群部署</h4><p>​        （1）修改配置文件</p>
<p>​        下载好对应的tar包并解压后，找到<code>config</code>目录下的<code>server.propertites</code>文件，修改<code>log.dirs</code>配置项，即配置Kafka运行日志的存放路径。</p>
<p>​        （2）启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties &amp;</span><br></pre></td></tr></table></figure>
<p><code>&amp;</code>指后台启动。        </p>
<p>​        （3）关闭</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure>
<p>​        （4）发生错误日志检查</p>
<p>​        当启动或运行时发生故障后，可以检查<code>logs/server.log</code>文件进行故障排查。</p>
<h4 id="2-2-Kafka命令行操作"><a href="#2-2-Kafka命令行操作" class="headerlink" title="2.2 Kafka命令行操作"></a>2.2 Kafka命令行操作</h4><p>​        （1）查看当前服务器中所有topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --list</span><br></pre></td></tr></table></figure>
<p>​        （2）创建topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 \</span><br><span class="line">--create --replication-factor 3 --partitions 1 --topic first</span><br></pre></td></tr></table></figure>
<p>选项说明：</p>
<p>​        —topic 定义 topic 名 </p>
<p>​        —replication-factor 定义副本数</p>
<p>​        —partitions 定义分区数</p>
<p>​        （3）删除topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic first</span><br></pre></td></tr></table></figure>
<p>注意：需要<code>server.properties</code>中设置<code>delete.topic.enable=true</code>否则只是标记删除或者直接重启。</p>
<p>​        （4）发送（生产）消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> bin/kafka-console-producer.sh \</span><br><span class="line">--broker-list localhost:9092 --topic first</span><br></pre></td></tr></table></figure>
<p>​        （5）消费消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> bin/kafka-console-consumer.sh \</span><br><span class="line">--bootstrap-server localhost:2181 --from-beginning --topic first</span><br></pre></td></tr></table></figure>
<p>​        （6）查看某个topic的详情</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic first</span><br></pre></td></tr></table></figure>
<h3 id="3-Kafka工作流程分析"><a href="#3-Kafka工作流程分析" class="headerlink" title="3. Kafka工作流程分析"></a>3. Kafka工作流程分析</h3><p>​        </p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-SpringMVC源码浅析" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/06/24/SpringMVC%E6%BA%90%E7%A0%81%E6%B5%85%E6%9E%90/"
    >SpringMVC源码浅析</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/06/24/SpringMVC%E6%BA%90%E7%A0%81%E6%B5%85%E6%9E%90/" class="article-date">
  <time datetime="2020-06-24T01:47:29.672Z" itemprop="datePublished">2020-06-24</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Java/">Java</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="SpringMVC源码浅析"><a href="#SpringMVC源码浅析" class="headerlink" title="SpringMVC源码浅析"></a>SpringMVC源码浅析</h2>
      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-生成对抗网络" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/06/22/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/"
    >生成对抗网络</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/06/22/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/" class="article-date">
  <time datetime="2020-06-22T05:22:08.407Z" itemprop="datePublished">2020-06-22</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="生成对抗网络（GAN）"><a href="#生成对抗网络（GAN）" class="headerlink" title="生成对抗网络（GAN）"></a>生成对抗网络（GAN）</h2><h3 id="1-入门简介"><a href="#1-入门简介" class="headerlink" title="1. 入门简介"></a>1. 入门简介</h3><p>​        gan整体的损失函数</p>
<script type="math/tex; mode=display">
\min_{G}\max_{D} V(G,D) = E_{x-P_{data}}\log D(x) + E_{z-P_z}\log (1-D(G(z)))</script><p>​        训练时，先训练Discriminator、然后训练Generator，迭代直至目标函数收敛。</p>
<p>​        需要注意的是，一切损失计算都是在D（判别器）输出处产生的，而D的输出一般是fake/true的判断，所以整体上采用的是二分类交叉熵函数。</p>
<p>​        首先看一下maxD部分，因为训练一般是先保持G（生成器）不变训练D的。D的训练目标是正确区分fake/true，如果我们以1/0代表true/fake，则对第一项E因为输入采样自真实数据所以我们期望D(x)趋近于1，也就是第一项更大。同理第二项E输入采样自G生成数据，所以我们期望D(G(z))趋近于0更好，也就是说第二项又是更大。所以是这一部分是期望训练使得整体更大了，也就是<code>maxD</code>的含义了。</p>
<p>　　第二部分<strong>保持D不变，训练G</strong>，这个时候只有第二项E有用了，<strong>关键来了，因为我们要迷惑D，所以这时将label设置为1(我们知道是fake，所以才叫迷惑)，希望D(G(z))输出接近于1，也就是这一项越小越好，这就是minG。当然判别器D哪有这么好糊弄，所以这个时候判别器就会产生比较大的误差，误差会更新G，那么G就会变得更好了，这次没有骗过你，只能下次更努力了</strong>。</p>
<p>​        Discriminator的损失函数</p>
<script type="math/tex; mode=display">
\max_D \log [D(x)] + \log [1 - D(G(z))]</script><p>​        Generator的损失函数</p>
<script type="math/tex; mode=display">
\min_G \log[1-D(G(z))]</script><p>​        <strong>在（近似）最优判别器下，最小化生成器的loss等价于最小化$P_r$与$P_g$之间的JS散度</strong>。</p>
<p>​        下图中可以发现，所有的loss都是由判别器产生的。如果没有D，G不知道自己生成的结果如何，便得不到权重更新。</p>
<p>​    <img src="E:\hexo\themes\ayer\source\images\gan-train.png" alt="image-20200625214641466"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"><span class="comment">#数据集的加载</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Lambda(<span class="keyword">lambda</span> x: x.repeat(<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">    transforms.Normalize(mean=(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), std=(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">'./MNIST_data/'</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">False</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">'./MNIST_data/'</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_dims, output_dims)</span>:</span></span><br><span class="line">        super(Generator,self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dims,<span class="number">256</span>)</span><br><span class="line">        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*<span class="number">2</span>)</span><br><span class="line">        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*<span class="number">2</span>)</span><br><span class="line">        self.fc4 = nn.Linear(self.fc3.out_features, output_dims)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.leaky_relu(self.fc1(x),<span class="number">0.2</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc2(x),<span class="number">0.3</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc3(x),<span class="number">0.4</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.tanh(self.fc4(x))</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_dim)</span>:</span></span><br><span class="line">        super(Discriminator,self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_dim, <span class="number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//<span class="number">2</span>)</span><br><span class="line">        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//<span class="number">2</span>)</span><br><span class="line">        self.fc4 = nn.Linear(self.fc3.out_features, <span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.leaky_relu(self.fc1(x), <span class="number">0.2</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.3</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc2(x), <span class="number">0.2</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.3</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc3(x), <span class="number">0.2</span>)</span><br><span class="line">        x = F.dropout(x, <span class="number">0.3</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(self.fc4(x))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 二分类交叉熵损失函数</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.0002</span></span><br><span class="line">D_optimizer = optim.Adam(D.parameters(), lr = lr)</span><br><span class="line">G_optimizer = optim.Adam(G.parameters(), lr = lr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">G_train</span><span class="params">(x)</span>:</span></span><br><span class="line">    G.zero_grad()</span><br><span class="line">    z = Variable(torch.randn(batch_size,z_dim).to(device))</span><br><span class="line">    <span class="comment"># label全为1</span></span><br><span class="line">    y = Variable(torch.ones(batch_size,<span class="number">1</span>).to(device))</span><br><span class="line">    </span><br><span class="line">    G_output = G(z)</span><br><span class="line">    D_output = D(G_output)</span><br><span class="line">    </span><br><span class="line">    G_loss = criterion(D_output, y)</span><br><span class="line">    G_loss.backward()</span><br><span class="line">    </span><br><span class="line">    G_optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> G_loss.data.item()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">D_train</span><span class="params">(x)</span>:</span></span><br><span class="line">    D.zero_grad()</span><br><span class="line">    <span class="comment"># x原来的shape [batch_size,3,28,28]</span></span><br><span class="line">    <span class="comment"># 3个通道都是一样的，取一个通道就行</span></span><br><span class="line">    x = x[:,<span class="number">0</span>,:,:]</span><br><span class="line">    x_real, y_real = x.view(<span class="number">-1</span>, mnist_dim), torch.ones(batch_size, <span class="number">1</span>)</span><br><span class="line">    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))</span><br><span class="line">    </span><br><span class="line">    D_output = D(x_real)</span><br><span class="line">    D_real_loss = criterion(D_output, y_real)</span><br><span class="line">    <span class="comment">#D_real_score = D_output</span></span><br><span class="line">    </span><br><span class="line">    z = Variable(torch.randn(batch_size, z_dim).to(device))</span><br><span class="line">    x_fake, y_fake = G(z), Variable(torch.zeros(batch_size, <span class="number">1</span>).to(device))</span><br><span class="line"></span><br><span class="line">    D_output = D(x_fake)</span><br><span class="line">    D_fake_loss = criterion(D_output, y_fake)</span><br><span class="line">    <span class="comment">#D_fake_score = D_output</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># gradient backprop &amp; optimize ONLY D's parameters</span></span><br><span class="line">    D_loss = D_real_loss + D_fake_loss</span><br><span class="line">    D_loss.backward()</span><br><span class="line">    D_optimizer.step()</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span>  D_loss.data.item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">n_epoch = <span class="number">200</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epoch):</span><br><span class="line">    D_losses, G_losses = [], []</span><br><span class="line">    <span class="keyword">for</span> index,(x,_) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        D_losses.append(D_train(x))</span><br><span class="line">        G_losses.append(G_train(x))</span><br><span class="line">    print(<span class="string">'[%d/%d]: loss_d: %.3f, loss_g: %.3f'</span> % (</span><br><span class="line">            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用训练好的GAN生成图片</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    test_z = Variable(torch.randn(batch_size, z_dim).to(device))</span><br><span class="line">    generated = G(test_z)</span><br><span class="line">    save_image(generated.view(generated.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), <span class="string">'./samples/sample_'</span> + <span class="string">'.png'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-各式各样的GAN"><a href="#2-各式各样的GAN" class="headerlink" title="2. 各式各样的GAN"></a>2. 各式各样的GAN</h3><h4 id="2-1DCGAN"><a href="#2-1DCGAN" class="headerlink" title="2.1DCGAN"></a>2.1DCGAN</h4><p>​        深度卷积生成对抗网络，在生成器中，对输入的一维向量不断进行转置卷积（上采样）最终生成对应的图像。在判别器中，则将输入的图像经过多层卷积最后经过sigmod函数进行二分类，判断这是原始数据图片还是生成器产生的图片。</p>
<p><img src="/images/dcgan.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span><span class="params">(m)</span>:</span></span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">'Conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        m.weight.data.normal_(<span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">'BatchNorm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        m.weight.data.normal_(<span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        m.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    input (N, in_dim)</span></span><br><span class="line"><span class="string">    output (N, 3, 64, 64)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim, dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dconv_bn_relu</span><span class="params">(in_dim, out_dim)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.ConvTranspose2d(in_dim, out_dim, <span class="number">5</span>, <span class="number">2</span>,</span><br><span class="line">                                   padding=<span class="number">2</span>, output_padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(out_dim),</span><br><span class="line">                nn.ReLU())</span><br><span class="line">        self.l1 = nn.Sequential(</span><br><span class="line">            nn.Linear(in_dim, dim * <span class="number">8</span> * <span class="number">4</span> * <span class="number">4</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm1d(dim * <span class="number">8</span> * <span class="number">4</span> * <span class="number">4</span>),</span><br><span class="line">            nn.ReLU())</span><br><span class="line">        self.l2_5 = nn.Sequential(</span><br><span class="line">            dconv_bn_relu(dim * <span class="number">8</span>, dim * <span class="number">4</span>),</span><br><span class="line">            dconv_bn_relu(dim * <span class="number">4</span>, dim * <span class="number">2</span>),</span><br><span class="line">            dconv_bn_relu(dim * <span class="number">2</span>, dim),</span><br><span class="line">            nn.ConvTranspose2d(dim, <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>, padding=<span class="number">2</span>, output_padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh())</span><br><span class="line">        self.apply(weights_init)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = self.l1(x)</span><br><span class="line">        y = y.view(y.size(<span class="number">0</span>), <span class="number">-1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">        y = self.l2_5(y)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    input (N, 3, 64, 64)</span></span><br><span class="line"><span class="string">    output (N, )</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim, dim=<span class="number">64</span>)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">conv_bn_lrelu</span><span class="params">(in_dim, out_dim)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_dim, out_dim, <span class="number">5</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                nn.BatchNorm2d(out_dim),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>))</span><br><span class="line">        self.ls = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_dim, dim, <span class="number">5</span>, <span class="number">2</span>, <span class="number">2</span>), nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            conv_bn_lrelu(dim, dim * <span class="number">2</span>),</span><br><span class="line">            conv_bn_lrelu(dim * <span class="number">2</span>, dim * <span class="number">4</span>),</span><br><span class="line">            conv_bn_lrelu(dim * <span class="number">4</span>, dim * <span class="number">8</span>),</span><br><span class="line">            nn.Conv2d(dim * <span class="number">8</span>, <span class="number">1</span>, <span class="number">4</span>),</span><br><span class="line">            nn.Sigmoid())</span><br><span class="line">        self.apply(weights_init)        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = self.ls(x)</span><br><span class="line">        y = y.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h4 id="2-2-Conditional-GAN"><a href="#2-2-Conditional-GAN" class="headerlink" title="2.2 Conditional GAN"></a>2.2 Conditional GAN</h4><p>CGAN的目标函数与原始的并无太大不同，只不过加了一个限定条件。</p>
<script type="math/tex; mode=display">
\min_G \max_D V(D,G) = E_{x-p_{data}}[\log(D(x|y))] + E_{z-p_z}[\log[1 - D(G(z|y))]]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># G(z)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># initializers</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(generator, self).__init__()</span><br><span class="line">        self.fc1_1 = nn.Linear(<span class="number">100</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc1_1_bn = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        <span class="comment"># 处理label one-hot向量的</span></span><br><span class="line">        self.fc1_2 = nn.Linear(<span class="number">10</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc1_2_bn = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        </span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc2_bn = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">512</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc3_bn = nn.BatchNorm1d(<span class="number">1024</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">1024</span>, <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># weight_init</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_init</span><span class="params">(self, mean, std)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self._modules:</span><br><span class="line">            normal_init(self._modules[m], mean, std)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, label)</span>:</span></span><br><span class="line">        x = F.relu(self.fc1_1_bn(self.fc1_1(input)))</span><br><span class="line">        y = F.relu(self.fc1_2_bn(self.fc1_2(label)))</span><br><span class="line">        <span class="comment"># 把两个向量进行合并</span></span><br><span class="line">        x = torch.cat([x, y], <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.fc2_bn(self.fc2(x)))</span><br><span class="line">        x = F.relu(self.fc3_bn(self.fc3(x)))</span><br><span class="line">        x = F.tanh(self.fc4(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># initializers</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(discriminator, self).__init__()</span><br><span class="line">        self.fc1_1 = nn.Linear(<span class="number">784</span>, <span class="number">1024</span>)</span><br><span class="line">        <span class="comment"># 处理label one-hot向量 batch_size * 10</span></span><br><span class="line">        self.fc1_2 = nn.Linear(<span class="number">10</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">2048</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc2_bn = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc3_bn = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># weight_init</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_init</span><span class="params">(self, mean, std)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self._modules:</span><br><span class="line">            normal_init(self._modules[m], mean, std)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, label)</span>:</span></span><br><span class="line">        x = F.leaky_relu(self.fc1_1(input), <span class="number">0.2</span>)</span><br><span class="line">        y = F.leaky_relu(self.fc1_2(label), <span class="number">0.2</span>)</span><br><span class="line">        </span><br><span class="line">        x = torch.cat([x, y], <span class="number">1</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc2_bn(self.fc2(x)), <span class="number">0.2</span>)</span><br><span class="line">        x = F.leaky_relu(self.fc3_bn(self.fc3(x)), <span class="number">0.2</span>)</span><br><span class="line">        x = F.sigmoid(self.fc4(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_init</span><span class="params">(m, mean, std)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(m, nn.Linear):</span><br><span class="line">        m.weight.data.normal_(mean, std)</span><br><span class="line">        m.bias.data.zero_()</span><br></pre></td></tr></table></figure>
<p>结合介绍的两种，可以定义<code>cDCNGAN</code>模型（就是把Linear全连接层换为了ConvTranspose2d或Conv2d卷积层）。</p>
<h4 id="2-3-Bidirectional-GAN"><a href="#2-3-Bidirectional-GAN" class="headerlink" title="2.3 Bidirectional GAN"></a>2.3 Bidirectional GAN</h4><p>讲述$BiGAN$的两篇论文分别为：</p>
<p>Donahue, Jeff, Philipp Krähenbühl, and Trevor Darrell. “Adversarial feature learning.” <em>arXiv preprint arXiv:1605.09782</em> (2016).</p>
<p>Dumoulin, Vincent, et al. “Adversarially learned inference.” <em>arXiv preprint arXiv:1606.00704</em> (2016).</p>
<ul>
<li><p>网络架构</p>
<p><img src="/images/bigan.png" alt="image-20200625104051977"></p>
</li>
</ul>
<ul>
<li>目标函数<script type="math/tex; mode=display">
\min_{G,E}\max_D V(D,E,G)</script><img src="/images/image-20200625104357166.png" alt="image-20200625104357166"></li>
</ul>
<p>代码参考：<a href="https://github.com/fmu2/Wasserstein-BiGAN" target="_blank" rel="noopener">https://github.com/fmu2/Wasserstein-BiGAN</a></p>
<h4 id="2-4-WGAN"><a href="#2-4-WGAN" class="headerlink" title="2.4 WGAN"></a>2.4 WGAN</h4><p>Arjovsky, M., Chintala, S., &amp; Bottou, L. (2017). Wasserstein gan. <em>arXiv preprint arXiv:1701.07875</em>.（gradient clipping）</p>
<p>Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., &amp; Courville, A. C. (2017). Improved training of wasserstein gans. In <em>Advances in neural information processing systems</em> (pp. 5767-5777).（gradient penalty）</p>
<p>​        参考：<a href="https://zhuanlan.zhihu.com/p/25071913（令人拍案叫绝的WGAN）。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25071913（令人拍案叫绝的WGAN）。</a></p>
<p>​        $Wasserstein$距离也被称为$Earth  mover’s$距离（推土机距离）。<strong>Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近。</strong></p>
<p>​        <strong>我们可以构造一个含参数$w$、最后一层不是非线性激活层的判别器网络$f_w$，在限制$w$不超过某个范围的条件下，使得</strong></p>
<script type="math/tex; mode=display">
L = E_{x-P_r}[f_w(x)] - E_{x-P_G}[f_w(x)]</script><p><strong>尽可能取到最大，此时$L$就会近似真实分布与生成分布之间的Wasserstein距离（忽略常数倍数$K$）。</strong></p>
<p><img src="https://pic1.zhimg.com/v2-6be6e2ef3d15c4b10c2a943e9bf4db70_r.jpg" alt=""></p>
<p><img src="/images/wgan-code.png" alt="image-20200622215347951"></p>
<p>注：判别器要迭代训练多次。而生成器只训练一次。</p>
<p><img src="/images/wgan-g.png" alt="image-20200622220223959"></p>
<p>WGAN在原生的GAN做出的改进：</p>
<ol>
<li>G和D的损失函数不用对数</li>
<li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li>
<li>D最后一层去掉$sigmod$二分类函数</li>
<li>采用gradient clipping和gradient penalty（改进）</li>
</ol>
<p>原始GAN存在的问题：</p>
<ul>
<li>判别器越好，生成器越容易产生梯度消失。</li>
<li>训练不稳定，容易导致$collapse mode$。</li>
</ul>
<h4 id="2-5-StackGAN由文本生成高分辨率图像"><a href="#2-5-StackGAN由文本生成高分辨率图像" class="headerlink" title="2.5 StackGAN由文本生成高分辨率图像"></a>2.5 StackGAN由文本生成高分辨率图像</h4><p>Zhang, Han, et al. “Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks.” <em>Proceedings of the IEEE international conference on computer vision</em>. 2017.</p>
<h4 id="2-6-GANomaly异常检测"><a href="#2-6-GANomaly异常检测" class="headerlink" title="2.6 GANomaly异常检测"></a>2.6 GANomaly异常检测</h4><ul>
<li>网络架构：</li>
</ul>
<p><img src="/images/ganomaly.png" alt="image-20200623103433011"></p>
<p>​        可以看出，模型包含两个encoder、一个decoder（相当于生成器）和一个判别器。模型划分为三个部分：第一部分为一个自动编码器，包含一个encoder（$G_E$）、一个decoder（$G_D$），这一部分被记为$G$；第二部分为一个encoder，记为$E$；第三部分为一个判别器网络，记为$D$。前两部分也被称为G-Net。</p>
<p>​        输入图片数据$x$经过一个encoder（$G_E$）编码为向量$z$，decoder（$G_D$）将向量$z$还原为原尺寸图像数据$\hat x$，另一个encoder（$E$）将$\hat x$又编码为向量$\hat z$。将$x$和$\hat x$输入判别器网络（$D$）判断图片是原始图片还是生成器生成的图片。</p>
<ul>
<li>损失函数</li>
</ul>
<p>​        损失函数共分为三部分，第一部分是$Enocder Loss$，衡量两个encoder编码向量的损失；第二部分是$Contextual Loss$，衡量原图像与生成器生成图像的损失，第三部分是$Adversial  Loss$，是常规的GAN中判别网络的损失，这里采用的是二分类的交叉熵损失。</p>
<p>​        优化D-net，采用$Adversial  Loss$。</p>
<p>​        优化G-net时，采用三部分损失函数的加权和。</p>
<ul>
<li>异常检测</li>
</ul>
<p>​        原理：由于训练输入的都是正常数据，第一个encoder学习到的是正常数据的分布，经过生成器的重建后再经过encoder编码差异不会很大，当输入异常数据时，encoder编码后会损失部分信息，经过生成器重建后再编码会与原来的数据差异很大，从而进行异常检测。</p>
<script type="math/tex; mode=display">
A(\hat x) = ||G_E(\hat x) - E(G(\hat x))||_1</script><p>​        当异常得分$A$大于某一阈值时，模型就会判定该数据为异常数据。（异常检测并没有用到判别器）。</p>
<h4 id="2-7-DiscoGAN关联分析"><a href="#2-7-DiscoGAN关联分析" class="headerlink" title="2.7 DiscoGAN关联分析"></a>2.7 DiscoGAN关联分析</h4><p><img src="/images/discogan.png" alt="image-20200623120348746"></p>
<p>​        模型主要由两个生成器和两个判别器构成。</p>
<ul>
<li><p>$G_{AB}$：输入A领域（domain）图片，生成B领域图片</p>
</li>
<li><p>$G_{BA}$：输入B领域图片，生成A领域图片</p>
</li>
<li><p>$D_A$：判别A领域原始图像和$G_{BA}$生成的A领域图像</p>
</li>
<li><p>$D_B$：判别B领域原始图像和$G_{AB}$生成的B领域图像</p>
</li>
</ul>
<p><img src="/images/disco-gan-loss.png" alt="image-20200623231830797"></p>
<p><img src="/images/disco-gan-loss2.png" alt="image-20200623231914928"></p>
<p><img src="/images/disco-gan-loss3.png" alt="image-20200623231950959"></p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
  </article>
  

  
  <nav class="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2019-2020
        张永剑
      </li>
      <li>
        
          Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
      <aside class="sidebar">
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="张永剑的博客"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>







<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>



<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>