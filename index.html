<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="description" content="天空如此辽阔，大地不过是必经之路" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     张永剑的博客
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<link rel="alternate" href="/atom.xml" title="张永剑的博客" type="application/atom+xml">
</head>

</html>

<body>
  <div id="app">
    <main class="content">
      
<div id="main">
  <section class="outer">
  <article class="articles">
    
    
    
    
    <article id="post-动态规划" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/25/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"
    >动态规划</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/10/25/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" class="article-date">
  <time datetime="2020-10-25T05:41:25.443Z" itemprop="datePublished">2020-10-25</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Leetcode/">Leetcode</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><h3 id="1-最大子段和"><a href="#1-最大子段和" class="headerlink" title="1. 最大子段和"></a>1. 最大子段和</h3><h3 id="2-最长上升子序列"><a href="#2-最长上升子序列" class="headerlink" title="2. 最长上升子序列"></a>2. 最长上升子序列</h3><h3 id="3-最长公共子序列"><a href="#3-最长公共子序列" class="headerlink" title="3. 最长公共子序列"></a>3. 最长公共子序列</h3><h3 id="4-最长公共子串"><a href="#4-最长公共子串" class="headerlink" title="4. 最长公共子串"></a>4. 最长公共子串</h3><h3 id="5-背包问题"><a href="#5-背包问题" class="headerlink" title="5. 背包问题"></a>5. 背包问题</h3><h3 id="6-编辑距离"><a href="#6-编辑距离" class="headerlink" title="6. 编辑距离"></a>6. 编辑距离</h3><h3 id="7-其他"><a href="#7-其他" class="headerlink" title="7. 其他"></a>7. 其他</h3><h4 id="7-1-矩阵相乘的最少次数"><a href="#7-1-矩阵相乘的最少次数" class="headerlink" title="7.1 矩阵相乘的最少次数"></a>7.1 矩阵相乘的最少次数</h4><h4 id="7-2-零钱兑换"><a href="#7-2-零钱兑换" class="headerlink" title="7.2 零钱兑换"></a>7.2 零钱兑换</h4>
      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-机器视觉基础Lecture1" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/21/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80Lecture1/"
    >机器视觉基础Lecture1</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/10/21/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80Lecture1/" class="article-date">
  <time datetime="2020-10-21T12:34:34.100Z" itemprop="datePublished">2020-10-21</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80/">机器视觉基础</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="机器视觉基础-Lecture-1"><a href="#机器视觉基础-Lecture-1" class="headerlink" title="机器视觉基础 Lecture 1"></a>机器视觉基础 Lecture 1</h2><h3 id="1-卷积"><a href="#1-卷积" class="headerlink" title="1. 卷积"></a>1. 卷积</h3><p>卷积操作即用一个窗口在图像上滑动，图像上的像素与窗口内的数值相乘再相加，最后得到新的图像的过程；这个窗口也被称为滤波器/核。</p>
<p><strong>卷积运算的性质</strong>：</p>
<ul>
<li>交换律：$a <em> b = b </em> a$</li>
<li>结合律：$a <em> (b </em> c) = (a <em> b) </em> c$</li>
<li>分配律：$a<em>(b + c) = a</em>b + a*c$</li>
<li>数乘：$ka<em>b = a </em> kb = k(a*b)$</li>
</ul>
<p>可以看出，卷积运算是一种<strong>线性</strong>运算。</p>
<p><strong>Question</strong>：</p>
<ul>
<li><p>设计一个窗口，卷积过后图像保持不变</p>
<p><img src="/images/k1.jpg" alt="image-20201019182152123"></p>
</li>
</ul>
<ul>
<li><p>设计一个窗口，卷积过后图像向左平移1像素</p>
<p><img src="/images/k2.png" alt="image-20201019182236198"></p>
</li>
</ul>
<p>  思考：为什么？对于每一个中心点像素，卷积过后更新为右边像素点像素值，整幅图卷积完就会向左平移。</p>
<ul>
<li><p>设计一个窗口，卷积过后达到锐化（sharpen）的效果</p>
<p><img src="/images/k3.png" alt="image-20201019182428504"></p>
</li>
</ul>
<p>  思考：为什么要这么设计？</p>
<p>  步骤1：</p>
<p>  <img src="/images/f1.png" alt="image-20201019182727419"></p>
<p>  步骤2：</p>
<p>  <img src="/images/f2.png" alt="image-20201019182746490"></p>
<p>  综合步骤1、2可以看出，相当于将原来的像素值乘2再减去均值滤波平滑后的图像，就可以得到锐化后的图像。写成卷积核形式就是上述形式。</p>
<h3 id="2-各种滤波器"><a href="#2-各种滤波器" class="headerlink" title="2. 各种滤波器"></a>2. 各种滤波器</h3><h4 id="2-1-均值滤波"><a href="#2-1-均值滤波" class="headerlink" title="2.1 均值滤波"></a>2.1 均值滤波</h4><p>会出现“振零”现象。</p>
<h4 id="2-2-高斯滤波"><a href="#2-2-高斯滤波" class="headerlink" title="2.2 高斯滤波"></a>2.2 高斯滤波</h4><p>如何生成一个高斯滤波器？高斯函数（连续）：$G(x,y)=e^{-\frac{x^2+y^2}{\sigma^2}}$到卷积核（离散）。</p>
<p>matlab生成$\sigma=1$和$\sigma=3$的高斯核（归一化后的）：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.0030</span>    <span class="number">0.0133</span>    <span class="number">0.0219</span>    <span class="number">0.0133</span>    <span class="number">0.0030</span></span><br><span class="line"><span class="number">0.0133</span>    <span class="number">0.0596</span>    <span class="number">0.0983</span>    <span class="number">0.0596</span>    <span class="number">0.0133</span></span><br><span class="line"><span class="number">0.0219</span>    <span class="number">0.0983</span>    <span class="number">0.1621</span>    <span class="number">0.0983</span>    <span class="number">0.0219</span></span><br><span class="line"><span class="number">0.0133</span>    <span class="number">0.0596</span>    <span class="number">0.0983</span>    <span class="number">0.0596</span>    <span class="number">0.0133</span></span><br><span class="line"><span class="number">0.0030</span>    <span class="number">0.0133</span>    <span class="number">0.0219</span>    <span class="number">0.0133</span>    <span class="number">0.0030</span></span><br></pre></td></tr></table></figure>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.0318</span>    <span class="number">0.0375</span>    <span class="number">0.0397</span>    <span class="number">0.0375</span>    <span class="number">0.0318</span></span><br><span class="line"><span class="number">0.0375</span>    <span class="number">0.0443</span>    <span class="number">0.0469</span>    <span class="number">0.0443</span>    <span class="number">0.0375</span></span><br><span class="line"><span class="number">0.0397</span>    <span class="number">0.0469</span>    <span class="number">0.0495</span>    <span class="number">0.0469</span>    <span class="number">0.0397</span></span><br><span class="line"><span class="number">0.0375</span>    <span class="number">0.0443</span>    <span class="number">0.0469</span>    <span class="number">0.0443</span>    <span class="number">0.0375</span></span><br><span class="line"><span class="number">0.0318</span>    <span class="number">0.0375</span>    <span class="number">0.0397</span>    <span class="number">0.0375</span>    <span class="number">0.0318</span></span><br></pre></td></tr></table></figure>
<p>一般设计的高斯滤波默认均值为0，并且根据“3$\sigma$原则”，窗口大小可以根据方差$\sigma$来设定，因此高斯滤波通常只指定一个参数：方差$\sigma$。</p>
<p><strong>3$\sigma$原则</strong>：一维高斯函数我们可以发现，在以原点为中心，向左向右$3|\sigma|$距离内，函数积分可以达到$99.73\%$，也就是说$|x|&gt;3\sigma$的点几乎约等于0，对卷积没有什么贡献。因此设计卷积核的时候。</p>
<p><strong>窗口大小一定，方差$\sigma$对平滑效果的影响：$\sigma$越大，平滑效果越明显。</strong></p>
<p>高斯函数的特点是：$\sigma$越小越集中（函数图像越尖，中心点的权值越大），这样加权平均自身权重更大，当$\sigma$逐渐增大，自身权重变小，加权过后自身信息丧失更加严重。</p>
<p><strong>方差一定且在3$\sigma$原则下，窗口大小对平滑效果的影响：窗口越大，平滑效果越明显。</strong></p>
<p>下面两张图展示了$\sigma=3$的情况下，窗口值大小为$3、9$的滤波结果。</p>
<p><img src="/images/var3_kernelsize_3.jpg" style="zoom:80%;" /></p>
<p><img src="/images/var3_kernelsize_9.jpg" style="zoom:80%;" /></p>
<p>窗口加大需要考虑的像素值越多自身信息丧失的也就越多。要求限定$3\sigma$原则原因是，超过这个范围，无论窗口多大平滑效果几乎一致（最外围差不多都是0了，对加权求和没有贡献）。</p>
<p>高斯噪声：采样于均值为0，方差为$\sigma$的高斯函数的值加到源图像上。</p>
<p>二维高斯函数（均值都是0）：</p>
<script type="math/tex; mode=display">
G(x,y) = \frac{1}{ {2\pi\sigma^2}}e^{-\frac{x^2+y^2}{2\sigma^2}} = G(x)G(y)</script><script type="math/tex; mode=display">
G_x(x,y) = \frac{\partial G}{\partial x} =-\frac{1}{2\pi\sigma^2}\frac{x}{\sigma^2}e^{-\frac{x^2+y^2}{2\sigma^2}}</script><script type="math/tex; mode=display">
G_y(x,y)=\frac{\partial G}{\partial y} =-\frac{1}{2\pi\sigma^2}\frac{y}{\sigma^2}e^{-\frac{x^2+y^2}{2\sigma^2}}</script><script type="math/tex; mode=display">
G_{xx}(x,y) =-\frac{1}{2\pi\sigma^2}(\frac{1}{\sigma^2}-\frac{x^2}{\sigma^4})e^{-\frac{x^2+y^2}{2\sigma^2}}</script><script type="math/tex; mode=display">
G_{yy}(x,y) =-\frac{1}{2\pi\sigma^2}(\frac{1}{\sigma^2}-\frac{y^2}{\sigma^4})e^{-\frac{x^2+y^2}{2\sigma^2}}</script><p>Laplacian of Gaussian (LoG)定义如下：</p>
<script type="math/tex; mode=display">
LoG = \frac{\partial^2 G}{\partial x^2} + \frac{\partial^2 G}{\partial y^2} = G_{xx}(x,y) + G_{yy}(x,y) = \frac{1}{2\pi\sigma^2}\frac{x^2 + y^2-2\sigma^2}{\sigma^4}e^{-\frac{x^2+y^2}{2\sigma^2}}</script><p>一个近似于该函数的离散卷积核($\sigma=1.4$)：</p>
<p><img src="https://homepages.inf.ed.ac.uk/rbf/HIPR2/figs/logdisc.gif" alt=""></p>
<p><strong>一维一阶</strong>高斯导数如下图所示（省略了系数$1/\sqrt{2\pi\sigma^2}$）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">-6</span>,<span class="number">6</span>,<span class="number">1000</span>)</span><br><span class="line">sigma_1 = <span class="number">1.0</span></span><br><span class="line">g_prime = -x/sigma_1**<span class="number">2</span> * np.exp(-x**<span class="number">2</span> / <span class="number">2</span> / sigma_1 ** <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/dergaussian.png" alt=""></p>
<p>一维二阶高斯导数如下图所示：</p>
<p><img src="C:\Users\zhang\Desktop\下载 (2" alt="">.png)</p>
<p><strong>将高斯函数离散化就可以得到高斯卷积核、一阶高斯卷积核</strong>。</p>
<p>在图像上，用高斯滤波就是取一个窗口，窗口内以中心点为（0,0）点然后给窗口内的不同坐标的窗口带入高斯函数中取值归一化后，在图像上滑动做卷积。</p>
<p>用高斯一阶导去和信号做卷积，干了两件事：一是平滑信号，二是求导。相当于先用高斯滤波平滑再求导。 同理高斯二阶导 Laplacian of Gaussian，用高斯二阶导去和信号卷积。</p>
<p>高斯差分 DoG （Difference of Gaussian）相当于一个核（滤波器）。下图显示两个正态分布（均值都为1）的函数图像以及二者的差值图像，即DoG。参考：<a href="http://fourier.eng.hmc.edu/e161/lectures/gradient/node9.html" target="_blank" rel="noopener">http://fourier.eng.hmc.edu/e161/lectures/gradient/node9.html</a></p>
<p><img src="http://fourier.eng.hmc.edu/e161/lectures/gradient/img111.png" alt=""></p>
<p><img src="http://fourier.eng.hmc.edu/e161/lectures/gradient/img112.png" alt=""></p>
<p><img src="/images/dog.png" alt=""></p>
<ul>
<li>高斯核求和为1，用途：平滑</li>
<li>高斯一阶导核求和为0，用途：边缘提取</li>
</ul>
<h4 id="2-3-中值滤波"><a href="#2-3-中值滤波" class="headerlink" title="2.3 中值滤波"></a>2.3 中值滤波</h4><p>中值滤波的窗口内没有权值，它的作用是把窗口内图像上所有像素值排序后取中值作为窗口中心点的像素值。</p>
<p><strong>对椒盐噪声有效，是非线性滤波</strong>。</p>
<h4 id="2-4-总结"><a href="#2-4-总结" class="headerlink" title="2.4 总结"></a>2.4 总结</h4><p>高斯滤波以及均值滤波都可以起到平滑图像的作用，高斯滤波可以视为有权的均值滤波，图像在平滑的时候不仅会把噪声，也会把图像边缘信息平滑掉（噪声、边缘都是高频成分，因为像素值通常会发生剧烈变化），两者都是低通滤波（low-pass filter）。</p>
<p>而中值滤波能够保留边缘信息，是高通滤波（high-pass filter）。</p>
<h3 id="3-其他滤波"><a href="#3-其他滤波" class="headerlink" title="3. 其他滤波"></a>3. 其他滤波</h3><p>双边滤波（bilateral filter）、导向滤波（guided filter）、拉普拉斯滤波等。</p>
<p>拉普拉斯滤波（和LoG区分一下）：</p>
<script type="math/tex; mode=display">
L(x,y) = \frac{\partial^2I}{\partial x^2} + \frac{\partial^2I}{\partial y^2}</script><p>近似图像上二阶梯度的离散卷积核（拉普拉斯算子）：</p>
<p><img src="https://homepages.inf.ed.ac.uk/rbf/HIPR2/figs/lapmask2.gif" alt=""></p>
<p>拉普拉斯滤波用途：边缘检测等。</p>
<p><img src="/images/Zly.jpg" alt=""></p>
<p>经过拉普拉斯卷积后：</p>
<p><img src="/images/edge.jpg" alt=""></p>
<p>手动实现效果与<code>cv2.Laplacian</code>效果相同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kernel = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">-4</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># no padding 卷积</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, img.shape[<span class="number">0</span>] - <span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, img.shape[<span class="number">1</span>] - <span class="number">2</span>):</span><br><span class="line">        img[i,j] = np.sum(kernel * img[i:i+<span class="number">3</span>, j:j+<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Lecture 3 局部特征" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/15/Lecture%203%20%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81/"
    >Lecture 3 局部特征</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/10/15/Lecture%203%20%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81/" class="article-date">
  <time datetime="2020-10-15T12:20:42.841Z" itemprop="datePublished">2020-10-15</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80/">机器视觉基础</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h3 id="Lecture-3-局部特征"><a href="#Lecture-3-局部特征" class="headerlink" title="Lecture 3 局部特征"></a>Lecture 3 局部特征</h3><h3 id="1-Harris角点检测"><a href="#1-Harris角点检测" class="headerlink" title="1. Harris角点检测"></a>1. Harris角点检测</h3><h4 id="1-1-特征点需要满足的性质"><a href="#1-1-特征点需要满足的性质" class="headerlink" title="1.1 特征点需要满足的性质"></a>1.1 特征点需要满足的性质</h4><p>可重复性/区分性/容易计算</p>
<p>为什么选角点？</p>
<p>一个窗口在图像上滑动，处于平坦（flat）区域时，窗口内的内容不会发生明显改变；处于边缘（edge）区域时，只有沿一个方向（竖直或水平）滑动会发生变化；而处于角点（corner）区域时，沿两个方向滑动窗口内容都会发生改变。</p>
<h4 id="1-2-角点"><a href="#1-2-角点" class="headerlink" title="1.2 角点"></a>1.2 角点</h4><script type="math/tex; mode=display">
E(u,v) = \sum_{x,y} w(x,y)[I(x + u, y + v) - I(x,y)]^2</script><p>说明：$x,y$的取值范围是窗口的范围；$w(x,y)$指权值函数，比如$Gaussian$函数，本质上取到一个加权的作用。$u,v$即平移的距离。$E(u,v)$表示窗口内容变化程度。</p>
<p>我们希望直接观察到$u,v$和$E(u,v)$之间的关系，而上式定义较麻烦不直观（还得通过$I(x,y)$），怎么直接建立关系呢？答案：二维泰勒展开！</p>
<script type="math/tex; mode=display">
E(u,v) \approx E(0,0) + [u,v]\begin{pmatrix}E_u(0,0)\\E_v(0,0)\end{pmatrix} +\frac{1}{2}[u,v]\begin{pmatrix}E_{uu}&E_{uv}\\E_{vu}&E_{vv}\end{pmatrix}\left [\begin{matrix}u\\v\end{matrix} \right]</script><script type="math/tex; mode=display">
E(0,0) = 0</script><script type="math/tex; mode=display">
E_{u}(u,v) = \sum_{x,y}2w(x,y)[I(x+u,y+v)-I(x,y)]I_x(x+u,y+v)</script><p>将(0,0)带入：得：$E_u(0,0) = 0$；同理可得$E_v(0,0)=0$；</p>
<p>为了表示方便，令：$I(x+u,y+v)$简写为$I$，$I_x(x+u,y+v)$简写为$I_x$等</p>
<script type="math/tex; mode=display">
E_{uu}(u,v) = \sum_{x,y}2w(x,y)I_xI_x+\sum_{x,y}2w(x,y)[I-I(x,y)]I_{xx}</script><script type="math/tex; mode=display">
E_{uv}(u,v) =\sum_{x,y}2w(x,y)I_yI_x+\sum_{x,y}2w(x,y)[I-I(x,y)]I_{xy}</script><p>带(0,0)有：</p>
<script type="math/tex; mode=display">
E_{uu}(0,0) = \sum_{x,y}2w(x,y)I_x(x,y)I_x(x,y)\\
E_{vv}(0,0) = \sum_{x,y}2w(x,y)I_y(x,y)I_y(x,y)\\
E_{uv}(0,0) =E_{vu}(0,0)= \sum_{x,y}2w(x,y)I_x(x,y)I_y(x,y)</script><p>即：</p>
<script type="math/tex; mode=display">
E(u,v) \approx [u,v]M\left[ \begin{matrix}u\\v\end{matrix} \right]</script><script type="math/tex; mode=display">
M_{2\times2} = \sum_{x,y}w(x,y)\left[\begin{matrix}I_x^2&I_xI_y\\I_xI_y&I_y^2\end{matrix}\right] = \left[\begin{matrix}\lambda_1&0\\0&\lambda_2\end{matrix}\right]</script><p>$I_x,I_y$即上一个Lecture讲过的图像上定义的梯度（同方向相邻两个像素的差值）。$M$被称为second moment matrix。它是一个对称正定矩阵，因此可以<strong>对角化</strong>为特征值组成的对角矩阵。</p>
<p>考虑：</p>
<script type="math/tex; mode=display">
E(u,v) = \lambda_1u^2 + \lambda_2v^2=C,C\in R</script><p>可知$E(u,v)$在几何上是一个椭圆。其轴分别为$1/\sqrt\lambda_1,1/\sqrt\lambda_2$。</p>
<p>$\lambda_1,\lambda_2$表示窗口内的$x,y$方向的图像梯度。当两个方向变化都很大时可能为角点。</p>
<script type="math/tex; mode=display">
R = \det(M) - \alpha *trace(M)^2 = \lambda_1\lambda_2-\alpha(\lambda_1+\lambda_2)^2</script><p><img src="/images/image-20201015212534161.png" alt="image-20201015212534161"></p>
<p>优点：对光照、亮度不敏感</p>
<p>缺点：对尺度（scale）变换敏感！</p>
<h3 id="2-Blob-Detection"><a href="#2-Blob-Detection" class="headerlink" title="2. Blob Detection"></a>2. Blob Detection</h3><h3 id="3-SIFT"><a href="#3-SIFT" class="headerlink" title="3. SIFT"></a>3. SIFT</h3><p>SIFT即尺度不变特征变换。光照、尺度、旋转、平移。</p>
<p>观察dog图像和log图像发现他俩走势几乎一致，可以用dog来代替log提高计算效率</p>
<p>sift 4 <em> 4 </em> 8 = 128 维描述子</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-降维方法" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/13/%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95/"
    >降维方法</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/10/13/%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95/" class="article-date">
  <time datetime="2020-10-13T11:25:37.660Z" itemprop="datePublished">2020-10-13</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="降维方法"><a href="#降维方法" class="headerlink" title="降维方法"></a>降维方法</h2><h3 id="1-主成分分析"><a href="#1-主成分分析" class="headerlink" title="1. 主成分分析"></a>1. 主成分分析</h3><h4 id="1-1-算法分析"><a href="#1-1-算法分析" class="headerlink" title="1.1 算法分析"></a>1.1 算法分析</h4><p>已知样本$\pmb a_1,\pmb a_2,…,\pmb a_N,\pmb a_i \in \mathbb{R}^n$，则按列组成的矩阵:</p>
<script type="math/tex; mode=display">
A_{n \times m} = \begin{pmatrix}\pmb a_1,&\pmb a_2,...&,\pmb a_N\end{pmatrix}</script><p><strong>矩阵的弗罗贝尼乌斯范数</strong>：</p>
<script type="math/tex; mode=display">
||A||_2 = \sqrt {tr (A^TA)} = \sqrt{\sum_{i=1}^m\sum_{j=1}^na_{ij}^2}</script><p>$tr$代表矩阵的迹函数$trace$（主对角线元素求和，$A^TA$一定是个方阵）。</p>
<p>给定一组标准正交基$\pmb w_1,\pmb w_2,…,\pmb w_N,\pmb w_i \in \mathbb{R}^n$，$\mathbb{R}^n$中的某个向量$\pmb a$可以表示为：</p>
<script type="math/tex; mode=display">
\pmb a = \sum_{j = 1}^n <\pmb a,\pmb w_j>\pmb w_j</script><p>$&lt;,&gt;$表示向量的内积操作，可以看出$&lt;\pmb a,\pmb w_j&gt;$即$\pmb a$在空间中的坐标。现在我们考虑，是否可以将$\pmb a$表示在由$\pmb w_1,\pmb w_2,…,\pmb w_m,m \mathbb{&lt;&lt;} n$，生成的子空间$\mathbb{R}^m$中，使得$&lt;\pmb a,\pmb w_{m+1}&gt;=0,…,&lt;\pmb a, \pmb w_n&gt; = 0$ ?也就是说只需要前$m$个基就可以表示$\pmb a$。</p>
<p><strong>降维准则：</strong> 找到一组基，使得数据在这组基表示下，$m+1,…n$维的坐标都是0。</p>
<p><strong>注意：</strong>$\pmb w_i$仍然是包含$n$个分量的向量，$m$个包含$n$个元素基向量构成了$n$维空间的一个$m$维子空间（空间的维数等于基向量个数）。</p>
<p>这就可以将问题转化为一个优化问题：</p>
<script type="math/tex; mode=display">
\min_{\pmb w_1,\pmb w_2,...,\pmb w_m} = \sum_{i=1}^N ||\pmb a_i - \sum_{j=1}^m<\pmb a_i,\pmb w_j>\pmb w_j||_2^2\\
s.t.<\pmb w_i,\pmb w_j> = \delta_{ij},i=j,\delta_{ij}=1;i\not=j,\delta_{ij}=0</script><p>写为矩阵形式即：</p>
<script type="math/tex; mode=display">
\arg \min_{\pmb w_1,\pmb w_2,...,\pmb w_m} = ||A-WW^TA||_2^2\\
s.t. W^TW = I_{m \times m}</script><p>其中，$A$为数据的特征矩阵，$W_{n \times m}$为前$m$个正交基按列组成的矩阵，它是半正交的。</p>
<p><strong>tips：</strong>因为有$m$个基向量，两两相乘，最后矩阵是一个$m \times m$的单位阵。</p>
<script type="math/tex; mode=display">
||A-WW^TA||_2^2 = tr\{(A-WW^TA)^T(A-WW^TA)\} = tr(A^TA) - tr(A^TWW^TA)</script><p>由于$tr(A^TA)$这一项与优化目标无关，因此目标函数变为：</p>
<script type="math/tex; mode=display">
\arg\min_{W}-tr(A^TWW^TA) = -tr\{(W^TA)^T(W^TA)\} = -||W^TA||^2_2\\s.t: W^TW = I</script><p>求出最忧$W$后，降维数据：</p>
<script type="math/tex; mode=display">
Y_{m \times N} = W_{m \times n}^TX_{n\times N}</script><p>以上推导过程从<strong>基向量</strong>、最优化角度解释了主成分分析算法。</p>
<p><strong>注意：</strong>主成分分析是无监督、线性的降维方法。线性判别分析是有监督的线性降维方法。</p>
<p><strong>说明：</strong>降维后的空间与原空间没有对应关系，主成分是原始维度的线性组合得到。</p>
<h4 id="1-2-算法描述"><a href="#1-2-算法描述" class="headerlink" title="1.2 算法描述"></a>1.2 算法描述</h4><p><img src="https://img-blog.csdnimg.cn/20191109182949751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05FRlVaWUo=,size_16,color_FFFFFF,t_70" alt="img"></p>
<p><img src="https://img-blog.csdnimg.cn/20191109191048929.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L05FRlVaWUo=,size_16,color_FFFFFF,t_70" alt="img"></p>
<h4 id="1-3-sklearn中的PCA"><a href="#1-3-sklearn中的PCA" class="headerlink" title="1.3 sklearn中的PCA"></a>1.3 sklearn中的PCA</h4><p>PCA位于$sklearn.decomposition$包下。</p>
<p>注意“explained_variance_ratio_”这个指标，表示了原始数据在不同主成分上的方差占比（保留了多少原始数据的方差信息）。</p>
<p>扩展的PCA：Incremental PCA，当数据量特别大无法一次性加载至内存进行奇异值分解时，可以将数据划分为多个mini-batch，然后进行降维。</p>
<p>Kernel PCA：非线性降维。</p>
<p><strong>Question: Can PCA  be used to reduce the dimensionality of a highly nonlinear dataset？</strong></p>
<p><strong>Answer</strong>: PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear, because it can at least get rid of useless dimensions. However, if there are no useless dimensions — for example, the Swiss roll — then reducing dimensionality with PCA will lose too much information. You want to unroll the Swiss roll, not squash it</p>
<h3 id="2-非负矩阵分解"><a href="#2-非负矩阵分解" class="headerlink" title="2. 非负矩阵分解"></a>2. 非负矩阵分解</h3><p>非负矩阵分解：Nonnegative Matrix Factorization</p>
<script type="math/tex; mode=display">
\min ||X - WH||^2\\
s.t.W>=0,H>=0</script><h3 id="3-局部线性嵌入"><a href="#3-局部线性嵌入" class="headerlink" title="3. 局部线性嵌入"></a>3. 局部线性嵌入</h3><p>局部线性嵌入：Locally Linear Embedding（LLE），<strong>属于非线性降维</strong></p>
<p>参考： <a href="https://www.cnblogs.com/pinard/p/6266408.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6266408.html</a></p>
<p>流形学习（manifold learning）第一次听说”manifold”是在学习生成对抗网络的时候看到的。</p>
<p>推导过程中需要注意的地方：</p>
<p>下图中由$(1)\rightarrow(2)$过程中，因为$\sum_{j\in Q(i)}w_{ij} = 1$，所以$x_i = x_i \times 1 = \sum_{j\in Q(i)}w_{ij}x_i$。</p>
<p><img src="E:\hexo\themes\ayer\source\images\image-20201013195547190.png" alt="image-20201013195547190"></p>
<h3 id="4-线性判别分析"><a href="#4-线性判别分析" class="headerlink" title="4. 线性判别分析"></a>4. 线性判别分析</h3><p>线性判别分析：Linear Discriminate Analysis（LDA）</p>
<p>降维原则：<strong>投影后</strong>最小化类内方差，最大化类间方差。属于线性、有监督的降维方法。</p>
<p>参考：<a href="https://www.jianshu.com/p/13ec606fdd5f" target="_blank" rel="noopener">https://www.jianshu.com/p/13ec606fdd5f</a> （二分类为例）</p>
<p>样本集：$\{(\pmb x_1,y_1),(\pmb x_2,y_2),…,(\pmb x_N,y_N)\},\pmb x_i \in \mathbb{R}^n$形成的样本矩阵$X_{n \times N}$，降维后数据$\pmb y$是一个$N \times 1$的向量</p>
<p>各个类的均值向量：</p>
<script type="math/tex; mode=display">
\pmb \mu_0 = \frac{1}{N_0}\sum_{\pmb x\in {0}}\pmb x\\
\pmb \mu_1 = \frac{1}{N_1}\sum_{\pmb x\in {1}}\pmb x</script><p>降维后的均值向量：</p>
<script type="math/tex; mode=display">
\tilde {\pmb \mu_0} = \frac{1}{N_0}\sum_{\pmb y\in 0}\pmb y =\frac{1}{N_0}\sum_{\pmb y\in 0}\pmb w^T\pmb x = \pmb w^T\pmb \mu_0</script><p>$\pmb w_{n \times 1}$就是我们要求的降维变换，同理：</p>
<script type="math/tex; mode=display">
\tilde {\pmb \mu_1} = \pmb w^T\pmb \mu_1</script><p>类内方差：</p>
<script type="math/tex; mode=display">
\tilde s_i^2 = \sum (\pmb y - \tilde{\pmb {\mu_i})^2}) = \sum (\pmb w^T\pmb x - \pmb w^T\pmb \mu_i)^2</script><p>即：</p>
<script type="math/tex; mode=display">
\tilde s_i^2 = \sum_{\pmb x\in i}[\pmb w^T(\pmb x - \pmb \mu_i)]^2 = \sum_{\pmb x\in i} [\pmb w^T(\pmb x-\pmb \mu_i)]^T[\pmb w^T(\pmb x-\pmb \mu_i)] = \sum_{\pmb x\in i} (\pmb x -\pmb \mu_i)^T\pmb w\pmb w^T(\pmb x-\pmb \mu_i)\\=\sum_{\pmb x\in i}\pmb w^T(\pmb x - \pmb \mu_i)(\pmb x - \pmb \mu_i)^T\pmb w</script><p><strong>说明：</strong>因为$(\pmb x -\pmb \mu_i)^T\pmb w$与$\pmb w^T(\pmb x-\pmb \mu_i)$ 结果分别是两个实数，可以交换。</p>
<p>令</p>
<script type="math/tex; mode=display">
\sum_{\pmb x\in i}(\pmb x - \pmb \mu_i)(\pmb x - \pmb \mu_i)^T = S_i</script><p>根据投影后类内方差小（$\tilde s_1^2 + \tilde s_0^2$ 两个都要小总体才能小），类间方差大（$||\pmb \mu_1 - \pmb \mu_2||^2$）。可以写出目标函数：</p>
<script type="math/tex; mode=display">
\max J(\pmb w) = \max \frac{||\pmb \mu_1 - \pmb \mu_2||^2}{\tilde s_1^2 + \tilde s_0^2}</script><p>并且根据上面的式子有：</p>
<script type="math/tex; mode=display">
\tilde s_1^2 + \tilde s_0^2 = \pmb w^T(S_0 + S_1)\pmb w =\pmb w^TS_W\pmb w = \tilde S_w</script><p>$S_W$：类内方差（with class）维度：$n \times n$</p>
<p>对于分子，我们有：</p>
<script type="math/tex; mode=display">
||\pmb \mu_1 - \pmb \mu_2||^2 = (\pmb w^T\pmb \mu_1 - \pmb w^T\pmb \mu_2)^2=\pmb w^T(\pmb \mu_1-\pmb \mu_2)(\pmb \mu_1-\pmb \mu_2)^T\pmb w</script><p>令：</p>
<script type="math/tex; mode=display">
(\pmb \mu_1-\pmb \mu_2)(\pmb \mu_1-\pmb \mu_2)^T = S_B</script><p>$S_B$：类间方差（between class）维度：$n \times n$</p>
<p>所以：</p>
<script type="math/tex; mode=display">
||\pmb \mu_1 - \pmb \mu_2||^2  = \pmb w^TS_B\pmb w = \tilde S_B</script><p>目标函数变为：</p>
<script type="math/tex; mode=display">
\max_{\pmb w} \frac{\pmb w^TS_B\pmb w}{\pmb w^TS_W\pmb w }</script><p>对$J(\pmb w)$对$\pmb w$求导并令其等于0即：</p>
<script type="math/tex; mode=display">
2S_B\pmb w(\pmb w^TS_W\pmb w)-2S_W\pmb w(\pmb w^TS_B\pmb w) = 0</script><p>注意：$\pmb w^TS_W\pmb w,\pmb w^TS_B\pmb w$结果都是实数，因此等式两边同除$\pmb w^TS_B\pmb w$可得：</p>
<script type="math/tex; mode=display">
S_W\pmb w =\frac{\pmb w^TS_W\pmb w}{\pmb w^TS_B\pmb w} S_B\pmb w</script><script type="math/tex; mode=display">
\pmb w = CS_W^{-1}S_B\pmb w</script><p>注意到：</p>
<script type="math/tex; mode=display">
S_B\pmb w =(\pmb \mu_1-\pmb \mu_2)(\pmb \mu_1-\pmb \mu_2)^T\pmb w</script><p>且$(\pmb \mu_1-\pmb \mu_2)^T\pmb w$结果是一个实数，C也是实数，我们要求的最关键的是$\pmb w$的方向，这些实数并不会改变方向，因此</p>
<script type="math/tex; mode=display">
\pmb w^* = S_W^{-1}(\pmb \mu_1 - \pmb \mu_2)</script><h3 id="5-多维尺度分析"><a href="#5-多维尺度分析" class="headerlink" title="5. 多维尺度分析"></a>5. 多维尺度分析</h3><p>Multidimensional Scaling</p>
<p>基本思想：尽量满足原始高维度空间中样本之间的距离在低维空间中得以保持。</p>
<p>“metric” or “non-metric”</p>
<p>给定样本$(\pmb x_1,\pmb x_2,…,\pmb x_N),\pmb x_i \in \mathbb{R}^n$构成的样本矩阵$X_{n \times N}$，降维后的样本矩阵$Y_{m \times N}$。根据多维尺度分析的基本思想：保持样本间距离不变，原始样本间距离$d_{ij}=||\pmb x_i - \pmb x_j||$构成的矩阵$D_{N \times N}$。</p>
<p>可知$X^TX = -\frac{1}{2}HD_xH, Y^TY = \frac{1}{2}HD_yH$，我们想让$D_x=D_y$，等价的可以通过以下目标函数来求得：</p>
<p>目标函数：</p>
<script type="math/tex; mode=display">
\min ||X^TX - Y^TY||^2</script><p>特征值分解：</p>
<script type="math/tex; mode=display">
X^TX = U^T\Lambda U</script><p>那么降维后的数据$Y$：</p>
<script type="math/tex; mode=display">
Y = \Lambda^{1/2}U</script><p>ISOMAP </p>
<h3 id="6-典型关联分析"><a href="#6-典型关联分析" class="headerlink" title="6. 典型关联分析"></a>6. 典型关联分析</h3><p>Canonical correlation analysis</p>
<p>参考：<a href="https://www.cnblogs.com/pinard/articles/6288716.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/articles/6288716.html</a></p>
<p>具体思想：相关系数 $\rho$ 可以分析两组一维数据$X,Y$的线性相关性，$\rho$取值越接近1则$X,Y$的线性相关性越高。虽然相关系数可以很好的帮我们分析一维数据的相关性，但是对于高维数据就不能直接使用了。比如$X$是2维数据，$Y$是三维数据，就不能用相关系数进行分析。那么有没有变通的方法呢？典型关联分析给出了思路，具体就是将多维的$X,Y$分别用线性变换为1维的$X^\prime,Y^\prime$，然后使用相关系数分析1维$X^\prime,Y^\prime$的相关性。问题又来了，如何把$X,Y$变换为$X^\prime，Y^\prime$呢？也就是说降维的准则是什么？<strong>典型关联分析使用的准则是变换到1维后，$X^\prime,Y^\prime$的相关系数最大。</strong></p>
<p>亦即：</p>
<script type="math/tex; mode=display">
arg \max_{\pmb a,\pmb b} ||X^T\pmb a - Y^T\pmb b||^2</script><h3 id="7-字典学习"><a href="#7-字典学习" class="headerlink" title="7. 字典学习"></a>7. 字典学习</h3><p>参考：<a href="https://zhuanlan.zhihu.com/p/46085035" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46085035</a></p>
<p><strong>基本思想：</strong></p>
<p>在人类发展的近几千年历史中，文字对人类文明的推动起着举足轻重的作用。人类用文字记述了千年的历史，用文字留下了各种思想火花，用文字抒发了各种各样的情感等等。但是这一切的内容，只需要一本字典就能表述完。因为人在这环节中的功能，无非就是使用字典当中的字词进行了适当的排列了而已。</p>
<p>基于这种思想，先前的大佬提出了字典学习——Dictionary Learning。</p>
<p>字典学习的目标，就是提取事物<strong>最本质的特征（类似于字典当中的字或词语）。</strong>如果我们能都获取这本包括<strong>最本质的特征</strong>的字典，那我们就掌握了这个事物的最本质的内涵。换言之，字典学习将我们的到的对于物体的信息降维，减少了该物体一些无关紧要信息对我们定义这个物体的干扰。</p>
<p><strong>模型建立：</strong></p>
<script type="math/tex; mode=display">
\pmb x = \sum_{i=1}^m \lambda_i\pmb w_i</script><p>$\lambda_i$称为表示系数，要求尽可能稀疏；$\pmb w_i$被称为”字典“。</p>
<p>问题是如何找到表示系数与字典？可转化为如下优化问题：</p>
<script type="math/tex; mode=display">
\min \sum_{i=1}^N||\pmb x_i - \sum_{j=1}^m y_{ij}\pmb w_j||^2 \\
s.t. ||\pmb y_i|| <= C</script><p>其中：$y$即为表示系数，约束条件对应要求表示系数尽可能稀疏。$m &gt;&gt;N$。</p>
<p>写成矩阵形式：</p>
<script type="math/tex; mode=display">
\min ||X - WY||^2\\
s.t.||Y|| <= C</script><p>由于上述优化问题含有两个优化变量，可采用坐标优化方法，即先固定系数$y$，求出最优的$\pmb w$；然后在求出系数。</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-机器视觉基础Lecture 2" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/08/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80Lecture%202/"
    >机器视觉基础Lecture 2</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/10/08/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80Lecture%202/" class="article-date">
  <time datetime="2020-10-08T09:28:38.672Z" itemprop="datePublished">2020-10-08</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80/">机器视觉基础</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="机器视觉基础-Lecture-2"><a href="#机器视觉基础-Lecture-2" class="headerlink" title="机器视觉基础 Lecture 2"></a>机器视觉基础 Lecture 2</h2><h3 id="0-数字图像基础"><a href="#0-数字图像基础" class="headerlink" title="0. 数字图像基础"></a>0. 数字图像基础</h3><h4 id="0-1-相机的成像原理"><a href="#0-1-相机的成像原理" class="headerlink" title="0.1 相机的成像原理"></a>0.1 相机的成像原理</h4><p>数码相机使用了感光器件（CMOS/CCD），将光信号转变为电信号（模拟信号），再经模/数转换后形成数字信号，在经过一系列处理形成特定的图像文件格式存储于存储卡上。</p>
<h4 id="0-2-图像的采样和量化"><a href="#0-2-图像的采样和量化" class="headerlink" title="0.2 图像的采样和量化"></a>0.2 图像的采样和量化</h4><p>采样，就是把一幅连续图像在空间上分割成M×N个网格，每个网格用一亮度值来表示。一般来说，采样间隔越大，所得图像像素数越少，空间分辨率低，质量差，严重时出现马赛克效应；采样间隔越小，所得图像像素数越多，空间分辨率高，图像质量好，但数据量大。</p>
<p>量化，就是把采样点上对应的亮度连续变化区间转换为单个特定数码的过程。量化后，图像就被表示成一个整数矩阵。每个像素具有两个属性：位置和灰度。位置由行、列表示。灰度表示该像素位置上亮暗程度的整数。此数字矩阵M×N就作为计算机处理的对象了。灰度级一般为0－255（8bit量化）。</p>
<h4 id="0-3-数字图像和信号处理"><a href="#0-3-数字图像和信号处理" class="headerlink" title="0.3 数字图像和信号处理"></a>0.3 数字图像和信号处理</h4><p>图片也是一种特殊的信号。</p>
<h4 id="0-4-颜色空间"><a href="#0-4-颜色空间" class="headerlink" title="0.4 颜色空间"></a>0.4 颜色空间</h4><p>RGB、HSV、Lab、CIE等等。</p>
<h4 id="0-5-图像的直方图"><a href="#0-5-图像的直方图" class="headerlink" title="0.5 图像的直方图"></a>0.5 图像的直方图</h4><h3 id="1-图像滤波"><a href="#1-图像滤波" class="headerlink" title="1. 图像滤波"></a>1. 图像滤波</h3><h4 id="1-1-滤波器"><a href="#1-1-滤波器" class="headerlink" title="1.1 滤波器"></a>1.1 滤波器</h4><p>滤波器英文filter/mask/kernel。可分为线性滤波器和非线性滤波器，高通滤波器（只允许图像中高频成分通过，换句话说滤掉低频成分）和低通滤波器。</p>
<p>均值滤波（低通滤波）、高斯滤波（低通滤波，重要的两个参数：kernel size和方差$\sigma^2$）、中值滤波（高通滤波，非线性，会锐化sharpen图片，比如增强边缘）、拉普拉斯滤波。</p>
<p>窗口滑动时如何处理边界问题：full、same、valid。</p>
<p>低通滤波会不同程度的平滑图片。</p>
<p>高频成分：图像中的边缘、噪声。</p>
<p>本质上：采用一个kernel然后在图像上滑动加权求和，因此<strong>kernel的大小和权重</strong>很重要。</p>
<h4 id="1-2-梯度与边缘检测"><a href="#1-2-梯度与边缘检测" class="headerlink" title="1.2 梯度与边缘检测"></a>1.2 梯度与边缘检测</h4><p>由于图像边缘处像素值通常变化剧烈，由此形成像素值上的不连续，因此可以通过求梯度来检测图像中的边缘。将图像看作一个二维离散函数$f(x,y)$，图像梯度其实就是这个二维离散函数的求导。</p>
<p>正常微分定义：</p>
<script type="math/tex; mode=display">
\frac{\partial f(x,y)}{\partial x} = \lim_{\epsilon \rightarrow 0} \frac{f(x + \epsilon,y) - f(x,y)}{\epsilon},\frac{\partial f(x,y)}{\partial y} = \lim_{\epsilon \rightarrow 0} \frac{f(x,y  + \epsilon) - f(x,y)}{\epsilon}</script><p>但是图像是按照像素来离散的，最小也就1像素，因此普通的微分形式变成了：</p>
<script type="math/tex; mode=display">
\frac{\partial f(x,y)}{\partial x} = f(x + 1,y) - f(x,y),\frac{\partial f(x,y)}{\partial y} =f(x,y+1) - f(x,y)</script><p><strong>可以看出，图像的梯度本质上相当于2个相邻像素之间的差值。</strong></p>
<p>因此定义梯度算子（kernel）：水平$[-1,1]$ 右边像素 - 左边像素；垂直：$[-1,1]^T$下边像素-上边像素</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">"126007.jpg"</span>,<span class="number">0</span>)</span><br><span class="line">img = img.astype(<span class="string">'float'</span>)</span><br><span class="line"></span><br><span class="line">row = img.shape[<span class="number">0</span>]</span><br><span class="line">col = img.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">gradient_x = np.zeros((row, col))</span><br><span class="line">gradient_y = np.zeros((row, col))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(row - <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(col - <span class="number">1</span>):</span><br><span class="line">        gy = abs(img[i + <span class="number">1</span>, j] - img[i, j]) <span class="comment">#下边像素-上边像素</span></span><br><span class="line">        gx = abs(img[i, j + <span class="number">1</span>] - img[i, j]) <span class="comment">#右边像素-左边像素</span></span><br><span class="line">        gradient_x[i, j] = gx</span><br><span class="line">        gradient_y[i, j] = gy</span><br><span class="line">gradient_x = gradient_x.astype(<span class="string">'uint8'</span>)</span><br><span class="line">gradient_y = gradient_y.astype(<span class="string">'uint8'</span>)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">"gradient_x"</span>, gradient_x)</span><br><span class="line">cv2.imshow(<span class="string">"gradient_y"</span>, gradient_y)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure>
<p><img src="/images/grad_x.jpg" alt=""></p>
<p>水平方向梯度</p>
<p><img src="/images/grad_y.jpg" alt=""></p>
<p>垂直方向梯度</p>
<p>水平方向求偏导：$\frac{\partial f}{\partial x}$，竖直方向求偏导：$\frac{\partial f}{\partial y}$，图像梯度：$\nabla f = [\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}]$。</p>
<p>水平：$\nabla f = [\frac{\partial f}{\partial x},0]$，垂直：$\nabla f = [0,\frac{\partial f}{\partial y}]$</p>
<p>梯度方向：$\theta = \tan^{-1}(\frac{\partial f}{\partial y}/\frac{\partial f}{\partial x})$</p>
<p><strong>梯度幅值（gradient magnitude）</strong>：$||\nabla f||=\sqrt {(\frac{\partial f}{\partial x})^2+(\frac{\partial f}{\partial y})^2}$</p>
<p>注意：求梯度之前先做平滑，否则噪声也是高频，求梯度后无法分辨哪里是噪声哪里是边缘。如下图。</p>
<p><img src="/images/image-20201008191252006.png" alt="image-20201008191252006" style="zoom:67%;" /></p>
<p><img src="/images/image-20201008191306409.png" alt="image-20201008191306409" style="zoom:67%;" /></p>
<p>首先平滑图像后：</p>
<p><img src="/images/image-20201008191404392.png" alt="image-20201008191404392" style="zoom:67%;" /></p>
<p>$f:signal;g:kernel$</p>
<script type="math/tex; mode=display">
\frac{d}{dx}(f*g) = f*\frac{d}{dx}g</script><p>用高斯一阶导去和信号做卷积，干了两件事：一是平滑信号，二是求导。相当于先用高斯滤波平滑再求导。 同理高斯二阶导，用高斯二阶导去和信号卷积。</p>
<ul>
<li>高斯核求和为1，用途：平滑</li>
<li>高斯一阶导核求和为0，用途：边缘提取</li>
</ul>
<p><strong>Canny边缘检测</strong></p>
<p>代码参见同专题下另一篇博客。</p>
<h4 id="1-3-模板匹配"><a href="#1-3-模板匹配" class="headerlink" title="1.3 模板匹配"></a>1.3 模板匹配</h4><p>滤波器的一个应用。</p>
<h3 id="2-图像的特征"><a href="#2-图像的特征" class="headerlink" title="2. 图像的特征"></a>2. 图像的特征</h3><h4 id="2-1-图像纹理texture"><a href="#2-1-图像纹理texture" class="headerlink" title="2.1 图像纹理texture"></a>2.1 图像纹理texture</h4><p>纹理分类</p>
<p>纹理是由重复的局部<strong>模式</strong>（local pattern）构成的。</p>
<p>如何描述？多个卷积核卷积得到特征向量。</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-多元高斯及其极大似然估计" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/06/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%8F%8A%E5%85%B6%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"
    >多元高斯及其极大似然估计</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/10/06/%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%8F%8A%E5%85%B6%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/" class="article-date">
  <time datetime="2020-10-06T07:54:18.217Z" itemprop="datePublished">2020-10-06</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="多元高斯及其极大似然估计"><a href="#多元高斯及其极大似然估计" class="headerlink" title="多元高斯及其极大似然估计"></a>多元高斯及其极大似然估计</h2><p>参考：<a href="https://blog.csdn.net/Joyliness/article/details/80097491" target="_blank" rel="noopener">https://blog.csdn.net/Joyliness/article/details/80097491</a></p>
<h3 id="1-独立多元高斯的概率密度函数"><a href="#1-独立多元高斯的概率密度函数" class="headerlink" title="1. 独立多元高斯的概率密度函数"></a>1. 独立多元高斯的概率密度函数</h3><p>$n$个独立的随机变量$(x_1,x_2,…x_n)$的联合密度函数为</p>
<script type="math/tex; mode=display">
f_{\mu,\Sigma}(\pmb x)=\frac{1}{\sqrt{(2\pi)^n\det\Sigma}}e^{-\frac{1}{2}(\pmb x - \pmb \mu)^T\Sigma^{-1}(\pmb x - \pmb \mu)}</script><p>就是多元高斯分布（Multivariate Normal Distribution，MVN）。其中，$\pmb \mu$是各个随机变量的期望值所组成的向量，$\Sigma$是随机变量间的协方差矩阵，是$n \times n$的正定矩阵。</p>
<h3 id="2-多元高斯的极大似然估计"><a href="#2-多元高斯的极大似然估计" class="headerlink" title="2. 多元高斯的极大似然估计"></a>2. 多元高斯的极大似然估计</h3><p>对于$N$个满足多元高斯分布的独立样本集：$\{\pmb x_1,\pmb x_2,..,\pmb x_N\},\pmb x \in  \mathbb{R}^n$，似然函数为：</p>
<script type="math/tex; mode=display">
\prod_{i=1}^Nf_{\mu,\Sigma}(\pmb x_i) = (2\pi)^{-\frac{Nn}{2}}|\Sigma|^{-\frac{N}{2}}e^{-\frac{1}{2}\sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)}</script><p>取对数：</p>
<script type="math/tex; mode=display">
\ln L(\pmb \mu,\Sigma) =\ln\prod_{i=1}^Nf_{\mu,\Sigma}(\pmb x_i) =-\frac{Nn}{2}\ln2\pi -\frac{N}{2}\ln |\Sigma| - \sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)\\
=C - \frac{N}{2}\ln |\Sigma| - \frac{1}{2}\sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)</script><p>其中$C = -\frac{Nn}{2}\ln2\pi$是一个与参数$\pmb \mu,\Sigma$ 无关的常数，因此：</p>
<script type="math/tex; mode=display">
arg\max_{\pmb \mu,\Sigma}\ln\prod_{i=1}^Nf_{\mu,\Sigma}(\pmb x_i)</script><ul>
<li>$\ln L(\pmb \mu,\Sigma)$对$\pmb \mu$求偏导（标量对向量求偏导）</li>
</ul>
<p>先将$\sum_{i=1}^N(\pmb x_i - \pmb \mu)^T\Sigma^{-1}(\pmb x_i - \pmb \mu)$展开，得到：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N\pmb x_i^T\Sigma^{-1}\pmb x_i - 2\sum_{i=1}^N\pmb x^T\Sigma^{-1}\pmb \mu + N\pmb \mu^T\Sigma^{-1}\pmb \mu</script><p>后两项对$\pmb \mu$求偏导得到：</p>
<script type="math/tex; mode=display">
-2\sum_{i=1}^N\Sigma^{-1}\pmb x_i + 2N\Sigma^{-1}\pmb \mu</script><p>因此：</p>
<script type="math/tex; mode=display">
\frac{\partial \ln L(\pmb \mu,\Sigma)}{\partial \pmb \mu} = 2\sum_{i=1}^N\Sigma^{-1}\pmb x_i - 2N\Sigma^{-1}\pmb \mu = 0</script><p>求得：</p>
<script type="math/tex; mode=display">
\bar{\pmb \mu} = \frac{1}{N}\sum_{i=1}^N \pmb x_i</script><ul>
<li>$\ln L(\pmb \mu, \Sigma)$对$\Sigma$求偏导（标量对矩阵求偏导）</li>
</ul>
<p>tips：$\frac{\partial \det(X)}{\partial X } = \det (X)tr(X^{-1})$</p>
<script type="math/tex; mode=display">
\frac{\partial -\frac{N}{2}\ln \det(\Sigma)}{\partial \Sigma} = -\frac{N}{2}tr(X^{-1})</script><script type="math/tex; mode=display">
\frac{\partial \ln L(\pmb \mu, \Sigma)}{\partial \Sigma} = 0</script><p>解得：</p>
<script type="math/tex; mode=display">
\bar \Sigma = \frac{1}{N} \sum_{i=1}^N(\pmb x_i - \bar{\pmb \mu})(\pmb x_i - \bar{\pmb \mu})^T</script>
      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Transformer模型介绍" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/03/Transformer%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/"
    >Transformer模型介绍</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/10/03/Transformer%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/" class="article-date">
  <time datetime="2020-10-03T06:58:05.171Z" itemprop="datePublished">2020-10-03</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="Transformer模型介绍"><a href="#Transformer模型介绍" class="headerlink" title="Transformer模型介绍"></a>Transformer模型介绍</h2><p>文献：Vaswani A,  Shazeer N,  Parmar N,  et al.  Attention is all you need[C]. Advances in neural information processing systems. 2017: 5998-6008.</p>
<p>详解：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a>  The Annotated Transformer.</p>
<p>视频：<a href="https://www.youtube.com/watch?v=ugWDIIOHtPA" target="_blank" rel="noopener">https://www.youtube.com/watch?v=ugWDIIOHtPA</a>  李宏毅：Transformer</p>
<h3 id="1-encoder-decoder"><a href="#1-encoder-decoder" class="headerlink" title="1. encoder-decoder"></a>1. encoder-decoder</h3><h3 id="2-self-attention"><a href="#2-self-attention" class="headerlink" title="2. self attention"></a>2. self attention</h3><h3 id="3-multi-head-attention"><a href="#3-multi-head-attention" class="headerlink" title="3. multi head attention"></a>3. multi head attention</h3><h3 id="4-attention和self-attention区别"><a href="#4-attention和self-attention区别" class="headerlink" title="4. attention和self-attention区别"></a>4. attention和self-attention区别</h3>
      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-贝叶斯垃圾邮件分类器" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/09/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/"
    >贝叶斯垃圾邮件分类器</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/09/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E5%88%86%E7%B1%BB%E5%99%A8/" class="article-date">
  <time datetime="2020-09-29T03:06:11.061Z" itemprop="datePublished">2020-09-29</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="贝叶斯垃圾邮件分类器"><a href="#贝叶斯垃圾邮件分类器" class="headerlink" title="贝叶斯垃圾邮件分类器"></a>贝叶斯垃圾邮件分类器</h2><p>利用朴素贝叶斯算法，在给定训练数据上，训练一个垃圾分类器。有关朴素贝叶斯的基本思想，可以参见另一篇博客：<a href="https://zyj-nova.github.io/2020/02/09/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88Naive%20Bayes%EF%BC%89/" target="_blank" rel="noopener">https://zyj-nova.github.io/2020/02/09/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88Naive%20Bayes%EF%BC%89/</a></p>
<h3 id="0-数据集获取"><a href="#0-数据集获取" class="headerlink" title="0 数据集获取"></a>0 数据集获取</h3><p>链接：<a href="https://pan.baidu.com/s/1P3Jg67nerg63GVbCkc1YJg" target="_blank" rel="noopener">https://pan.baidu.com/s/1P3Jg67nerg63GVbCkc1YJg</a> ，提取码：576u 。</p>
<p>数据集为英文语料，分为训练集和测试集。训练集包含351封垃圾邮件（文件名标有“spmsg”）、351封正常邮件；测试集包含130封正常邮件和130封垃圾邮件。</p>
<h3 id="1-特征提取"><a href="#1-特征提取" class="headerlink" title="1 特征提取"></a>1 特征提取</h3><p>由于每一封邮件都不是数值型的特征，因此我们需要把他们编码。首先需要构建一个字典，统计所有邮件中单词出现的频率，同时删除字典中的非字母字符、长度为1的字符，之后取前3000个出现频率最高的作为最终的字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">n_dim = <span class="number">3000</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_Dictionary</span><span class="params">(train_dir)</span>:</span></span><br><span class="line">    emails = [os.path.join(train_dir, f) <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(train_dir)]</span><br><span class="line">    all_words = []</span><br><span class="line">    <span class="keyword">for</span> mail <span class="keyword">in</span> emails:</span><br><span class="line">        <span class="keyword">with</span> open(mail) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment"># 遍历每一行</span></span><br><span class="line">            <span class="keyword">for</span> i,line <span class="keyword">in</span> enumerate(f):</span><br><span class="line">                <span class="keyword">if</span>(i == <span class="number">2</span>):</span><br><span class="line">                    <span class="comment">#正文开始</span></span><br><span class="line">                    words = line.split()<span class="comment">#默认按照空格分词</span></span><br><span class="line">                    all_words += words</span><br><span class="line">    dictionary = Counter(all_words)</span><br><span class="line">    <span class="comment"># 词典构建好，删除多余的词汇</span></span><br><span class="line">    list_to_remove = list(dictionary.keys())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> list_to_remove:</span><br><span class="line">        <span class="keyword">if</span> item.isalpha() == <span class="literal">False</span>:</span><br><span class="line">            <span class="keyword">del</span> dictionary[item]</span><br><span class="line">        <span class="keyword">elif</span> len(item) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">del</span> dictionary[item]</span><br><span class="line">    dictionary = dictionary.most_common(n_dim)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dictionary</span><br></pre></td></tr></table></figure>
<p>构建完字典以后，就可以对每一封邮件进行编码了。特征向量的每一个分量即邮件中出现的对应词频。例如邮件“do over，do again”可以被编码为$[0,0,…,2,…,1,…,1]$。每一个分量都代表对应的词频。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将每个邮件转化为词频向量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_features</span><span class="params">(mail_dir)</span>:</span></span><br><span class="line">    </span><br><span class="line">    files = [os.path.join(mail_dir, f) <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(mail_dir)]</span><br><span class="line">    labels = np.ones(len(files))</span><br><span class="line">    feature_matrix = np.zeros((len(files), n_dim))</span><br><span class="line">    docID = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        <span class="keyword">with</span> open(file) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">if</span> f.name[<span class="number">30</span>:].find(<span class="string">"sp"</span>) != <span class="number">-1</span>:</span><br><span class="line">                <span class="comment">#垃圾邮件</span></span><br><span class="line">                labels[docID] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(f):</span><br><span class="line">                <span class="keyword">if</span> i == <span class="number">2</span>:</span><br><span class="line">                    words = line.split()</span><br><span class="line">                    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">                        wordID = <span class="number">0</span></span><br><span class="line">                        <span class="keyword">for</span> i,d <span class="keyword">in</span> enumerate(dictionary):</span><br><span class="line">                            <span class="keyword">if</span>(d[<span class="number">0</span>] == w):</span><br><span class="line">                                wordID = i</span><br><span class="line">                                feature_matrix[docID, wordID] += <span class="number">1</span></span><br><span class="line">        docID += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> feature_matrix, labels</span><br></pre></td></tr></table></figure>
<h3 id="2-计算概率"><a href="#2-计算概率" class="headerlink" title="2 计算概率"></a>2 计算概率</h3><p>$P(word|y =spam) = P(word, spam) / P(spam)$，$P(word, spam)$即每个单词在垃圾邮件中出现的频数，$P(spam)$即垃圾邮件中所有单词的词频总和。正常邮件同理，得到概率矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计垃圾邮件中的所有词频和，以及各个词的频数</span></span><br><span class="line">spam_words = <span class="number">0</span></span><br><span class="line">non_spam_words = <span class="number">0</span></span><br><span class="line">spam_mails = []</span><br><span class="line">non_spam_mails = []</span><br><span class="line"><span class="keyword">for</span> i,line <span class="keyword">in</span> enumerate(feature_matrix):</span><br><span class="line">    <span class="keyword">if</span> labels[i] == <span class="number">1</span>:</span><br><span class="line">        non_spam_words += int(feature_matrix[i].sum())</span><br><span class="line">        non_spam_mails.append(feature_matrix[i])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        spam_mails.append(feature_matrix[i])</span><br><span class="line">        spam_words += int(feature_matrix[i].sum())</span><br><span class="line">print(<span class="string">"spam total words are &#123;&#125;, non-spam total words are &#123;&#125;"</span>.format(spam_words, non_spam_words))</span><br><span class="line"></span><br><span class="line">spam_mails = np.array(spam_mails)</span><br><span class="line">non_spam_mails = np.array(non_spam_mails)</span><br><span class="line"></span><br><span class="line">weight_matrix = np.zeros((<span class="number">2</span>,n_dim))</span><br><span class="line">weight_matrix[<span class="number">0</span>, :] = spam_mails.sum(axis = <span class="number">0</span>) / spam_words</span><br><span class="line">weight_matrix[<span class="number">1</span>, :] = non_spam_mails.sum(axis = <span class="number">0</span>) / non_spam_words</span><br></pre></td></tr></table></figure>
<p>为了避免概率矩阵中出现0的情况，使得计算概率时出现乘0，最后让权重矩阵每个分量加一个非常小的扰动</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sigma = <span class="number">1e-8</span></span><br><span class="line">weight_matrix += sigma</span><br></pre></td></tr></table></figure>
<h3 id="3-模型预测与评估"><a href="#3-模型预测与评估" class="headerlink" title="3 模型预测与评估"></a>3 模型预测与评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pred_labels = []</span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> test_feature:</span><br><span class="line">    spam_prob = <span class="number">1.0</span></span><br><span class="line">    non_spam_prob = <span class="number">1.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i,w <span class="keyword">in</span> enumerate(feature):</span><br><span class="line">        <span class="keyword">if</span> w != <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 计算P(w|spam) P(w|non-spam)</span></span><br><span class="line">            spam_prob += np.log10(weight_matrix[<span class="number">0</span>, i])</span><br><span class="line">            non_spam_prob += np.log10(weight_matrix[<span class="number">1</span>, i])</span><br><span class="line">    print(spam_prob, non_spam_prob)</span><br><span class="line">    <span class="keyword">if</span> spam_prob &gt; non_spam_prob:</span><br><span class="line">        pred_labels.append(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pred_labels.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>在计算概率的时候用到了一个trick，由于非常小的数不断连乘最后会趋于0，因此利用取对数的方法将乘法化为加法（极大似然估计就是这么干的）。同时取对数时不可以为0，上面加的非常小的扰动是很有必要的。</p>
<p>模型在测试集上的准确率、召回率、f1_score如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">recall_score : <span class="number">0.9923076923076923</span></span><br><span class="line">accuracy_score : <span class="number">0.9846153846153847</span></span><br><span class="line">f1_score : <span class="number">0.9847328244274809</span></span><br></pre></td></tr></table></figure>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-Strang教授线性代数笔记" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/09/23/Strang%E6%95%99%E6%8E%88%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%AC%94%E8%AE%B0/"
    >Strang教授线性代数笔记</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/09/23/Strang%E6%95%99%E6%8E%88%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2020-09-23T02:44:37.995Z" itemprop="datePublished">2020-09-23</time>
</a>
      
      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h3 id="Strang教授线性代数笔记"><a href="#Strang教授线性代数笔记" class="headerlink" title="Strang教授线性代数笔记"></a>Strang教授线性代数笔记</h3><p>具体参见：<a href="https://github.com/zyj-nova/LinearAlgebraNotes" target="_blank" rel="noopener">https://github.com/zyj-nova/LinearAlgebraNotes</a></p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
    <article id="post-浅谈逻辑斯蒂回归模型" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/08/03/%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"
    >浅谈逻辑斯蒂回归模型</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/08/03/%E6%B5%85%E8%B0%88%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2020-08-03T08:01:07.919Z" itemprop="datePublished">2020-08-03</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

      
      
      
    </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="浅谈逻辑斯蒂回归模型"><a href="#浅谈逻辑斯蒂回归模型" class="headerlink" title="浅谈逻辑斯蒂回归模型"></a>浅谈逻辑斯蒂回归模型</h2><h3 id="sigmod函数"><a href="#sigmod函数" class="headerlink" title="sigmod函数"></a>sigmod函数</h3><p>​        $sigmod$函数及其导数形式如下：</p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">
f^\prime (x)= f(x)*(1-f(x))</script><p>​        逻辑斯蒂回归或者是$soft\max$回归，说是回归其实是在解决二分类或者多分类的问题。逻辑斯蒂模型如下：</p>
<script type="math/tex; mode=display">
y = \frac{1}{1 + e^{-（w^Tx +b)}}</script><p>​        其中，$x \in R^n, w \in R^n$，$b$为偏置bias可以设置为0，$x$为样本的特征向量，$w$即要学习的参数。在给定一批有label的训练数据集后，我们可以根据极大似然估计或者梯度下降来得到最优的参数值，之后，给定某个测试数据，当：$y &gt;= 0.5$，我们就把这个样本划为1类，否则就划为0类。</p>
<p>​        其实，完整的二项逻辑斯蒂回归模型是定义如下的条件概率分布：</p>
<script type="math/tex; mode=display">
P(Y = 1|x) = \frac{\exp(w*x + b)}{1 + \exp(w*x + b)}</script><script type="math/tex; mode=display">
P(Y = 0|x) = \frac{1}{1 + \exp(w*x + b)}</script><p>​        观察上述条件概率计算公式我们发现，当$w<em>x+b &gt; 0$的时候，$\exp(w</em>x+b)&gt;1,P(Y=1)$的概率一定高于$P(Y=0)$，我们就把其划分为1类。对于$P(Y=1)$的分布函数上下同除以分子$\exp$就得到了熟悉的逻辑斯蒂函数。</p>
<p>​        事实上，逻辑斯蒂函数是在计算样本为1类的概率，由于是二分类，不是1类就是0类。</p>
<p>​        <strong>本质上，逻辑斯蒂回归是一个线性分类器，以二维特征为例，如果不对特征进行任何组合，不使用核函数，其决策平面始终是一条直线：$w^Tx = 0$</strong>。</p>
<p><img src="/images/make_moons.png" alt=""></p>
<p>​        以make moons生成的数据集为例，不对特征进行任何处理，决策平面如下：</p>
<p><img src="/images/log_bundary2.png" alt=""></p>
<p>​        若对两维特征进行平方处理，绘制的决策平面如下：</p>
<p><img src="/images/log_bundary1.png" alt=""></p>
<p>​        谈到特征融合，决策树就可以拿来做特征融合（从根节点到叶子节点的每一条路径都是一个组合特征）。</p>
<h3 id="softmax回归模型"><a href="#softmax回归模型" class="headerlink" title="softmax回归模型"></a>softmax回归模型</h3><p>参考：<a href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" target="_blank" rel="noopener">https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/</a></p>
<p><strong>softmax函数</strong></p>
<script type="math/tex; mode=display">
\sigma (\pmb x)_i = \frac{e^{x_i}}{\sum_j^n e^{x_j}},i = 1,...,n,\pmb x=(x_1,...,x_n)</script><p>定义：</p>
<script type="math/tex; mode=display">
S_j = \frac{e^{x_j}}{\sum_{k=1}^ne^{x_k}}, j = 1,2,...,n</script><p>​        可以看到，softmax函数输入一个向量，输出是一个向量，是一个$\R^n \rightarrow \R^n$的函数。下面讨论其输入的各个分量的偏导，可以看到其输出分量对输入分量的偏导构成了一个$n \times n$的雅可比矩阵。</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{\partial S_i}{\partial x_j}</script><script type="math/tex; mode=display">
DS = \begin{pmatrix}D_1S_1&D_1S_2&...&D_1S_n\\...&...&...&...\\D_nS_1&..&...&D_nS_n\end{pmatrix}</script><p>现在计算$D_jS_i$:</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{\partial S_i}{\partial x_j} ,S_i =\frac{e^{x_i}}{\sum_{k=1}^ne^{x_k}}</script><p>$S_i$是个复合函数</p>
<script type="math/tex; mode=display">
S_i = \frac{g(x)}{h(x)},g(x) = e^{x_i},h(x)=\sum_{k=1}^ne^{x_k}</script><p>根据求导的除法法则：</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{g^\prime(x)h(x)-h^\prime(x)g(x)}{h^2(x)}</script><p>这里需要分类讨论，即$i =j$和$i\not=j$的情况：$i=j$时，$g^\prime(x_j)=e^{x_j}=e^{x_i},h^\prime(x_i)=e^{x_i} = e^{x_j}$；否则$i\not=j$，$g^\prime(x_i)=0,h^\prime(x_i)=e^{x_j}$.</p>
<p>$i=j$时带入得：</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{e^{x_i}\Sigma - e^{x_j}e^{x_i}}{\Sigma^2} = \frac{e^{x_i}}{\Sigma}\frac{\Sigma - e^{x_j}}{\Sigma} = S_i(1-S_j)</script><p>$i\not=j$时，带入得：</p>
<script type="math/tex; mode=display">
D_jS_i = \frac{0 - e^{x_i}e^{x_j}}{\Sigma^2} = -S_iS_j</script><p>综合以上讨论，有：</p>
<script type="math/tex; mode=display">
D_jS_i = S_i(\delta_{ij}-S_j)</script><script type="math/tex; mode=display">
i = j,\delta_{ij} = 1;i \not=j,\delta_{ij}=0</script><p><strong>softmax回归模型</strong>        </p>
<p>​        值得注意的是，这里讲的softmax回归与softmax函数还是有一点不同的。softmax函数只是把给定的输入做了一个指数归一化；相比logistic回归模型的参数只是一个向量，softmax回归模型包含参数矩阵$W$，也就是每一个类别都对应一个n维向量，n为输入特征数量。</p>
<p>假设有$k$个类别，则权重矩阵：</p>
<script type="math/tex; mode=display">
W_{k \times n} = \begin{pmatrix}\pmb w_1^T\\...\\\pmb w_k^T\end{pmatrix}</script><p>输入为第$j$个类别的概率为：</p>
<script type="math/tex; mode=display">
P(y = j | \pmb x) = \frac{e^{\pmb x^T\pmb w_j}}{\sum_{i=1}^k e^{\pmb x^T\pmb w_i}}</script><p>模型针对给定输入预测类别的过程如下图所示：</p>
<p><img src="https://eli.thegreenplace.net/images/2016/softmax-layer-generic.png" alt=""></p>
<p>注：T表示类别数量。</p>
<h3 id="二者的关联"><a href="#二者的关联" class="headerlink" title="二者的关联"></a>二者的关联</h3><p>当$k = 2$的时候，</p>
<script type="math/tex; mode=display">
P(y = 1) = \frac{e^{\pmb x^T\pmb w_1}}{e^{\pmb x^T\pmb w_1} + e^{\pmb x^T\pmb w_0}} = \frac{e^{\pmb x^T(\pmb w_1 - \pmb w_0)}}{1 + e^{\pmb x^T(\pmb w_1 - \pmb w_0)}}</script><script type="math/tex; mode=display">
P(y = 0) = \frac{e^{\pmb x^T\pmb w_0}}{e^{\pmb x^T\pmb w_1} + e^{\pmb x^T\pmb w_0}} = \frac{1}{1 + e^{\pmb x^T(\pmb w_1 - \pmb w_0)}}</script><p>令$x^T(\pmb w_1 - \pmb w_0) = \pmb t$，那么：</p>
<script type="math/tex; mode=display">
P(y = 1) = \frac{e^{\pmb t}}{1 + e^{\pmb t}}</script><script type="math/tex; mode=display">
P(y = 0) = \frac{1}{1 + e^{\pmb t}}</script><p>我们可以发现，当类别数等于$2$时，softmax模型就会退化为logistic回归模型。</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      
      
    </footer>

  </div>

  

  

  

</article>
    
  </article>
  

  
  <nav class="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2019-2020
        张永剑
      </li>
      <li>
        
          Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
      <aside class="sidebar">
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="张永剑的博客"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>







<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>



<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>